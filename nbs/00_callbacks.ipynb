{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callbacks\n",
    "#|default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| nbflags skip_exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks\n",
    "\n",
    "> Callbacks used by the BLURR library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import importlib, sys, torch\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.torch_core import *\n",
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import gc, pdb\n",
    "\n",
    "import GPUtil as GPU\n",
    "from IPython.display import display\n",
    "from fastai.text.all import *\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n",
    "\n",
    "from blurr.text.modeling.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "\n",
    "nvidia_smi_idx = 2\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CheckpointingNotSupported(Exception):\n",
    "    def __init__(self, msg=\"Model does not support gradient checkpointing.\"):\n",
    "        super().__init__(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GradientCheckpointing(Callback):\n",
    "    \"\"\"A fastai callback to enable gradient checkpointing for compatible HuggingFace models.\"\"\"\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"\"\"Enable gradient checkpointing on before_fit event.\"\"\"\n",
    "        \n",
    "        # Check that huggingface model supports gradient checkpointing\n",
    "        if not self.model.hf_model.supports_gradient_checkpointing:\n",
    "            raise CheckpointingNotSupported()\n",
    "            \n",
    "        if self.model.hf_model.is_gradient_checkpointing == False:\n",
    "            self.model.hf_model.gradient_checkpointing_enable()\n",
    "            \n",
    "    def after_fit(self):\n",
    "        \"\"\"Disable gradient checkpointing on after_fit event.\"\"\"\n",
    "        if self.model.hf_model.is_gradient_checkpointing:\n",
    "            self.model.hf_model.gradient_checkpointing_disable()\n",
    "    \n",
    "    @staticmethod\n",
    "    def supported(model: PreTrainedModel):\n",
    "        \"\"\"Tests whether a HuggingFace `PreTrainedModel` supports gradient checkpointing.\"\"\"\n",
    "        return model.supports_gradient_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_memory(device_idx=nvidia_smi_idx):\n",
    "    return GPU.getGPUs()[device_idx].memoryUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: load Data\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "model_path = Path(\"models\")\n",
    "imdb_df = pd.read_csv(path / \"texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: create Learner\n",
    "learn = BlearnerForSequenceClassification.from_data(\n",
    "    imdb_df, \n",
    "    \"roberta-large\", \n",
    "    dl_kwargs={\"bs\": 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.350596</td>\n",
       "      <td>0.233957</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9585.0 MBs used.\n"
     ]
    }
   ],
   "source": [
    "# train for a single epoch for baseline memory usage\n",
    "learn.fit_one_cycle(1, lr_max=1e-3)\n",
    "\n",
    "base_mem = gpu_memory()\n",
    "print(f\"{base_mem} MBs used.\")\n",
    "\n",
    "# Clear gpu memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.207228</td>\n",
       "      <td>0.226048</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3017.0 MBs used.\n"
     ]
    }
   ],
   "source": [
    "# train with GradientCheckpointing\n",
    "learn.fit_one_cycle(1, lr_max=1e-3, cbs=[GradientCheckpointing()])\n",
    "\n",
    "check_mem = gpu_memory()\n",
    "print(f\"{check_mem} MBs used.\")\n",
    "\n",
    "test_eq(base_mem > check_mem, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_callbacks.ipynb.\n",
      "Converted 00_utils.ipynb.\n",
      "Converted 01_text-callbacks.ipynb.\n",
      "Converted 01_text-utils.ipynb.\n",
      "Converted 11_text-data-core.ipynb.\n",
      "Converted 11_text-modeling-core.ipynb.\n",
      "Converted 12_text-data-language-modeling.ipynb.\n",
      "Converted 12_text-modeling-language-modeling.ipynb.\n",
      "Converted 13_text-data-token-classification.ipynb.\n",
      "Converted 13_text-modeling-token-classification.ipynb.\n",
      "Converted 14_text-data-question-answering.ipynb.\n",
      "Converted 14_text-modeling-question-answering.ipynb.\n",
      "Converted 20_text-data-seq2seq-core.ipynb.\n",
      "Converted 20_text-modeling-seq2seq-core.ipynb.\n",
      "Converted 21_text-data-seq2seq-summarization.ipynb.\n",
      "Converted 21_text-modeling-seq2seq-summarization.ipynb.\n",
      "Converted 22_text-data-seq2seq-translation.ipynb.\n",
      "Converted 22_text-modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_text-examples-high-level-api.ipynb.\n",
      "Converted 99b_text-examples-glue.ipynb.\n",
      "Converted 99c_text-examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_text-examples-multilabel.ipynb.\n",
      "Converted 99e_text-examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('dev')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
