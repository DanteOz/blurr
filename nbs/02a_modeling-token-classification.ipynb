{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *\n",
    "from blurr.data.token_classification import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.7.1+cu110\n",
      "Using fastai 2.2.5\n",
      "Using transformers 4.2.1\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.models.bert.configuration_bert.BertConfig,\n",
       " transformers.models.bert.tokenization_bert_fast.BertTokenizerFast,\n",
       " transformers.models.bert.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                     is_split_into_words=True, \n",
    "                                                     tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Zugang', 'O'), ('und', 'O'), ('Engagement', 'O'), (':', 'O'), ('das', 'O'), ('eigentlich', 'O'), ('Neue', 'O'), ('an', 'O'), ('der', 'O'), ('Netz', 'O'), ('(', 'O'), ('werk', 'O'), (')', 'O'), ('kunst,', 'O'), ('in', 'O'), (':', 'O'), ('Medien', 'O'), ('Kunst', 'O'), ('Netz,', 'O'), ('2004,', 'O'), ('URL', 'O'), (':', 'O'), ('*', 'O'), ('Arns,', 'B-PER'), ('Inke', 'O'), (':', 'B-PER'), ('Netzkulturen,', 'O'), ('Hamburg', 'O'), ('(', 'O'), ('eva', 'B-LOC'), ('),', 'O'), ('2002,', 'O'), ('S.', 'O'), ('46', 'O'), ('und', 'O'), ('81', 'O'), ('*', 'O'), ('Armin', 'O'), ('Medosch', 'O'), (':', 'O'), ('Public', 'B-PER'), ('Netbase', 'I-PER'), ('Wien.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Das', 'O'), ('Triebwerk', 'O'), ('des', 'O'), ('Hydrogen', 'B-OTH'), ('7', 'I-OTH'), ('leistet', 'O'), ('letztendlich', 'O'), ('191', 'O'), ('kW', 'O'), ('(', 'O'), ('260', 'O'), ('PS', 'O'), (')', 'O'), ('und', 'O'), ('hat', 'O'), ('ein', 'O'), ('Drehmoment', 'O'), ('von', 'O'), ('390', 'O'), ('Nm', 'O'), ('(', 'O'), ('zum', 'O'), ('Vergleich,', 'O'), ('der', 'O'), ('„', 'O'), ('normale', 'O'), ('760i', 'O'), ('leistet', 'O'), ('327', 'B-OTH'), ('kW', 'O'), ('/', 'O'), ('445', 'O'), ('PS', 'O'), ('und', 'O'), ('600', 'O'), ('Nm', 'O'), (').', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if (metric_key == 'accuracy'): return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'precision'): return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'recall'): return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'f1'): return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "        \n",
    "    if (metric_key == 'classification_report'): return seq_metrics.classification_report(targ_toks, pred_toks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(filter(lambda el: isinstance(el, HF_TokenCategorize), learn.dls.tfms[1]), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in learn.dls.tfms[1]: print(isinstance(x,HF_TokenCategorize), type(x), HF_TokenCategorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdf = HF_TokenCategorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isinstance(asdf,HF_TokenCategorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_blurr_tfm(learn.dls.tfms, tfm_class=HF_TokenCategorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self=self, names='tok_metrics, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in tok_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the HF_TokenClassBeforeBatchTransform\n",
    "        hf_before_batch_tfm = get_blurr_tfm(self.learn.dls.before_batch)\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=HF_TokenCategorize)\n",
    "        \n",
    "        self.hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = hf_before_batch_tfm.kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "    def before_fit(self): self.setup()\n",
    "    \n",
    "    \n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        if (self.training or self.learn.y is None): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self): self.results = []\n",
    "        \n",
    "    def after_validate(self):\n",
    "        if (len(self.results) < 1): return\n",
    "        \n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys(): \n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "        \n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, \n",
    "                                                                                   preds, \n",
    "                                                                                   'classification_report')\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f'Couldn\\'t calcualte classification report: {err}')\n",
    "        \n",
    "        \n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "learn_cbs = [HF_BaseModelCallback]\n",
    "fit_cbs = [HF_TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model,opt_func=partial(Adam),cbs=learn_cbs,splitter=hf_splitter)\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([2, 61, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([2, 61]), 2, torch.Size([2, 61]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122, 18]) torch.Size([122])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0002511886414140463, lr_steep=3.0199516913853586e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.218139</td>\n",
       "      <td>0.183780</td>\n",
       "      <td>0.945181</td>\n",
       "      <td>0.547529</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.546490</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090415</td>\n",
       "      <td>0.150739</td>\n",
       "      <td>0.959210</td>\n",
       "      <td>0.676806</td>\n",
       "      <td>0.700787</td>\n",
       "      <td>0.688588</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.115544</td>\n",
       "      <td>0.961808</td>\n",
       "      <td>0.733840</td>\n",
       "      <td>0.709559</td>\n",
       "      <td>0.721495</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8), cbs=fit_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.79      0.77      0.78        74\n",
      "    LOCderiv       0.91      0.87      0.89        23\n",
      "     LOCpart       0.00      0.00      0.00         0\n",
      "         ORG       0.80      0.60      0.68        65\n",
      "     ORGpart       0.25      0.67      0.36         3\n",
      "         OTH       0.37      0.39      0.38        33\n",
      "     OTHpart       0.00      0.00      0.00         0\n",
      "         PER       0.90      0.84      0.87        74\n",
      "     PERpart       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.73      0.71      0.72       272\n",
      "   macro avg       0.45      0.46      0.44       272\n",
      "weighted avg       0.77      0.71      0.74       272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "print(learn.token_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, learner, \n",
    "                 ctxs=None, max_n=6, trunc_at=None, **kwargs):    \n",
    "    \n",
    "    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)\n",
    "    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "    ignore_token_id = hf_before_batch_tfm.ignore_token_id\n",
    "    \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # recontstruct the string and split on space to get back your pre-tokenized list of tokens\n",
    "        toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=True)\n",
    "        pretokenized_toks =  hf_tokenizer.convert_tokens_to_string(toks).split()\n",
    "        \n",
    "        # get predictions for subtokens that aren't ignored (e.g. special toks and token parts)\n",
    "        pred_labels = [ pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != -ignore_token_id ]\n",
    "        \n",
    "        trg_labels = ast.literal_eval(sample[1])\n",
    "        res.append([f'{[ (tok, trg, pred) for idx, (tok, pred, trg) in enumerate(zip(pretokenized_toks, pred_labels, trg_labels)) if (trunc_at is None or idx < trunc_at) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'O'), ('et', 'I-OTH', 'B-PER'), ('al.', 'I-OTH', 'I-PER'), ('(', 'O', 'I-PER'), ('1994', 'O', 'O'), (')', 'O', 'O'), ('S.', 'O', 'O'), ('593.', 'O', 'O'), ('Wink', 'O', 'O'), ('&amp;', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('NEWSru.', 'B-OTH', 'O'), ('ua', 'O', 'B-ORG'), ('/', 'O', 'I-ORG'), (':', 'B-OTH', 'I-ORG'), ('Политисполком', 'I-OTH', 'I-ORG'), ('СПУ', 'I-OTH', 'O'), ('отказал', 'I-OTH', 'O'), ('Морозу', 'I-OTH', 'O'), ('в', 'I-OTH', 'O'), ('отставке', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _blurr_predict_tokens(predict_func, items, hf_before_batch_tfm):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.blurr_predict` or `blurrONNX.predict.\n",
    "    Aligns the predicted labels, label ids, and probabilities with what you passed in excluding subword tokens\n",
    "    \"\"\"\n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_before_batch_tfm.tok_kwargs\n",
    "    \n",
    "    if (isinstance(items[0], str)): items = [items]\n",
    "        \n",
    "    outs = []\n",
    "    for inp, res in zip(items, predict_func(items)):\n",
    "        # blurr_predict returns a list for each, we only doing one at a time so git first element of each\n",
    "        pred_lbls, pred_lbl_ids, probs = res[0][0], res[1][0], res[2][0]\n",
    "  \n",
    "        # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "        # return\n",
    "        subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp ]\n",
    "    \n",
    "        # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "        # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "        # (e.g., [CLS], [SEP], etc...)\n",
    "        res = hf_tokenizer(inp, None, \n",
    "                           max_length=hf_before_batch_tfm.max_length,\n",
    "                           padding=hf_before_batch_tfm.padding,\n",
    "                           truncation=hf_before_batch_tfm.truncation,\n",
    "                           is_split_into_words=hf_before_batch_tfm.is_split_into_words,\n",
    "                           **tok_kwargs)\n",
    "\n",
    "        special_toks_msk = L(res['special_tokens_mask'])\n",
    "        actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "        # using the indexes to the actual tokens, get that info from the results returned above\n",
    "        pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "        actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "        actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "        actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "        # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "        # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "        offset = 0\n",
    "        raw_trg_idxs = []\n",
    "        for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "            raw_trg_idxs.append(idx+offset)\n",
    "            offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "            \n",
    "        outs.append((inp, \n",
    "                     actual_pred_lbls[raw_trg_idxs], \n",
    "                     actual_pred_lbl_ids[raw_trg_idxs], \n",
    "                     actual_probs[raw_trg_idxs]))\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict_tokens(self:Learner, items, **kargs):\n",
    "    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n",
    "    return _blurr_predict_tokens(self.blurr_predict, items, hf_before_batch_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**, **\\*\\*`kargs`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-ORG')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = 'tok_class_learn_export'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-ORG')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f'{export_fname}.pkl')\n",
    "inf_learn = load_learner(fname=f'{export_fname}.pkl')\n",
    "\n",
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# @patch\n",
    "# def predict_tokens(self:blurrONNX, items, **kargs):\n",
    "#     hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n",
    "#     return _blurr_predict_tokens(self.predict, items, hf_before_batch_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow\n",
    "# learn.blurr_to_onnx(export_fname, quantize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow\n",
    "# onnx_inf = blurrONNX(export_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow\n",
    "# res = onnx_inf.predict_tokens(txt.split())\n",
    "# for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.models.albert.modeling_albert.AlbertForTokenClassification,\n",
       " transformers.models.auto.modeling_auto.AutoModelForTokenClassification,\n",
       " transformers.models.bert.modeling_bert.BertForTokenClassification,\n",
       " transformers.models.camembert.modeling_camembert.CamembertForTokenClassification,\n",
       " transformers.models.distilbert.modeling_distilbert.DistilBertForTokenClassification,\n",
       " transformers.models.electra.modeling_electra.ElectraForTokenClassification,\n",
       " transformers.models.flaubert.modeling_flaubert.FlaubertForTokenClassification,\n",
       " transformers.models.funnel.modeling_funnel.FunnelForTokenClassification,\n",
       " transformers.models.layoutlm.modeling_layoutlm.LayoutLMForTokenClassification,\n",
       " transformers.models.longformer.modeling_longformer.LongformerForTokenClassification,\n",
       " transformers.models.mpnet.modeling_mpnet.MPNetForTokenClassification,\n",
       " transformers.models.mobilebert.modeling_mobilebert.MobileBertForTokenClassification,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification,\n",
       " transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForTokenClassification,\n",
       " transformers.models.xlm.modeling_xlm.XLMForTokenClassification,\n",
       " transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForTokenClassification,\n",
       " transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ model_type for model_type in BLURR_MODEL_HELPER.get_models(task='TokenClassification') \n",
    " if (not model_type.__name__.startswith('TF')) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'google/electra-small-discriminator',\n",
    "    'flaubert/flaubert_small_cased',\n",
    "    'huggingface/funnel-small-base',\n",
    "    'allenai/longformer-base-4096',\n",
    "    'microsoft/mpnet-base',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'squeezebert/squeezebert-uncased',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.524979</td>\n",
       "      <td>1.339716</td>\n",
       "      <td>0.899777</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('helbig', 'B-OTH', 'B-OTHpart'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'B-PERderiv'), ('1994', 'O', 'B-PERderiv'), (')', 'O', 'O'), ('s.', 'O', 'O'), ('593.', 'O', 'O'), ('wink', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('nach', 'O', 'B-OTHpart'), ('seiner', 'O', 'O'), ('ruckkehr', 'O', 'O'), ('hielt', 'O', 'O'), ('strummer', 'B-PER', 'O'), ('ein', 'O', 'O'), ('bandmeeting', 'O', 'O'), ('ab', 'O', 'O'), (',', 'O', 'O'), ('in', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-multilingual-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.004920</td>\n",
       "      <td>1.833674</td>\n",
       "      <td>0.891415</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'I-ORGpart'), ('et', 'I-OTH', 'O'), ('al.', 'I-OTH', 'B-OTHpart'), ('(', 'O', 'O'), ('1994', 'O', 'O'), (')', 'O', 'O'), ('S.', 'O', 'O'), ('593.', 'O', 'O'), ('Wink', 'O', 'O'), ('&amp;', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Scenes', 'B-OTH', 'I-ORGpart'), ('of', 'I-OTH', 'O'), ('a', 'I-OTH', 'O'), ('Sexual', 'I-OTH', 'O'), ('Nature', 'I-OTH', 'O'), ('(', 'O', 'O'), ('GB', 'O', 'O'), ('2006', 'O', 'O'), (')', 'O', 'O'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.406108</td>\n",
       "      <td>2.344383</td>\n",
       "      <td>0.902692</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('(', 'O', 'O'), ('Standard', 'B-ORG', 'O'), ('Oil', 'I-ORG', 'O'), ('of', 'I-ORG', 'O'), ('New', 'I-ORG', 'O'), ('Jersey', 'I-ORG', 'O'), (')', 'O', 'O'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ausgesprochen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Nach', 'O', 'O'), ('seiner', 'O', 'O'), ('R&lt;unk&gt;ckkehr', 'O', 'O'), ('hielt', 'O', 'O'), ('Strummer', 'B-PER', 'O'), ('ein', 'O', 'O'), ('Bandmeeting', 'O', 'O'), ('ab', 'O', 'O'), (',', 'O', 'O'), ('in', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n",
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.870939</td>\n",
       "      <td>1.693202</td>\n",
       "      <td>0.908778</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('nach', 'O', 'O'), ('seiner', 'O', 'O'), ('ruckkehr', 'O', 'O'), ('hielt', 'O', 'O'), ('strummer', 'B-PER', 'O'), ('ein', 'O', 'O'), ('bandmeeting', 'O', 'O'), ('ab,', 'O', 'O'), ('in', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('es', 'O', 'O'), ('ist', 'O', 'O'), ('beabsichtigt,', 'O', 'O'), ('die', 'O', 'O'), ('aktien', 'O', 'O'), ('im', 'O', 'O'), ('ersten', 'O', 'O'), ('halbjahr', 'O', 'O'), ('2007', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/electra-small-discriminator ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.381015</td>\n",
       "      <td>2.242515</td>\n",
       "      <td>0.882031</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('scenes', 'B-OTH', 'O'), ('of', 'I-OTH', 'I-LOCderiv'), ('a', 'I-OTH', 'I-OTH'), ('sexual', 'I-OTH', 'B-PER'), ('nature', 'I-OTH', 'I-LOCderiv'), ('(', 'O', 'B-PER'), ('gb', 'O', 'B-OTHderiv'), ('2006', 'O', 'B-OTHderiv'), (')', 'O', 'B-OTHderiv'), ('-', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('außerdem', 'O', 'O'), ('befindet', 'O', 'O'), ('sich', 'O', 'O'), ('im', 'O', 'O'), ('nordwesten', 'O', 'O'), ('der', 'O', 'O'), ('stadt', 'O', 'O'), ('(', 'O', 'O'), ('auf', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n",
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.387230</td>\n",
       "      <td>1.017416</td>\n",
       "      <td>0.905962</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Außerdem', 'O', 'O'), ('befindet', 'O', 'O'), ('sich', 'O', 'O'), ('im', 'O', 'O'), ('Nordwesten', 'O', 'O'), ('der', 'O', 'O'), ('Stadt', 'O', 'O'), ('(', 'O', 'O'), ('auf', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'O'), ('einer', 'O', 'O'), ('Fernsehdiskussion', 'O', 'O'), ('traf', 'O', 'O'), ('er', 'O', 'O'), ('auf', 'O', 'O'), ('den', 'O', 'O'), ('Kritiker', 'O', 'O'), ('Alexander', 'B-PER', 'O'), ('Walker', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.625950</td>\n",
       "      <td>1.334077</td>\n",
       "      <td>0.799323</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('in', 'O', 'O'), ('einer', 'O', 'O'), ('fernsehdiskussion', 'O', 'O'), ('traf', 'O', 'O'), ('er', 'O', 'O'), ('auf', 'O', 'O'), ('den', 'O', 'O'), ('kritiker', 'O', 'O'), ('alexander', 'B-PER', 'O'), ('walker,', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('ein', 'O', 'O'), ('wesentlicher', 'O', 'O'), ('unterschied', 'O', 'O'), ('zu', 'O', 'O'), ('der', 'O', 'O'), ('position,', 'O', 'O'), ('die', 'O', 'O'), ('er', 'O', 'O'), ('in', 'O', 'O'), ('der', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.216611</td>\n",
       "      <td>2.117853</td>\n",
       "      <td>0.893969</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('NEWSru.ua', 'B-OTH', 'O'), ('/', 'O', 'O'), (':', 'O', 'O'), ('Политисполком', 'B-OTH', 'O'), ('СПУ', 'I-OTH', 'O'), ('отказал', 'I-OTH', 'O'), ('Морозу', 'I-OTH', 'O'), ('в', 'I-OTH', 'O'), ('отставке', 'I-OTH', 'B-LOCpart'), ('Die', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'O'), ('einer', 'O', 'O'), ('Fernsehdiskussion', 'O', 'O'), ('traf', 'O', 'O'), ('er', 'O', 'O'), ('auf', 'O', 'O'), ('den', 'O', 'O'), ('Kritiker', 'O', 'O'), ('Alexander', 'B-PER', 'O'), ('Walker', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.389585</td>\n",
       "      <td>2.301636</td>\n",
       "      <td>0.908370</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('senden', 'O', 'O'), ('exxon', 'B-ORG', 'O'), ('mobil', 'I-ORG', 'O'), ('\"', 'O', 'O'), ('buy', 'O', 'O'), ('\"', 'O', 'O'), ('paris', 'B-LOC', 'O'), ('(', 'O', 'O'), ('aktiencheck.', 'B-ORG', 'O'), ('de', 'I-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('es', 'O', 'O'), ('ist', 'O', 'O'), ('beabsichtigt,', 'O', 'O'), ('die', 'O', 'O'), ('aktien', 'O', 'O'), ('im', 'O', 'O'), ('ersten', 'O', 'O'), ('halbjahr', 'O', 'O'), ('2007', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>4.938655</td>\n",
       "      <td>0.018177</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('helbig', 'B-OTH', 'B-LOC'), ('et', 'I-OTH', 'I-LOCderiv'), ('al.', 'I-OTH', 'I-PER'), ('(', 'O', 'B-PERderiv'), ('1994', 'O', 'B-LOCpart'), (')', 'O', 'B-LOC'), ('s.', 'O', 'B-LOC'), ('593.', 'O', 'B-LOCpart'), ('wink', 'O', 'B-ORG'), ('&amp;', 'B-OTH', 'B-ORGpart')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('die', 'O', 'B-LOC'), ('flugel', 'O', 'I-ORG'), ('die', 'O', 'O'), ('geoffneten', 'O', 'B-PERderiv'), ('flugel', 'O', 'I-ORG'), ('zeigen', 'O', 'B-LOCderiv'), ('in', 'O', 'I-ORG'), ('vier', 'O', 'I-ORG'), ('szenen', 'O', 'O'), ('hohepunkte', 'O', 'B-PERderiv')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.488893</td>\n",
       "      <td>2.402814</td>\n",
       "      <td>0.803790</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'B-LOCpart'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'B-OTH'), ('.', 'O', 'B-ORGpart'), ('(', 'O', 'I-LOC'), ('1994', 'O', 'B-OTH'), (')', 'O', 'O'), ('S.', 'O', 'O'), ('593.', 'O', 'I-LOC'), ('Wink', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('(', 'O', 'B-LOCpart'), ('Standard', 'B-ORG', 'O'), ('Oil', 'I-ORG', 'B-PER'), ('of', 'I-ORG', 'B-PER'), ('New', 'I-ORG', 'B-ORGpart'), ('Jersey', 'I-ORG', 'B-PER'), (')', 'O', 'B-PER'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ausgesprochen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.389049</td>\n",
       "      <td>2.331840</td>\n",
       "      <td>0.897055</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('zugang', 'O', 'O'), ('und', 'O', 'O'), ('engagement', 'O', 'O'), (':', 'O', 'O'), ('das', 'O', 'O'), ('eigentlich', 'O', 'O'), ('neue', 'O', 'O'), ('an', 'O', 'O'), ('der', 'O', 'O'), ('netz', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('nach', 'O', 'O'), ('seiner', 'O', 'O'), ('ruckkehr', 'O', 'O'), ('hielt', 'O', 'O'), ('strummer', 'B-PER', 'O'), ('ein', 'O', 'O'), ('bandmeeting', 'O', 'O'), ('ab,', 'O', 'O'), ('in', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n",
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.970896</td>\n",
       "      <td>0.824918</td>\n",
       "      <td>0.882533</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('helbig', 'B-OTH', 'O'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'O'), ('1994', 'O', 'O'), (')', 'O', 'O'), ('s', 'O', 'O'), ('.', 'O', 'O'), ('593', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('da', 'O', 'O'), ('ich', 'O', 'O'), ('mir', 'O', 'O'), ('als', 'O', 'O'), ('kleine', 'O', 'O'), ('rentnerin', 'O', 'O'), ('nicht', 'O', 'O'), ('sehr', 'O', 'O'), ('viel', 'O', 'O'), ('leisten', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.502142</td>\n",
       "      <td>2.446536</td>\n",
       "      <td>0.653743</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Nach', 'O', 'I-ORGpart'), ('seiner', 'O', 'B-PER'), ('Rückkehr', 'O', 'O'), ('hielt', 'O', 'O'), ('Strummer', 'B-PER', 'B-PER'), ('ein', 'O', 'B-PER'), ('Bandmeeting', 'O', 'B-PER'), ('ab', 'O', 'B-PER'), (',', 'O', 'O'), ('in', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('NEWSru.ua', 'B-OTH', 'I-ORGpart'), ('/', 'O', 'I-LOC'), (':', 'O', 'I-ORGpart'), ('Политисполком', 'B-OTH', 'O'), ('СПУ', 'I-OTH', 'B-LOC'), ('отказал', 'I-OTH', 'B-PER'), ('Морозу', 'I-OTH', 'I-ORGpart'), ('в', 'I-OTH', 'O'), ('отставке', 'I-OTH', 'B-PER'), ('Die', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.341977</td>\n",
       "      <td>1.337892</td>\n",
       "      <td>0.471706</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'O'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'B-LOCderiv'), ('.', 'O', 'B-LOCderiv'), ('(', 'O', 'B-LOCderiv'), ('1994', 'O', 'B-LOCderiv'), (')', 'O', 'B-LOCderiv'), ('S.', 'O', 'B-LOCderiv'), ('593.', 'O', 'B-LOCderiv'), ('Wink', 'B-OTH', 'B-LOCderiv')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Zugang', 'O', 'B-LOCderiv'), ('und', 'O', 'O'), ('Engagement', 'O', 'B-LOCderiv'), (':', 'O', 'O'), ('das', 'O', 'O'), ('eigentlich', 'O', 'O'), ('Neue', 'O', 'O'), ('an', 'O', 'B-LOCderiv'), ('der', 'O', 'B-LOCderiv'), ('Netz(', 'O', 'B-LOCderiv')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config=config)\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n')\n",
    "    \n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if (hf_tokenizer.pad_token is None): \n",
    "        hf_tokenizer.add_special_tokens({'pad_token': '<pad>'})  \n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()['<pad>']\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer)) \n",
    "    \n",
    "    before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                         max_length=seq_sz,\n",
    "                                                         padding='max_length',\n",
    "                                                         is_split_into_words=True, \n",
    "                                                         tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "    blocks = (\n",
    "        HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n",
    "        HF_TokenCategoryBlock(vocab=labels)\n",
    "    )\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('tokens'),\n",
    "                       get_y= lambda inp: [ \n",
    "                           (label, len(hf_tokenizer.tokenize(str(entity)))) \n",
    "                           for entity, label in zip(inp.tokens, inp.labels) \n",
    "                       ],\n",
    "                       splitter=RandomSplitter())\n",
    "    \n",
    "    dls = dblock.dataloaders(germ_eval_df, bs=bsz)\n",
    "\n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter).to_fp16()\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max= 3e-5, moms=(0.8,0.7,0.8), \n",
    "                            cbs=[HF_TokenClassMetricsCallback(tok_metrics=['accuracy'])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "        \n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "        \n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizerFast</td>\n",
       "      <td>DistilBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model_name', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01za_data-seq2seq-core.ipynb.\n",
      "Converted 01zb_data-seq2seq-language-modeling.ipynb.\n",
      "Converted 01zc_data-seq2seq-summarization.ipynb.\n",
      "Converted 01zd_data-seq2seq-translation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02za_modeling-seq2seq-core.ipynb.\n",
      "Converted 02zb_modeling-seq2seq-language-modeling.ipynb.\n",
      "Converted 02zc_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 02zc_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-multilabel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
