{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *\n",
    "from blurr.data.token_classification import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.7.0\n",
      "Using fastai 2.1.5\n",
      "Using transformers 3.5.0\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_tokenizer, is_split_into_words=True, \n",
    "                                                     tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(before_batch_tfms=before_batch_tfm, input_return_type=HF_TokenClassInput), \n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH'), ('et', 'I-OTH'), ('al', 'I-OTH'), ('.', 'O'), ('(', 'O'), ('1994', 'O'), (')', 'O'), ('S', 'O'), ('.', 'O'), ('593', 'B-OTH'), ('.', 'I-OTH'), ('Wink', 'I-OTH'), ('&amp;', 'I-OTH'), ('Seibold', 'I-OTH'), ('et', 'O'), ('al', 'O'), ('.', 'O'), ('(', 'O'), ('1998', 'O'), (')', 'O'), ('S', 'O'), ('.', 'O'), ('32', 'O'), ('Inwieweit', 'O'), ('noch', 'O'), ('andere', 'O'), ('Falken', 'O'), (',', 'B-LOCderiv'), ('wie', 'O'), ('der', 'O'), ('Afrikanische', 'O'), ('Baumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('cuvieri', 'O'), (')', 'O'), ('oder', 'O'), ('der', 'O'), ('Malaienbaumfalke', 'O'), ('(', 'O'), ('Falco', 'O'), ('serverus', 'O'), (')', 'O'), ('dieser', 'O'), ('Gruppe', 'O'), ('zuzuzählen', 'O'), ('sind', 'O'), (',', 'O'), ('ist', 'O'), ('Gegenstand', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('NEWSru', 'B-OTH'), ('.', 'O'), ('ua', 'O'), ('/', 'B-OTH'), (':', 'I-OTH'), ('Политисполком', 'I-OTH'), ('СПУ', 'I-OTH'), ('отказал', 'I-OTH'), ('Морозу', 'I-OTH'), ('в', 'O'), ('отставке', 'B-ORG'), ('Die', 'O'), ('SPU', 'O'), ('legte', 'O'), (',', 'O'), ('wie', 'O'), ('auch', 'O'), ('vier', 'O'), ('weitere', 'O'), ('Parteien', 'O'), (',', 'O'), ('beim', 'O'), ('Obersten', 'O'), ('Gericht', 'B-LOC'), ('der', 'O'), ('Ukraine', 'O'), ('Beschwerde', 'O'), ('gegen', 'O'), ('den', 'O'), ('Ablauf', 'O'), ('der', 'O'), ('Wahl', 'O'), ('ein', 'O'), ('und', 'O'), ('behauptet', 'O'), (',', 'O'), ('es', 'O'), ('sei', 'O'), ('bei', 'O'), ('der', 'O'), ('Auszählung', 'O'), ('zu', 'O'), ('Unregelmäßigkeiten', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if (metric_key == 'accuracy'): return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'precision'): return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'recall'): return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'f1'): return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "        \n",
    "    if (metric_key == 'classification_report'): return seq_metrics.classification_report(targ_toks, pred_toks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenClassCallback(HF_BaseModelCallback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self=self, names='tok_metrics, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in tok_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the target's HF_TokenizerTransform (used for rouge metrics)\n",
    "        hf_textblock_tfm = self.dls.before_batch[0]\n",
    "        self.hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = self.dls.tfms[1].ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = hf_textblock_tfm.kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "    def before_fit(self): self.setup()\n",
    "    \n",
    "    \n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        if (self.training or self.learn.y is None): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self): self.results = []\n",
    "        \n",
    "    def after_validate(self):\n",
    "        if (len(self.results) < 1): return\n",
    "        \n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys(): \n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "        \n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, \n",
    "                                                                                   preds, \n",
    "                                                                                   'classification_report')\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f'Couldn\\'t calcualte classification report: {err}')\n",
    "        \n",
    "        \n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                cbs=[HF_TokenClassCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([2, 76, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([2, 76]), 2, torch.Size([2, 76]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([152, 18]) torch.Size([152])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0013182567432522773, lr_steep=3.0199516913853586e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.222455</td>\n",
       "      <td>0.179930</td>\n",
       "      <td>0.949694</td>\n",
       "      <td>0.540426</td>\n",
       "      <td>0.529167</td>\n",
       "      <td>0.534737</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116176</td>\n",
       "      <td>0.123512</td>\n",
       "      <td>0.959542</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.049742</td>\n",
       "      <td>0.118629</td>\n",
       "      <td>0.964333</td>\n",
       "      <td>0.693617</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.684874</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ORG       0.59      0.52      0.55        56\n",
      "      PER       0.92      0.85      0.88        54\n",
      "      LOC       0.84      0.72      0.77        67\n",
      " LOCderiv       0.87      0.83      0.85        24\n",
      "      OTH       0.33      0.38      0.35        32\n",
      "  ORGpart       0.33      1.00      0.50         3\n",
      "  LOCpart       0.62      1.00      0.77         5\n",
      "\n",
      "micro avg       0.69      0.68      0.68       241\n",
      "macro avg       0.73      0.68      0.70       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "print(learn.token_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, learner, \n",
    "                 ctxs=None, max_n=6, trunc_at=None, **kwargs):    \n",
    "    # grab tokenizer\n",
    "    hf_textblock_tfm = learner.dls.before_batch[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    ignore_token_id = hf_textblock_tfm.ignore_token_id\n",
    "    \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # recontstruct the string and split on space to get back your pre-tokenized list of tokens\n",
    "        toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=True)\n",
    "        pretokenized_toks =  hf_tokenizer.convert_tokens_to_string(toks).split()\n",
    "        \n",
    "        # get predictions for subtokens that aren't ignored (e.g. special toks and token parts)\n",
    "        pred_labels = [ pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != -ignore_token_id ]\n",
    "        \n",
    "        trg_labels = ast.literal_eval(sample[1])\n",
    "        res.append([f'{[ (tok, trg, pred) for idx, (tok, pred, trg) in enumerate(zip(pretokenized_toks, pred_labels, trg_labels)) if (trunc_at is None or idx < trunc_at) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('(', 'O', 'O'), ('Standard', 'B-ORG', 'O'), ('Oil', 'I-ORG', 'B-ORG'), ('of', 'I-ORG', 'I-ORG'), ('New', 'I-ORG', 'I-ORG'), ('Jersey', 'I-ORG', 'B-LOC'), (')', 'O', 'I-LOC'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ausgesprochen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Erstmals', 'O', 'O'), ('Urkundlich', 'O', 'O'), ('erwähnt', 'O', 'O'), ('ist', 'O', 'O'), ('Nimburg', 'B-LOC', 'O'), ('bereits', 'O', 'O'), ('im', 'O', 'O'), ('Jahre', 'O', 'B-LOC'), ('977', 'O', 'I-LOC'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict_tokens(self:Learner, inp, **kargs):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
    "    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input\n",
    "    \"\"\"\n",
    "    pred_lbls, pred_lbl_ids, probs = self.blurr_predict(inp)\n",
    "\n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_textblock_tfm = self.dls.before_batch[0]\n",
    "    hf_tokenizer = hf_textblock_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_textblock_tfm.tok_kwargs\n",
    "    \n",
    "    # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "    # return\n",
    "    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp ]\n",
    "    \n",
    "    # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "    # (e.g., [CLS], [SEP], etc...)\n",
    "    res = hf_tokenizer(inp, None, \n",
    "                       max_length=hf_textblock_tfm.max_length,\n",
    "                       padding=hf_textblock_tfm.padding,\n",
    "                       truncation=hf_textblock_tfm.truncation,\n",
    "                       is_split_into_words=hf_textblock_tfm.is_split_into_words,\n",
    "                       **tok_kwargs)\n",
    "\n",
    "    special_toks_msk = L(res['special_tokens_mask'])\n",
    "    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "    # using the indexes to the actual tokens, get that info from the results returned above\n",
    "    pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "    actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "    # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "    offset = 0\n",
    "    raw_trg_idxs = []\n",
    "    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "        raw_trg_idxs.append(idx+offset)\n",
    "        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`inp`**, **\\*\\*`kargs`**)\n",
       "\n",
       "Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only\n",
       "get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "print([(tok, lbl) for tok,lbl in zip(res[0],res[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_albert.AlbertForTokenClassification,\n",
       " transformers.modeling_auto.AutoModelForTokenClassification,\n",
       " transformers.modeling_bert.BertForTokenClassification,\n",
       " transformers.modeling_camembert.CamembertForTokenClassification,\n",
       " transformers.modeling_distilbert.DistilBertForTokenClassification,\n",
       " transformers.modeling_electra.ElectraForTokenClassification,\n",
       " transformers.modeling_flaubert.FlaubertForTokenClassification,\n",
       " transformers.modeling_funnel.FunnelForTokenClassification,\n",
       " transformers.modeling_layoutlm.LayoutLMForTokenClassification,\n",
       " transformers.modeling_longformer.LongformerForTokenClassification,\n",
       " transformers.modeling_mobilebert.MobileBertForTokenClassification,\n",
       " transformers.modeling_roberta.RobertaForTokenClassification,\n",
       " transformers.modeling_squeezebert.SqueezeBertForTokenClassification,\n",
       " transformers.modeling_xlm.XLMForTokenClassification,\n",
       " transformers.modeling_xlm_roberta.XLMRobertaForTokenClassification,\n",
       " transformers.modeling_xlnet.XLNetForTokenClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='TokenClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    #'<electra>', # currently no pre-trained electra model works for token classification\n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-ende-1024',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.330705</td>\n",
       "      <td>0.299386</td>\n",
       "      <td>0.928237</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.332016</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('zugang', 'O', 'O'), ('und', 'O', 'O'), ('engagement', 'O', 'O'), (':', 'O', 'O'), ('das', 'O', 'O'), ('eigentlich', 'O', 'O'), ('neue', 'O', 'O'), ('an', 'O', 'O'), ('der', 'O', 'O'), ('netz(', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('wie', 'O', 'O'), ('wenige', 'O', 'O'), ('wochen', 'O', 'O'), ('vor', 'O', 'O'), ('der', 'O', 'O'), ('bekanntgabe', 'O', 'O'), ('in', 'O', 'O'), ('einem', 'O', 'O'), ('interview', 'O', 'O'), ('angekundigt', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-multilingual-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.202446</td>\n",
       "      <td>0.188083</td>\n",
       "      <td>0.945370</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.531496</td>\n",
       "      <td>0.546559</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Senden', 'O', 'O'), ('Exxon', 'B-ORG', 'O'), ('Mobil', 'I-ORG', 'O'), ('\"', 'O', 'B-ORG'), ('buy', 'O', 'B-ORG'), ('\"', 'O', 'B-ORG'), ('Paris', 'B-LOC', 'O'), ('(', 'O', 'O'), ('aktiencheck', 'B-ORG', 'O'), ('.', 'I-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('Eine', 'O', 'O'), ('Armee', 'O', 'O'), ('von', 'O', 'O'), ('160', 'O', 'O'), ('000', 'O', 'O'), ('bis', 'O', 'O'), ('180', 'O', 'O'), ('000', 'O', 'O'), ('Berufs', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.878381</td>\n",
       "      <td>0.802971</td>\n",
       "      <td>0.908404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Zugang', 'O', 'O'), ('und', 'O', 'O'), ('Engagement', 'O', 'O'), (':', 'O', 'O'), ('das', 'O', 'O'), ('eigentlich', 'O', 'O'), ('Neue', 'O', 'O'), ('an', 'O', 'O'), ('der', 'O', 'O'), ('Netz(', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Vietnam', 'B-LOC', 'O'), (',', 'O', 'O'), ('Irak', 'B-LOC', 'O'), (':', 'O', 'O'), ('\"', 'O', 'O'), ('Der', 'O', 'O'), ('Friedensstifter', 'O', 'O'), ('Amerika', 'B-LOC', 'O'), ('ist', 'O', 'O'), ('eine', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n",
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.370298</td>\n",
       "      <td>0.322095</td>\n",
       "      <td>0.918027</td>\n",
       "      <td>0.133721</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('der', 'O', 'O'), ('28', 'O', 'O'), ('-', 'O', 'O'), ('jahrige', 'O', 'O'), ('und', 'O', 'O'), ('sein', 'O', 'O'), ('team', 'O', 'O'), (',', 'O', 'O'), ('zu', 'O', 'O'), ('dem', 'B-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('eine', 'O', 'O'), ('armee', 'O', 'O'), ('von', 'O', 'O'), ('160', 'O', 'O'), ('000', 'O', 'O'), ('bis', 'O', 'O'), ('180', 'O', 'O'), ('000', 'O', 'O'), ('berufs', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.364293</td>\n",
       "      <td>0.320835</td>\n",
       "      <td>0.916350</td>\n",
       "      <td>0.137725</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.172285</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Mit', 'O', 'O'), ('der', 'O', 'O'), ('Servicefrau', 'O', 'O'), ('verband', 'O', 'O'), ('Bianca', 'B-PER', 'O'), ('offenbar', 'O', 'O'), ('eine', 'O', 'O'), ('Art', 'O', 'O'), ('Freundschaft', 'O', 'O'), ('–', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('(', 'O', 'O'), ('1995', 'O', 'O'), (')', 'O', 'O'), ('drehte', 'O', 'O'), ('der', 'O', 'O'), ('damals', 'O', 'O'), ('22-jährige', 'O', 'O'), ('Fatih', 'B-PER', 'O'), ('Akin', 'I-PER', 'O'), ('seinen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.449983</td>\n",
       "      <td>0.469836</td>\n",
       "      <td>0.894260</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.049793</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('die', 'O', 'O'), ('flugel', 'O', 'O'), ('die', 'O', 'O'), ('geoffneten', 'O', 'O'), ('flugel', 'O', 'O'), ('zeigen', 'O', 'O'), ('in', 'O', 'O'), ('vier', 'O', 'O'), ('szenen', 'O', 'O'), ('hohepunkte', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('erstmals', 'O', 'O'), ('urkundlich', 'O', 'O'), ('erwahnt', 'O', 'O'), ('ist', 'O', 'O'), ('nimburg', 'B-LOC', 'O'), ('bereits', 'O', 'O'), ('im', 'O', 'O'), ('jahre', 'O', 'O'), ('977', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.394925</td>\n",
       "      <td>0.379584</td>\n",
       "      <td>0.900767</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Zugang', 'O', 'O'), ('und', 'O', 'O'), ('Engagement', 'O', 'O'), (':', 'O', 'O'), ('das', 'O', 'O'), ('eigentlich', 'O', 'O'), ('Neue', 'O', 'O'), ('an', 'O', 'O'), ('der', 'O', 'O'), ('Netz(', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Außerdem', 'O', 'O'), ('befindet', 'O', 'O'), ('sich', 'O', 'O'), ('im', 'O', 'O'), ('Nordwesten', 'O', 'O'), ('der', 'O', 'O'), ('Stadt', 'O', 'O'), ('(', 'O', 'O'), ('auf', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-ende-1024 ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45098ef69ed8425cbab69bbdb8901f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1020.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4151f524154c8eb8b561a5bca6adad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1443538.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412268f2c8cc4a4cbf3c0b7afd33614b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1003294.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5a067702f14887a68a55a32b3dd403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=834712214.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.496922</td>\n",
       "      <td>0.547081</td>\n",
       "      <td>0.897001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't calcualte classification report: Weights sum to zero, can't be normalized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('nach', 'O', 'O'), ('seiner', 'O', 'O'), ('ruckkehr', 'O', 'O'), ('hielt', 'O', 'O'), ('strummer', 'B-PER', 'O'), ('ein', 'O', 'O'), ('bandmeeting', 'O', 'O'), ('ab', 'O', 'O'), (',', 'O', 'O'), ('in', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('wenn', 'O', 'O'), ('kleine', 'O', 'O'), ('gymnasiasten', 'O', 'O'), ('schon', 'O', 'O'), ('in', 'O', 'O'), ('der', 'O', 'O'), ('funften', 'O', 'O'), ('klasse', 'O', 'O'), ('mit', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.265464</td>\n",
       "      <td>0.266505</td>\n",
       "      <td>0.936125</td>\n",
       "      <td>0.439462</td>\n",
       "      <td>0.420601</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('(', 'O', 'O'), ('Standard', 'B-ORG', 'O'), ('Oil', 'I-ORG', 'B-ORG'), ('of', 'I-ORG', 'B-ORG'), ('New', 'I-ORG', 'B-ORG'), ('Jersey', 'I-ORG', 'B-LOC'), (')', 'O', 'B-LOC'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ausgesprochen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'O'), ('einem', 'O', 'O'), ('Telegramm', 'O', 'O'), ('an', 'O', 'O'), ('US-Präsident', 'B-LOCpart', 'O'), ('George', 'B-PER', 'O'), ('W.Bush', 'I-PER', 'B-ORG'), ('drückte', 'O', 'O'), ('er', 'O', 'O'), ('im', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.408323</td>\n",
       "      <td>0.369326</td>\n",
       "      <td>0.907382</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'O'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'O'), ('1994', 'O', 'O'), (')', 'O', 'O'), ('S.', 'O', 'O'), ('593.', 'O', 'O'), ('Wink', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Mit', 'O', 'O'), ('der', 'O', 'O'), ('Servicefrau', 'O', 'O'), ('verband', 'O', 'O'), ('Bianca', 'B-PER', 'O'), ('offenbar', 'O', 'O'), ('eine', 'O', 'O'), ('Art', 'O', 'O'), ('Freundschaft', 'O', 'O'), ('–', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "bsz = 2\n",
    "seq_sz = 32\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config=config)\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n')\n",
    "    \n",
    "    before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_tokenizer, \n",
    "                                                         max_length=seq_sz,\n",
    "                                                         padding='max_length',\n",
    "                                                         is_split_into_words=True, \n",
    "                                                         tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "    blocks = (\n",
    "        HF_TextBlock(before_batch_tfms=before_batch_tfm, input_return_type=HF_TokenClassInput), \n",
    "        HF_TokenCategoryBlock(vocab=labels)\n",
    "    )\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('tokens'),\n",
    "                       get_y= lambda inp: [ \n",
    "                           (label, len(hf_tokenizer.tokenize(str(entity)))) \n",
    "                           for entity, label in zip(inp.tokens, inp.labels) \n",
    "                       ],\n",
    "                       splitter=RandomSplitter())\n",
    "    \n",
    "    dls = dblock.dataloaders(germ_eval_df, bs=bsz)\n",
    "\n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                cbs=[HF_TokenClassCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.unfreeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(len(preds[0]), bsz)\n",
    "        test_eq(preds[0].shape, torch.Size([bsz, seq_sz, len(labels)]))\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max= 3e-5, moms=(0.8,0.7,0.8))\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizer</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizer</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizer</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizer</td>\n",
       "      <td>DistilBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizer</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizer</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizer</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizer</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model_name', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01za_data-text2text-core.ipynb.\n",
      "Converted 01zb_data-text2text-language-modeling.ipynb.\n",
      "Converted 01zc_data-text2text-summarization.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02za_modeling-text2text-core.ipynb.\n",
      "Converted 02zb_modeling-text2text-language-modeling.ipynb.\n",
      "Converted 02zc_modeling-text2text-summarization.ipynb.\n",
      "Converted 99a_examples-multilabel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
