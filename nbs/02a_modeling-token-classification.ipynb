{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks like named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast, torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *\n",
    "from blurr.data.token_classification import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "from seqeval import metrics as seq_metrics\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.7.1+cu110\n",
      "Using fastai 2.2.7\n",
      "Using transformers 4.3.3\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image. Named entity recognition (NER) is an example of token classification in the NLP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>nested-labels</th>\n",
       "      <th>ds_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n-tv.de vom 26.02.2005 [2005-02-26]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>welt.de vom 29.10.2005 [2005-10-29]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&amp;utm_medium=rss-feed&amp;utm_campaign=sport [2010-03-25]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stern.de vom 21.03.2006 [2006-03-21]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                                                                        source  \\\n",
       "0                                                                                                                                                         n-tv.de vom 26.02.2005 [2005-02-26]    \n",
       "1                                                                                                                                                         welt.de vom 29.10.2005 [2005-10-29]    \n",
       "2  http://www.stern.de/sport/fussball/krawalle-in-der-fussball-bundesliga-dfb-setzt-auf-falsche-konzepte-1553657.html#utm_source=standard&utm_medium=rss-feed&utm_campaign=sport [2010-03-25]    \n",
       "3                                                                                                                                                        stern.de vom 21.03.2006 [2006-03-21]    \n",
       "4                                                                         http://www.fr-online.de/in_und_ausland/sport/aktuell/1618625_Frings-schaut-finster-in-die-Zukunft.html [2008-10-24]    \n",
       "\n",
       "                                                                                                                                                                  tokens  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                 nested-labels  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "  ds_type  \n",
       "0   train  \n",
       "1   train  \n",
       "2   train  \n",
       "3   train  \n",
       "4   train  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensures these cols are represented as lists (rather than string)\n",
    "df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval, 'nested-labels': ast.literal_eval}\n",
    "\n",
    "# full nlp dataset\n",
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner_cleaned.csv', converters=df_converters)\n",
    "\n",
    "# demo nlp dataset\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv', converters=df_converters)\n",
    "\n",
    "print(len(germ_eval_df))\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to be working with small sample from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) data set ... so the results might not be all that great :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-OTHderiv', 'B-OTHpart', 'B-PER', 'B-PERderiv', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.models.bert.configuration_bert.BertConfig,\n",
       " transformers.models.bert.tokenization_bert_fast.BertTokenizerFast,\n",
       " transformers.models.bert.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                     is_split_into_words=True, \n",
    "                                                     tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('tokens'),\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a `get_y` that creates the same number of labels as there are subtokens for a particular token. For example, my name \"Wayde\" gets split up into two subtokens, \"Way\" and \"##de\". The label for \"Wayde\" is \"B-PER\" and we just repeat it for the subtokens.  This all get cleaned up when we show results and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Zugang', 'O'), ('und', 'O'), ('Engagement', 'O'), (':', 'O'), ('das', 'O'), ('eigentlich', 'O'), ('Neue', 'O'), ('an', 'O'), ('der', 'O'), ('Netz', 'O'), ('(', 'O'), ('werk', 'O'), (')', 'O'), ('kunst,', 'O'), ('in', 'O'), (':', 'O'), ('Medien', 'O'), ('Kunst', 'O'), ('Netz,', 'O'), ('2004,', 'O'), ('URL', 'O'), (':', 'O'), ('*', 'O'), ('Arns,', 'B-PER'), ('Inke', 'O'), (':', 'B-PER'), ('Netzkulturen,', 'O'), ('Hamburg', 'O'), ('(', 'O'), ('eva', 'B-LOC'), ('),', 'O'), ('2002,', 'O'), ('S.', 'O'), ('46', 'O'), ('und', 'O'), ('81', 'O'), ('*', 'O'), ('Armin', 'O'), ('Medosch', 'O'), (':', 'O'), ('Public', 'B-PER'), ('Netbase', 'I-PER'), ('Wien.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Mit', 'O'), ('der', 'O'), ('Servicefrau', 'O'), ('verband', 'O'), ('Bianca', 'B-PER'), ('offenbar', 'O'), ('eine', 'O'), ('Art', 'O'), ('Freundschaft', 'O'), ('oder', 'O'), ('wenigstens', 'O'), ('wünschte', 'O'), ('sich', 'O'), ('das', 'O'), ('Bianca', 'O'), ('B.', 'B-PER'), ('«', 'I-PER'), ('Sie', 'O'), ('gab', 'O'), ('mir', 'O'), ('immer', 'O'), ('wieder', 'O'), ('Geld', 'O'), ('oder', 'O'), ('kleine', 'O'), ('Geschenke', 'O'), ('»,', 'O'), ('so', 'O'), ('L.', 'O'), ('Doch', 'O'), ('gleichzeitig', 'B-PER'), ('sei', 'O'), ('Bianca', 'O'), ('B.', 'O'), ('ihr', 'B-PER'), ('gegenüber', 'I-PER'), ('nicht', 'O'), ('ehrlich', 'O'), ('gewesen.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if (metric_key == 'accuracy'): return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'precision'): return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'recall'): return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "    if (metric_key == 'f1'): return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "        \n",
    "    if (metric_key == 'classification_report'): return seq_metrics.classification_report(targ_toks, pred_toks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(filter(lambda el: isinstance(el, HF_TokenCategorize), learn.dls.tfms[1]), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in learn.dls.tfms[1]: print(isinstance(x,HF_TokenCategorize), type(x), HF_TokenCategorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdf = HF_TokenCategorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isinstance(asdf,HF_TokenCategorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_blurr_tfm(learn.dls.tfms, tfm_class=HF_TokenCategorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "    \n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "        \n",
    "        store_attr(self=self, names='tok_metrics, kwargs')\n",
    "        self.custom_metrics_dict = { k:None for k in tok_metrics }\n",
    "        \n",
    "        self.do_setup = True\n",
    "        \n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if (not self.do_setup): return\n",
    "        \n",
    "        # grab the hf_tokenizer from the HF_TokenClassBeforeBatchTransform\n",
    "        hf_before_batch_tfm = get_blurr_tfm(self.learn.dls.before_batch)\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=HF_TokenCategorize)\n",
    "        \n",
    "        self.hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = hf_before_batch_tfm.kwargs\n",
    "        \n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "        \n",
    "        self.do_setup = False\n",
    "        \n",
    "    def before_fit(self): self.setup()\n",
    "    \n",
    "    \n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        if (self.training or self.learn.y is None): return\n",
    "        \n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0] # yb is TensorText tuple, item 0 is the data\n",
    "        \n",
    "        preds_list, targets_list = [], []   \n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "            \n",
    "            for j in range(targs.shape[1]):\n",
    "                if (targs[i, j] != self.ignore_label_token_id):\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "                    \n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "            \n",
    "        self.results += [ (res[0], res[1]) for res in zip(preds_list, targets_list) ]\n",
    "        \n",
    "        \n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self): self.results = []\n",
    "        \n",
    "    def after_validate(self):\n",
    "        if (len(self.results) < 1): return\n",
    "        \n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys(): \n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "        \n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, \n",
    "                                                                                   preds, \n",
    "                                                                                   'classification_report')\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f'Couldn\\'t calcualte classification report: {err}')\n",
    "        \n",
    "        \n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "learn_cbs = [HF_BaseModelCallback]\n",
    "fit_cbs = [HF_TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model,opt_func=partial(Adam),cbs=learn_cbs,splitter=hf_splitter)\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([2, 61, 18]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([2, 61]), 2, torch.Size([2, 61]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0]['input_ids'].shape, len(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122, 18]) torch.Size([122])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0009120108559727668, lr_steep=3.0199516913853586e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.unfreeze()\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.219978</td>\n",
       "      <td>0.191613</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.545802</td>\n",
       "      <td>0.556420</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.100652</td>\n",
       "      <td>0.147648</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.681992</td>\n",
       "      <td>0.693957</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053306</td>\n",
       "      <td>0.151991</td>\n",
       "      <td>0.954413</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.611296</td>\n",
       "      <td>0.665461</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max= 3e-5, moms=(0.8,0.7,0.8), cbs=fit_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.80      0.65      0.72        75\n",
      "    LOCderiv       0.79      0.79      0.79        29\n",
      "     LOCpart       0.25      0.50      0.33         2\n",
      "         ORG       0.75      0.49      0.60        79\n",
      "     ORGpart       0.20      1.00      0.33         2\n",
      "         OTH       0.43      0.30      0.35        40\n",
      "    OTHderiv       0.00      0.00      0.00         0\n",
      "     OTHpart       0.00      0.00      0.00         0\n",
      "         PER       0.92      0.78      0.85        74\n",
      "     PERpart       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.73      0.61      0.67       301\n",
      "   macro avg       0.41      0.45      0.40       301\n",
      "weighted avg       0.76      0.61      0.67       301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "print(learn.token_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, learner, \n",
    "                 ctxs=None, max_n=6, trunc_at=None, **kwargs):    \n",
    "    \n",
    "    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)\n",
    "    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "    ignore_token_id = hf_before_batch_tfm.ignore_token_id\n",
    "    \n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # recontstruct the string and split on space to get back your pre-tokenized list of tokens\n",
    "        toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=True)\n",
    "        pretokenized_toks =  hf_tokenizer.convert_tokens_to_string(toks).split()\n",
    "        \n",
    "        # get predictions for subtokens that aren't ignored (e.g. special toks and token parts)\n",
    "        pred_labels = [ pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != -ignore_token_id ]\n",
    "        \n",
    "        trg_labels = ast.literal_eval(sample[1])\n",
    "        res.append([f'{[ (tok, trg, pred) for idx, (tok, pred, trg) in enumerate(zip(pretokenized_toks, pred_labels, trg_labels)) if (trunc_at is None or idx < trunc_at) ]}'])\n",
    "        \n",
    "    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'O'), ('et', 'I-OTH', 'B-PER'), ('al.', 'I-OTH', 'I-PER'), ('(', 'O', 'I-PER'), ('1994', 'O', 'I-PER'), (')', 'O', 'I-PER'), ('S.', 'O', 'O'), ('593.', 'O', 'O'), ('Wink', 'O', 'O'), ('&amp;', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Der', 'O', 'O'), ('28', 'O', 'O'), ('-', 'O', 'O'), ('Jährige', 'O', 'O'), ('und', 'O', 'O'), ('sein', 'O', 'O'), ('Team,', 'O', 'O'), ('zu', 'O', 'O'), ('dem', 'O', 'O'), ('auch', 'B-PER', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict('My name is Wayde and I live in San Diego'.split())\n",
    "print(res[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _blurr_predict_tokens(predict_func, items, hf_before_batch_tfm):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.blurr_predict` or `blurrONNX.predict.\n",
    "    Aligns the predicted labels, label ids, and probabilities with what you passed in excluding subword tokens\n",
    "    \"\"\"\n",
    "    # grab the huggingface tokenizer from the learner's dls.tfms\n",
    "    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "    tok_kwargs = hf_before_batch_tfm.tok_kwargs\n",
    "    \n",
    "    if (isinstance(items[0], str)): items = [items]\n",
    "        \n",
    "    outs = []\n",
    "    for inp, res in zip(items, predict_func(items)):\n",
    "        # blurr_predict returns a list for each, we only doing one at a time so git first element of each\n",
    "        pred_lbls, pred_lbl_ids, probs = res[0][0], res[1][0], res[2][0]\n",
    "  \n",
    "        # calculate the number of subtokens per raw/input token so that we can determine what predictions to\n",
    "        # return\n",
    "        subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp ]\n",
    "    \n",
    "        # very similar to what HF_BatchTransform does with the exception that we are also grabbing\n",
    "        # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens\n",
    "        # (e.g., [CLS], [SEP], etc...)\n",
    "        res = hf_tokenizer(inp, None, \n",
    "                           max_length=hf_before_batch_tfm.max_length,\n",
    "                           padding=hf_before_batch_tfm.padding,\n",
    "                           truncation=hf_before_batch_tfm.truncation,\n",
    "                           is_split_into_words=hf_before_batch_tfm.is_split_into_words,\n",
    "                           **tok_kwargs)\n",
    "\n",
    "        special_toks_msk = L(res['special_tokens_mask'])\n",
    "        actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "    \n",
    "        # using the indexes to the actual tokens, get that info from the results returned above\n",
    "        pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "        actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "        actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "        actual_probs = probs[actual_tok_idxs]\n",
    "    \n",
    "        # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "        # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "        offset = 0\n",
    "        raw_trg_idxs = []\n",
    "        for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok): \n",
    "            raw_trg_idxs.append(idx+offset)\n",
    "            offset += sub_tok_count-1 if (sub_tok_count > 1) else 0\n",
    "            \n",
    "        outs.append((inp, \n",
    "                     actual_pred_lbls[raw_trg_idxs], \n",
    "                     actual_pred_lbl_ids[raw_trg_idxs], \n",
    "                     actual_probs[raw_trg_idxs]))\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict_tokens(self:Learner, items, **kargs):\n",
    "    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n",
    "    return _blurr_predict_tokens(self.blurr_predict, items, hf_before_batch_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**, **\\*\\*`kargs`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'B-PER'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-ORG')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (and very cool) how well this model performs on English even thought it was trained against a German corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = 'tok_class_learn_export'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'B-PER'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-ORG')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f'{export_fname}.pkl')\n",
    "inf_learn = load_learner(fname=f'{export_fname}.pkl')\n",
    "\n",
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# @patch\n",
    "# def predict_tokens(self:blurrONNX, items, **kargs):\n",
    "#     hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n",
    "#     return _blurr_predict_tokens(self.predict, items, hf_before_batch_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow\n",
    "# learn.blurr_to_onnx(export_fname, quantize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow\n",
    "# onnx_inf = blurrONNX(export_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow\n",
    "# res = onnx_inf.predict_tokens(txt.split())\n",
    "# for r in res: print(f'{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.models.albert.modeling_albert.AlbertForTokenClassification,\n",
       " transformers.models.auto.modeling_auto.AutoModelForTokenClassification,\n",
       " transformers.models.bert.modeling_bert.BertForTokenClassification,\n",
       " transformers.models.camembert.modeling_camembert.CamembertForTokenClassification,\n",
       " transformers.models.convbert.modeling_convbert.ConvBertForTokenClassification,\n",
       " transformers.models.deberta.modeling_deberta.DebertaForTokenClassification,\n",
       " transformers.models.distilbert.modeling_distilbert.DistilBertForTokenClassification,\n",
       " transformers.models.electra.modeling_electra.ElectraForTokenClassification,\n",
       " transformers.models.flaubert.modeling_flaubert.FlaubertForTokenClassification,\n",
       " transformers.models.funnel.modeling_funnel.FunnelForTokenClassification,\n",
       " transformers.models.layoutlm.modeling_layoutlm.LayoutLMForTokenClassification,\n",
       " transformers.models.longformer.modeling_longformer.LongformerForTokenClassification,\n",
       " transformers.models.mpnet.modeling_mpnet.MPNetForTokenClassification,\n",
       " transformers.models.mobilebert.modeling_mobilebert.MobileBertForTokenClassification,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification,\n",
       " transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForTokenClassification,\n",
       " transformers.models.xlm.modeling_xlm.XLMForTokenClassification,\n",
       " transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForTokenClassification,\n",
       " transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ model_type for model_type in BLURR_MODEL_HELPER.get_models(task='TokenClassification') \n",
    " if (not model_type.__name__.startswith('TF')) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'bert-base-multilingual-cased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'google/electra-small-discriminator',\n",
    "    'flaubert/flaubert_small_cased',\n",
    "    'huggingface/funnel-small-base',\n",
    "    'allenai/longformer-base-4096',\n",
    "    'microsoft/mpnet-base',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'squeezebert/squeezebert-uncased',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.355603</td>\n",
       "      <td>1.254349</td>\n",
       "      <td>0.890860</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('helbig', 'B-OTH', 'B-OTH'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'O'), ('1994', 'O', 'O'), (')', 'O', 'O'), ('s.', 'O', 'O'), ('593.', 'O', 'O'), ('wink', 'B-OTH', 'B-LOCderiv')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('zugang', 'O', 'O'), ('und', 'O', 'O'), ('engagement', 'O', 'O'), (':', 'O', 'O'), ('das', 'O', 'O'), ('eigentlich', 'O', 'O'), ('neue', 'O', 'O'), ('an', 'O', 'O'), ('der', 'O', 'O'), ('netz(', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-multilingual-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.018331</td>\n",
       "      <td>1.827816</td>\n",
       "      <td>0.896117</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Mit', 'O', 'O'), ('der', 'O', 'O'), ('Servicefrau', 'O', 'O'), ('verband', 'O', 'O'), ('Bianca', 'B-PER', 'O'), ('offenbar', 'O', 'O'), ('eine', 'O', 'O'), ('Art', 'O', 'O'), ('Freundschaft', 'O', 'O'), ('oder', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Das', 'O', 'B-OTH'), ('\"', 'O', 'O'), ('Torchwood', 'B-OTH', 'O'), ('\"', 'O', 'O'), ('-', 'O', 'O'), ('Team', 'O', 'O'), ('besteht', 'O', 'O'), ('neben', 'O', 'O'), ('Captain', 'B-PER', 'O'), ('Jack', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.354326</td>\n",
       "      <td>2.300568</td>\n",
       "      <td>0.908685</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('NEWSru.ua', 'B-OTH', 'O'), ('/', 'O', 'O'), (':', 'O', 'O'), ('&lt;unk&gt;о&lt;unk&gt;о&lt;unk&gt;о&lt;unk&gt;', 'B-OTH', 'O'), ('&lt;unk&gt;', 'I-OTH', 'O'), ('о&lt;unk&gt;а&lt;unk&gt;а&lt;unk&gt;', 'I-OTH', 'O'), ('&lt;unk&gt;о&lt;unk&gt;о&lt;unk&gt;', 'I-OTH', 'O'), ('&lt;unk&gt;', 'I-OTH', 'O'), ('о&lt;unk&gt;а&lt;unk&gt;', 'I-OTH', 'O'), ('Die', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Au&lt;unk&gt;erdem', 'O', 'O'), ('befindet', 'O', 'O'), ('sich', 'O', 'O'), ('im', 'O', 'O'), ('Nordwesten', 'O', 'O'), ('der', 'O', 'O'), ('Stadt', 'O', 'O'), ('(', 'O', 'O'), ('auf', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n",
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.820278</td>\n",
       "      <td>1.700890</td>\n",
       "      <td>0.893623</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('mit', 'O', 'O'), ('der', 'O', 'O'), ('servicefrau', 'O', 'O'), ('verband', 'O', 'O'), ('bianca', 'B-PER', 'O'), ('offenbar', 'O', 'O'), ('eine', 'O', 'O'), ('art', 'O', 'O'), ('freundschaft', 'O', 'O'), ('–', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('senden', 'O', 'O'), ('exxon', 'B-ORG', 'O'), ('mobil', 'I-ORG', 'O'), ('\"', 'O', 'O'), ('buy', 'O', 'I-LOC'), ('\"', 'O', 'B-OTH'), ('paris', 'B-LOC', 'O'), ('(', 'O', 'O'), ('aktiencheck.', 'B-ORG', 'O'), ('de', 'I-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/electra-small-discriminator ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.381443</td>\n",
       "      <td>2.296770</td>\n",
       "      <td>0.826150</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('außerdem', 'O', 'B-OTHderiv'), ('befindet', 'O', 'O'), ('sich', 'O', 'I-ORG'), ('im', 'O', 'O'), ('nordwesten', 'O', 'O'), ('der', 'O', 'O'), ('stadt', 'O', 'O'), ('(', 'O', 'O'), ('auf', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('senden', 'O', 'O'), ('exxon', 'B-ORG', 'O'), ('mobil', 'I-ORG', 'O'), ('\"', 'O', 'I-ORG'), ('buy', 'O', 'I-ORG'), ('\"', 'O', 'B-LOC'), ('paris', 'B-LOC', 'B-LOC'), ('(', 'O', 'B-OTHpart'), ('aktiencheck.', 'B-ORG', 'O'), ('de', 'I-ORG', 'B-OTHpart')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n",
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.713634</td>\n",
       "      <td>1.256108</td>\n",
       "      <td>0.888920</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Helbig', 'B-OTH', 'O'), ('et', 'I-OTH', 'O'), ('al', 'I-OTH', 'O'), ('.', 'O', 'O'), ('(', 'O', 'O'), ('1994', 'O', 'B-ORG'), (')', 'O', 'O'), ('S.', 'O', 'O'), ('593', 'O', 'O'), ('.', 'B-OTH', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Mit', 'O', 'O'), ('der', 'O', 'O'), ('Servicefrau', 'O', 'O'), ('verband', 'O', 'O'), ('Bianca', 'B-PER', 'O'), ('offenbar', 'O', 'O'), ('eine', 'O', 'O'), ('Art', 'O', 'O'), ('Freundschaft', 'O', 'O'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.945749</td>\n",
       "      <td>0.778169</td>\n",
       "      <td>0.882304</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('scenes', 'B-OTH', 'B-LOC'), ('of', 'I-OTH', 'O'), ('a', 'I-OTH', 'O'), ('sexual', 'I-OTH', 'O'), ('nature', 'I-OTH', 'O'), ('(', 'O', 'I-ORGpart'), ('gb', 'O', 'I-ORGpart'), ('2006', 'O', 'O'), (')', 'O', 'B-ORGpart'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('newsru.', 'B-OTH', 'B-LOC'), ('ua', 'O', 'O'), ('/', 'O', 'O'), (':', 'B-OTH', 'O'), ('политисполком', 'I-OTH', 'O'), ('спу', 'I-OTH', 'O'), ('отказал', 'I-OTH', 'O'), ('морозу', 'I-OTH', 'O'), ('в', 'I-OTH', 'O'), ('отставке', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.468229</td>\n",
       "      <td>2.381964</td>\n",
       "      <td>0.594662</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Scenes', 'B-OTH', 'O'), ('of', 'I-OTH', 'B-PERpart'), ('a', 'I-OTH', 'B-PERpart'), ('Sexual', 'I-OTH', 'B-PERpart'), ('Nature', 'I-OTH', 'B-PERpart'), ('(', 'O', 'B-PERpart'), ('GB', 'O', 'B-PERpart'), ('2006', 'O', 'B-PERpart'), (')', 'O', 'B-PERpart'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Die', 'O', 'O'), ('Flügel', 'O', 'O'), ('Die', 'O', 'B-PERpart'), ('geöffneten', 'O', 'B-PERpart'), ('Flügel', 'O', 'O'), ('zeigen', 'O', 'O'), ('in', 'O', 'B-PERpart'), ('vier', 'O', 'B-PERpart'), ('Szenen', 'O', 'O'), ('Höhepunkte', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.334105</td>\n",
       "      <td>2.291434</td>\n",
       "      <td>0.892274</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('der', 'O', 'O'), ('28', 'O', 'O'), ('-', 'O', 'O'), ('jahrige', 'O', 'O'), ('und', 'O', 'O'), ('sein', 'O', 'O'), ('team,', 'O', 'O'), ('zu', 'O', 'O'), ('dem', 'O', 'O'), ('auch', 'B-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('eine', 'O', 'O'), ('armee', 'O', 'O'), ('von', 'O', 'O'), ('160', 'O', 'O'), ('000', 'O', 'O'), ('bis', 'O', 'O'), ('180', 'O', 'O'), ('000', 'O', 'O'), ('berufs', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.882904</td>\n",
       "      <td>5.907832</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('nach', 'O', 'B-LOC'), ('seiner', 'O', 'B-ORG'), ('ruckkehr', 'O', 'B-LOCpart'), ('hielt', 'O', 'B-OTHderiv'), ('strummer', 'B-PER', 'B-PER'), ('ein', 'O', 'B-PERpart'), ('bandmeeting', 'O', 'I-OTH'), ('ab,', 'O', 'B-OTH'), ('in', 'O', 'B-PER'), ('dem', 'O', 'B-OTHderiv')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('newsru.', 'B-OTH', 'B-LOC'), ('ua', 'O', 'B-PER'), ('/', 'O', 'I-ORGpart'), (':', 'B-OTH', 'I-ORGpart'), ('политисполком', 'I-OTH', 'B-PERderiv'), ('спу', 'I-OTH', 'I-ORGpart'), ('отказал', 'I-OTH', 'B-PERpart'), ('морозу', 'I-OTH', 'I-LOC'), ('в', 'I-OTH', 'I-ORGpart'), ('отставке', 'O', 'I-ORGpart')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.146287</td>\n",
       "      <td>2.082637</td>\n",
       "      <td>0.888826</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('In', 'O', 'O'), ('einer', 'O', 'O'), ('Fernsehdiskussion', 'O', 'O'), ('traf', 'O', 'O'), ('er', 'O', 'O'), ('auf', 'O', 'O'), ('den', 'O', 'O'), ('Kritiker', 'O', 'O'), ('Alexander', 'B-PER', 'O'), ('Walker', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Wie', 'O', 'O'), ('wenige', 'O', 'O'), ('Wochen', 'O', 'O'), ('vor', 'O', 'O'), ('der', 'O', 'O'), ('Bekanntgabe', 'O', 'O'), ('in', 'O', 'O'), ('einem', 'O', 'O'), ('Interview', 'O', 'O'), ('angekündigt', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.456370</td>\n",
       "      <td>2.388646</td>\n",
       "      <td>0.909446</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('außerdem', 'O', 'O'), ('befindet', 'O', 'O'), ('sich', 'O', 'O'), ('im', 'O', 'O'), ('nordwesten', 'O', 'O'), ('der', 'O', 'O'), ('stadt', 'O', 'O'), ('(', 'O', 'O'), ('auf', 'O', 'O'), ('dem', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('jeder', 'O', 'O'), ('rest', 'O', 'O'), ('von', 'O', 'O'), ('darstellung,', 'O', 'O'), ('bildlich', 'O', 'O'), ('oder', 'O', 'O'), ('figurlich,', 'O', 'O'), ('wurde', 'O', 'O'), ('abgelehnt', 'O', 'O'), (':', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n",
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.797168</td>\n",
       "      <td>0.792537</td>\n",
       "      <td>0.883206</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('(', 'O', 'O'), ('standard', 'B-ORG', 'O'), ('oil', 'I-ORG', 'O'), ('of', 'I-ORG', 'O'), ('new', 'I-ORG', 'O'), ('jersey', 'I-ORG', 'O'), (')', 'O', 'O'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ausgesprochen', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('mit', 'O', 'O'), ('der', 'O', 'O'), ('servicefrau', 'O', 'O'), ('verband', 'O', 'O'), ('bianca', 'B-PER', 'O'), ('offenbar', 'O', 'O'), ('eine', 'O', 'O'), ('art', 'O', 'O'), ('freundschaft', 'O', 'O'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.355332</td>\n",
       "      <td>2.292146</td>\n",
       "      <td>0.868674</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('(', 'O', 'I-PER'), ('Standard', 'B-ORG', 'O'), ('Oil', 'I-ORG', 'O'), ('of', 'I-ORG', 'O'), ('New', 'I-ORG', 'O'), ('Jersey', 'I-ORG', 'O'), (')', 'O', 'I-OTH'), (',', 'O', 'O'), ('die', 'O', 'O'), ('ausgesprochen', 'O', 'I-OTH')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-OTH'), ('Eine', 'O', 'O'), ('Armee', 'O', 'O'), ('von', 'O', 'O'), ('160', 'O', 'O'), ('000', 'O', 'O'), ('bis', 'O', 'O'), ('180', 'O', 'O'), ('000', 'O', 'O'), ('Berufs-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.724691</td>\n",
       "      <td>0.649909</td>\n",
       "      <td>0.881342</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Scenes', 'B-OTH', 'O'), ('of', 'I-OTH', 'O'), ('a', 'I-OTH', 'B-LOCpart'), ('Sexual', 'I-OTH', 'O'), ('Nature', 'I-OTH', 'O'), ('(', 'O', 'O'), ('GB', 'O', 'O'), ('2006', 'O', 'B-LOCpart'), (')', 'O', 'O'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Das', 'O', 'O'), ('Triebwerk', 'O', 'O'), ('des', 'O', 'O'), ('Hydrogen', 'B-OTH', 'O'), ('7', 'I-OTH', 'O'), ('leistet', 'O', 'O'), ('letztendlich', 'O', 'O'), ('191', 'O', 'O'), ('kW', 'O', 'O'), ('(', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.TokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config=config)\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n')\n",
    "    \n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if (hf_tokenizer.pad_token is None): \n",
    "        hf_tokenizer.add_special_tokens({'pad_token': '<pad>'})  \n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()['<pad>']\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer)) \n",
    "    \n",
    "    before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                         max_length=seq_sz,\n",
    "                                                         padding='max_length',\n",
    "                                                         is_split_into_words=True, \n",
    "                                                         tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "    blocks = (\n",
    "        HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n",
    "        HF_TokenCategoryBlock(vocab=labels)\n",
    "    )\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('tokens'),\n",
    "                       get_y= lambda inp: [ \n",
    "                           (label, len(hf_tokenizer.tokenize(str(entity)))) \n",
    "                           for entity, label in zip(inp.tokens, inp.labels) \n",
    "                       ],\n",
    "                       splitter=RandomSplitter())\n",
    "    \n",
    "    dls = dblock.dataloaders(germ_eval_df, bs=bsz)\n",
    "\n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter).to_fp16()\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max= 3e-5, moms=(0.8,0.7,0.8), \n",
    "                            cbs=[HF_TokenClassMetricsCallback(tok_metrics=['accuracy'])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "        \n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "        \n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizerFast</td>\n",
       "      <td>DistilBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model_name', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01za_data-seq2seq-core.ipynb.\n",
      "Converted 01zb_data-seq2seq-language-modeling.ipynb.\n",
      "Converted 01zc_data-seq2seq-summarization.ipynb.\n",
      "Converted 01zd_data-seq2seq-translation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02za_modeling-seq2seq-core.ipynb.\n",
      "Converted 02zb_modeling-seq2seq-language-modeling.ipynb.\n",
      "Converted 02zc_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 02zc_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-multilabel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
