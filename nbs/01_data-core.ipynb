{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.core\n",
    "\n",
    "> This module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, inspect\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce, partial\n",
    "from typing import Any, Callable, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.core import Datasets, DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import DataCollatorWithPadding, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.7.1\n",
      "fastai: 2.5.3\n",
      "transformers: 4.13.0\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API: Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_BaseInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    def show(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The \"context\" associated to the current `show_batch/results` call\n",
    "        ctx=None,\n",
    "        # Any truncation you want to apply to the decoded tokenized inputs\n",
    "        trunc_at: int = None,\n",
    "        # A decoded string of your tokenized inputs (input_ids)\n",
    "    ) -> str:\n",
    "        input_ids = self.cpu().numpy()\n",
    "        decoded_input = str(hf_tokenizer.decode(input_ids, skip_special_tokens=True))[:trunc_at]\n",
    "\n",
    "        return show_title(decoded_input, ctx=ctx, label=\"text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `HF_BaseInput` object is returned from the decodes method of `HF_AfterBatchTransform` as a means to customize @typedispatched functions like `DataLoaders.show_batch` and `Learner.show_results`. It uses the \"input_ids\" of a Hugging Face object as the representative tensor for `show` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_BeforeBatchTransform(Transform):\n",
    "    \"\"\"Handles everything you need to assemble a mini-batch of inputs and targets, as well as \n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `HF_BeforeBatchTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        store_attr(self=self, names=\"hf_arch, hf_config, hf_tokenizer, hf_model\")\n",
    "        store_attr(self=self, names=\"max_length, padding, truncation, is_split_into_words, tok_kwargs\")\n",
    "        store_attr(self=self, names=\"kwargs\")\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding = False):  # A subset of data to put into a mini-batch\n",
    "        \"\"\"This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        if is_listy(samples[0][0]) and not self.is_split_into_words:\n",
    "            inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = samples.itemgot(0).items\n",
    "\n",
    "        # tokenize\n",
    "        tok_d = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs\n",
    "        )\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), ensureing that if \n",
    "        # \"overflow_to_sample_mapping\" = True we include each sample chunk\n",
    "        d_keys = tok_d.keys()\n",
    "        updated_samples = []\n",
    "        if (\"overflow_to_sample_mapping\" in d_keys):\n",
    "            for idx, seq_idx in enumerate(tok_d[\"overflow_to_sample_mapping\"]):\n",
    "                s = (*[{k: tok_d[k][idx] for k in d_keys}], *samples[seq_idx][1:])\n",
    "                updated_samples.append(s)\n",
    "        else:\n",
    "            updated_samples = [(*[{k: tok_d[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "\n",
    "        if (return_batch_encoding):\n",
    "            return updated_samples, tok_d\n",
    "            \n",
    "        return updated_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_BeforeBatchTransform` was inspired by this [article](https://docs.fast.ai/tutorial.transformers.html).\n",
    "\n",
    "Inputs can come in as a string or a list of tokens, the later being for tasks like Named Entity Recognition (NER), where you want to predict the label of each token.\n",
    "\n",
    "**Notes re: on-the-fly batch-time tokenization**: The previous version of the library performed the tokenization/numericalization as a type transform when the raw data was read, and included a couple batch transforms to prepare the data for collation (e.g., to be made into a mini-batch). With this update, everything is done in a single batch transform.  Why?  Part of the inspiration had to do with the mechanics of the huggingrace tokenizer, in particular how by default it returns a collated mini-batch of data given a list of sequences. And where do we get a list of examples with fastai? In the batch transforms!  So I thought, hey, why not do everything dynamically at batch time?  And with a bit of tweaking, I got everything to work pretty well.  The result is less code, faster mini-batch creation, less RAM utilization and time spent tokenizing (really helps with very large datasets), and more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_AfterBatchTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs into something understandable in fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = HF_BaseInput,\n",
    "    ):\n",
    "        store_attr(self=self, names=\"hf_tokenizer, input_return_type\")\n",
    "\n",
    "    def decodes(\n",
    "        self,\n",
    "        # The encoded samples for your batch. `input_ids` will be pulled out of your dictionary of Hugging Face\n",
    "        # inputs, cast to `self.input_return_type` and returned for methods such as `show_batch`\n",
    "        encoded_samples: Type,\n",
    "    ):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        if isinstance(encoded_samples, dict):\n",
    "            return self.input_return_type(encoded_samples[\"input_ids\"], hf_tokenizer=self.hf_tokenizer)\n",
    "        return encoded_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `HF_AfterBatchTransform`, that will do the decoding for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def blurr_sort_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "    # if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    if is_split_into_words:\n",
    "        return len(example[0])\n",
    "    return len(hf_tokenizer.tokenize(example[0], **tok_kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TfmdDL)\n",
    "class OverflowDL(SortedDL):\n",
    "    def __init__(self, dataset, sort_func=None, res=None, overflow_map_key=\"overflow_to_sample_mapping\", **kwargs):\n",
    "        super().__init__(dataset, sort_func=sort_func, res=res, **kwargs)\n",
    "        self.overflow_map_key = overflow_map_key\n",
    "        self.batch_items = None\n",
    "\n",
    "    def create_batches(self, samps):\n",
    "        if self.dataset is not None:\n",
    "            self.it = iter(self.dataset)\n",
    "        res = filter(lambda o: o is not None, map(self.do_item, samps))\n",
    "\n",
    "        for b in map(self.do_batch, self.chunkify(res)):\n",
    "            while self._n_batch_items() >= self.bs:\n",
    "                yield self._get_batch()\n",
    "\n",
    "    def do_batch(self, b):\n",
    "        b = super().do_batch(b)\n",
    "        self._add_batch(b)\n",
    "\n",
    "    def _add_batch(self, b):\n",
    "        if not self.batch_items:\n",
    "            self.batch_items = b\n",
    "        else:\n",
    "            for i in range(len(b)):\n",
    "                if isinstance(b[i], dict):\n",
    "                    for k in self.batch_items[i].keys():\n",
    "                        self.batch_items[i][k] = torch.cat([self.batch_items[i][k], b[i][k]])\n",
    "                else:\n",
    "                    self.batch_items[i].data = torch.cat([self.batch_items[i], b[i]])\n",
    "\n",
    "        # update \"n\" to reflect the additional samples\n",
    "        overflow_map = b[0][self.overflow_map_key].numpy()\n",
    "        self.n += np.sum([i - 1 for i in Counter(overflow_map).values()])\n",
    "\n",
    "    def _get_batch(self):\n",
    "        chunked_batch = []\n",
    "\n",
    "        for i in range(len(self.batch_items)):\n",
    "            if isinstance(self.batch_items[i], dict):\n",
    "                chunked_d = {}\n",
    "                for k in self.batch_items[i].keys():\n",
    "                    chunked_d[k] = self.batch_items[i][k][: self.bs]\n",
    "                    self.batch_items[i][k] = self.batch_items[i][k][self.bs :]\n",
    "\n",
    "                chunked_batch.append(chunked_d)\n",
    "            else:\n",
    "                chunked_batch.append(self.batch_items[i][: self.bs])\n",
    "                self.batch_items[i].data = self.batch_items[i][self.bs :]\n",
    "\n",
    "        return tuplify(chunked_batch)\n",
    "\n",
    "    def _n_batch_items(self):\n",
    "        return len(self.batch_items[0][self.overflow_map_key]) if self.batch_items else 0\n",
    "\n",
    "    def _one_pass(self):\n",
    "        self.do_batch([self.do_item(0)])\n",
    "        b = self._get_batch()\n",
    "        if self.device is not None:\n",
    "            b = to_device(b, self.device)\n",
    "        its = self.after_batch(b)\n",
    "        self._n_inp = 1 if not isinstance(its, (list, tuple)) or len(its) == 1 else len(its) - 1\n",
    "        self._types = explode_types(its)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your data for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The before batch transform you want to use to tokenize your raw data on the fly\n",
    "        # (defaults to an instance of `HF_BeforeBatchTransform` created using the Hugging Face objects defined above)\n",
    "        before_batch_tfm: HF_BeforeBatchTransform = None,\n",
    "        # The batch_tfms to apply to the creation of your DataLoaders,\n",
    "        # (defaults to HF_AfterBatchTransform created using the Hugging Face objects defined above)\n",
    "        after_batch_tfm: HF_AfterBatchTransform = None,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = HF_BaseInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: DataLoader = None,\n",
    "        # Any keyword arguments you want applied to your before batch tfm\n",
    "        before_batch_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to your after batch tfm (or referred to in fastai as `batch_tfms`)\n",
    "        after_batch_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `HF_TextBlock`\n",
    "        **kwargs\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and before_batch_tfm is None:\n",
    "            raise ValueError(\n",
    "                \"\"\"You must supply the Hugging Face architecture, config, tokenizer, and model\n",
    "                - or - an instances of HF_BeforeBatchTransform\"\"\"\n",
    "            )\n",
    "\n",
    "        if before_batch_tfm is None:\n",
    "            # if allowing overflow, if we have to ensure mixed batch items are the same shape\n",
    "            if (\"return_overflowing_tokens\" in tok_kwargs):\n",
    "                padding = 'max_length'\n",
    "\n",
    "            before_batch_tfm = HF_BeforeBatchTransform(\n",
    "                hf_arch,\n",
    "                hf_config,\n",
    "                hf_tokenizer,\n",
    "                hf_model,\n",
    "                max_length=max_length,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                is_split_into_words=is_split_into_words,\n",
    "                tok_kwargs=tok_kwargs.copy(),\n",
    "                **before_batch_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if after_batch_tfm is None:\n",
    "            after_batch_tfm = HF_AfterBatchTransform(\n",
    "                hf_tokenizer=before_batch_tfm.hf_tokenizer, input_return_type=input_return_type, **after_batch_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                blurr_sort_func,\n",
    "                hf_tokenizer=before_batch_tfm.hf_tokenizer,\n",
    "                is_split_into_words=before_batch_tfm.is_split_into_words,\n",
    "                tok_kwargs=before_batch_tfm.tok_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "            # `OverflowDL` is a `DataLoader` that knows how to serve batches of items that are created on the fly as a result\n",
    "            # of asking the tokenizer to return an input in chunks if the lenght > max_length\n",
    "            if (\"return_overflowing_tokens\" in before_batch_tfm.tok_kwargs):\n",
    "              dl_type = partial(OverflowDL, sort_func=dl_sort_func)\n",
    "            else: \n",
    "                partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "\n",
    "        # set the TransformBlock's Hugging Face face objects\n",
    "        self.hf_arch = before_batch_tfm.hf_arch\n",
    "        self.hf_config = before_batch_tfm.hf_config\n",
    "        self.hf_tokenizer = before_batch_tfm.hf_tokenizer\n",
    "        self.hf_model = before_batch_tfm.hf_model\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs={\"before_batch\": before_batch_tfm}, batch_tfms=after_batch_tfm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic wrapper that links defaults transforms for the data block API\n",
    "\n",
    "`HF_TextBlock` has been dramatically simplified from it's predecessor. It handles setting up your `HF_BeforeBatchTransform` and `HF_AfterBatchTransform` transforms regardless of data source (e.g., this will work with files, DataFrames, whatever). You must either pass in your own instance of a `HF_BeforeBatchTransform` class or the Hugging Face architecture and tokenizer via the `hf_arch` and `hf_tokenizer` (the other args are optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level API: For working with PyTorch and/or fast.ai Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a low-level API for working with basic PyTorch Datasets (e.g., a dataset from the Hugging Face datasets library) and DataLoaders. Use the approach detailed below if you already have, or want to use, a plain ol' PyTorch `Dataset` instead of the fast.ai `DataBlock` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class BlurrBatchCreator:\n",
    "    \"\"\"A class that can be assigned to a `TfmdDL.create_batch` method; used to in Blurr's low-level API\n",
    "    to create batches that can be used in the Blurr library\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Your Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator: Type = None,\n",
    "    ):\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.data_collator = data_collator if (data_collator) else DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "\n",
    "    def __call__(self, features):  # A mini-batch (list of examples to run through your model)\n",
    "        \"\"\"This method will collate your data using `self.data_collator` and add a target element to the\n",
    "        returned tuples if `labels` are defined as is the case when most Hugging Face datasets\n",
    "        \"\"\"\n",
    "        batch = self.data_collator(features)\n",
    "        if isinstance(features[0], dict):\n",
    "            return dict(batch), batch[\"labels\"] if (\"labels\" in features[0]) else dict(batch)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BlurrBatchTransform(HF_AfterBatchTransform):\n",
    "    \"\"\"A class used to cast your inputs into something understandable in fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The token ID to ignore when calculating loss/metrics\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any text generation keyword arguments\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = HF_BaseInput,\n",
    "        # Any other keyword arguments you need to pass to `HF_AfterBatchTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(hf_tokenizer=hf_tokenizer, input_return_type=input_return_type)\n",
    "\n",
    "        store_attr(self=self, names=\"hf_arch, hf_config, hf_model, tok_kwargs, text_gen_kwargs\")\n",
    "        store_attr(self=self, names=\"is_split_into_words, ignore_token_id, kwargs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates()\n",
    "class BlurrDataLoader(TfmdDL):\n",
    "    \"\"\"A class that makes creating a fast.ai `DataLoader` that works with Blurr\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A standard PyTorch Dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `HF_BeforeBatchTransform`\n",
    "        # to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `HF_BeforeBatchTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model (not required if passing in an instance of `HF_BeforeBatchTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel,\n",
    "        # An instance of `BlurrBatchCreator` or equivalent\n",
    "        batch_creator: BlurrBatchCreator = None,\n",
    "        # The batch_tfm used to decode Blurr batches (default: HF_AfterBatchTransform)\n",
    "        batch_tfm: BlurrBatchTransform = None,\n",
    "        # (optional) A preprocessing function that will be applied to your dataset\n",
    "        preproccesing_func: Callable[\n",
    "            [Union[torch.utils.data.dataset.Dataset, Datasets], PreTrainedTokenizerBase, PreTrainedModel],\n",
    "            Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        ] = None,\n",
    "        # (optional) list of corresponding labels names for classes; if included then methods like `show_batch` will\n",
    "        # show the name corresponding to the label index vs. just the integer index.\n",
    "        label_names: Optional[list] = None,\n",
    "        # Keyword arguments to be applied to your `batch_tfm`\n",
    "        batch_tfm_kwargs: dict = {},\n",
    "        # Keyword arguments to be applied to `BlurrDataLoader`\n",
    "        **kwargs\n",
    "    ):\n",
    "        if preproccesing_func:\n",
    "            dataset = preproccesing_func(dataset, hf_tokenizer, hf_model)\n",
    "\n",
    "        if \"create_batch\" in kwargs:\n",
    "            kwargs.pop(\"create_batch\")\n",
    "        if not batch_creator:\n",
    "            batch_creator = BlurrBatchCreator(hf_tokenizer=hf_tokenizer)\n",
    "\n",
    "        if \"after_batch\" in kwargs:\n",
    "            kwargs.pop(\"after_batch\")\n",
    "        if not batch_tfm:\n",
    "            batch_tfm = BlurrBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, **batch_tfm_kwargs.copy())\n",
    "\n",
    "        super().__init__(dataset=dataset, create_batch=batch_creator, after_batch=batch_tfm, **kwargs)\n",
    "        store_attr(self=self, names=\"hf_arch, hf_config, hf_tokenizer, hf_model, label_names\")\n",
    "\n",
    "    def new(\n",
    "        self,\n",
    "        # A standard PyTorch and fastai dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets] = None,\n",
    "        # The class you want to create an instance of (will be \"self\" if None)\n",
    "        cls: Type = None,\n",
    "        #  Any additional keyword arguments you want to pass to the __init__ method of `cls`\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"We have to override the new method in order to add back the Hugging Face objects in this factory \n",
    "        method (called for example in places like `show_results`). With the exception of the additions to the kwargs\n",
    "        dictionary, the code below is pulled from the `DataLoaders.new` method as is.\n",
    "        \"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "        if cls is None:\n",
    "            cls = type(self)\n",
    "\n",
    "        cur_kwargs = dict(\n",
    "            dataset=dataset,\n",
    "            num_workers=self.fake_l.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            timeout=self.timeout,\n",
    "            bs=self.bs,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last,\n",
    "            indexed=self.indexed,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        for n in self._methods:\n",
    "            o = getattr(self, n)\n",
    "            if not isinstance(o, MethodType):\n",
    "                cur_kwargs[n] = o\n",
    "\n",
    "        # we need to add these arguments back in (these, after_batch, and create_batch will go in as kwargs)\n",
    "        kwargs[\"hf_arch\"] = self.hf_arch\n",
    "        kwargs[\"hf_config\"] = self.hf_config\n",
    "        kwargs[\"hf_tokenizer\"] = self.hf_tokenizer\n",
    "        kwargs[\"hf_model\"] = self.hf_model\n",
    "\n",
    "        return cls(**merge(cur_kwargs, kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods for getting blurr transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = HF_BeforeBatchTransform,\n",
    "):\n",
    "    \"\"\"Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_blurr_tfm\" class=\"doc_header\"><code>get_blurr_tfm</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_blurr_tfm</code>(**`tfms_list`**:`Pipeline`, **`tfm_class`**:`Transform`=*`HF_BeforeBatchTransform`*)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`tfms_list`** : *`<class 'fastcore.transform.Pipeline'>`*\t<p>A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)</p>\n",
       "\n",
       "\n",
       " - **`tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The transform to find</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_blurr_tfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_blurr_tfm(\n",
    "    dls: DataLoaders,  # Your fast.ai `DataLoaders\n",
    "    before_batch_tfm_class: Transform = HF_BeforeBatchTransform,  # The before_batch transform to look for\n",
    "    blurr_batch_tfm_class: Transform = BlurrBatchTransform,  # The after_batch (or batch_tfm) to look for\n",
    "):\n",
    "    \"\"\"This convenience method will find the first Blurr transform required for methods such as \n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    # try our befor_batch tfms (this will be used if you're using the mid-level DataBlock API)\n",
    "    tfm = get_blurr_tfm(dls.before_batch, tfm_class=before_batch_tfm_class)\n",
    "    if tfm:\n",
    "        return tfm\n",
    "\n",
    "    # try our after_batch tfms (this will be used if you're using the low-level Blurr data API)\n",
    "    return get_blurr_tfm(dls.after_batch, tfm_class=blurr_batch_tfm_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"first_blurr_tfm\" class=\"doc_header\"><code>first_blurr_tfm</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>first_blurr_tfm</code>(**`dls`**:`DataLoaders`, **`before_batch_tfm_class`**:`Transform`=*`HF_BeforeBatchTransform`*, **`blurr_batch_tfm_class`**:`Transform`=*`BlurrBatchTransform`*)\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as \n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`dls`** : *`<class 'fastai.data.core.DataLoaders'>`*\t<p>Your fast.ai `DataLoaders</p>\n",
       "\n",
       "\n",
       " - **`before_batch_tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The before_batch transform to look for</p>\n",
       "\n",
       "\n",
       " - **`blurr_batch_tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The after_batch (or batch_tfm) to look for</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(first_blurr_tfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base `show_batch` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `HF_BaseInput` typed inputs\n",
    "    x: HF_BaseInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    trg_labels = None\n",
    "    if hasattr(dataloaders, \"label_names\"):\n",
    "        trg_labels = dataloaders.label_names\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    for idx, (input_ids, label, sample) in enumerate(zip(x, y, samples)):\n",
    "        if idx >= max_n:\n",
    "            break\n",
    "\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "        for item in sample[n_inp:]:\n",
    "            if not torch.is_tensor(item):\n",
    "                trg = item\n",
    "            elif is_listy(item.tolist()):\n",
    "                trg = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.item()\n",
    "            else:\n",
    "                trg = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "Below demonstrates both how to contruct your `DataBlock` for a sequence classification task (e.g., a model that requires a single text input) using the mid-level API, and also with the low-level API should you wish to work with standard PyTorch or fast.ai Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mid-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path(\"models\")\n",
    "imdb_df = pd.read_csv(path / \"texts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic use\n",
    "\n",
    "There are a bunch of ways we can get at the four Hugging Face elements we need (e.g., architecture name, tokenizer, config, and model).  We can just create them directly, or we can use one of the helper methods available via `BLURR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"  # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have those elements, you can create your `DataBlock` as simple as the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 512]), 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film is about a family trying to come to terms with the death of the mother/wife by moving to Genova, Italy.&lt;br /&gt;&lt;br /&gt;The plot of \"Genova\" sounds promising, but unfortunately it is empty and without focus. The film only consists of a collection of scenes depicting the daily life of the family, such as swimming, taking piano lessons or cooking eggs. Most of such scenes are redundant and tiresome, completely failing to engage viewers emotionally. The ending is very disappointing as it is n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recently, I had opportunity to view a working print in Kansas City (Olathe, KS.) of this title. It is difficult for me, being a lover of the art as I am, to report the following, but, the truth sometimes hurts, and quite frankly after sitting through this tripe (I'm using the slang definition here - worthless statements or writing) for an hour and a half, I feel obligated to share (WARN) any interested parties. Let's begin at the beginning, a good place to start as always. The first 15 minutes</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with long documents\n",
    "\n",
    "There are two options when dealing with texts longer than your model can handle.\n",
    "\n",
    "First, as illustrated above, you can simply provide a `truncation` strategy to ensure they are <= the maximum length your model can handle.\n",
    "\n",
    "Second, in the case we want to process the entirety of each document regardless of length, we can split text greater than the max length allowed by our model and then treat each of these chunks as separate examples. This approach is accomplished by setting `\"return_overflowing_tokens\": True` into our tokenizer function's via `tok_kwargs`. \n",
    "\n",
    "While the second approach is traditionaly performed as part of the data preprocessing, blurr can do this on-the-fly when using it's `OverflowDL` DataLoader (which is automatically used by blurr when you pass  `\"return_overflowing_tokens\": True` in the `tok_kwargs` argument of  `HF_TextBlock`.  Below is an example of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=128, tok_kwargs={\"return_overflowing_tokens\": True, \"stride\": 2}),\n",
    "    CategoryBlock,\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n",
    "\n",
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raising Victor Vargas: A Review&lt;br /&gt;&lt;br /&gt;You know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It's warm and gooey, but you're not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn't quite feel right. Victor Vargas suffers from a certain overconfidence on the director's part. Apparently, the director thought that the ethnic backdrop of a Latino family on the lower east side, and an idyllic</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yllic storyline would make the film critic proof. He was right, but it didn't fool me. Raising Victor Vargas is the story about a seventeen-year old boy called, you guessed it, Victor Vargas (Victor Rasuk) who lives his teenage years chasing more skirt than the Rolling Stones could do in all the years they've toured. The movie starts off in `Ugly Fat' Donna's bedroom where Victor is sure to seduce her, but a cry from outside disrupts his plans when his best-friend Harold (Kevin Rivera) comes-a-looking for him. Caught in the</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in the attempt by Harold and his sister, Victor Vargas runs off for damage control. Yet even with the embarrassing implication that he's been boffing the homeliest girl in the neighborhood, nothing dissuades young Victor from going off on the hunt for more fresh meat. On a hot, New York City day they make way to the local public swimming pool where Victor's eyes catch a glimpse of the lovely young nymph Judy (Judy Marte), who's not just pretty, but a strong and independent too. The relationship that develops between Victor and Judy becomes the focus of the film. The story also focuses on</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>focuses on Victor's family that is comprised of his grandmother or abuelita (Altagracia Guzman), his brother Nino (also played by real life brother to Victor, Silvestre Rasuk) and his sister Vicky (Krystal Rodriguez). The action follows Victor between scenes with Judy and scenes with his family. Victor tries to cope with being an oversexed pimp-daddy, his feelings for Judy and his grandmother's conservative Catholic upbringing.&lt;br /&gt;&lt;br /&gt;The problems that arise from Raising Victor Vargas are a few, but glaring errors. Throughout the film you get to know</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Grab your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features\n",
    "raw_datasets[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4f0830154c418c95888463286c7f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52126225919f423090d1062de5af3141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac52c72db41433c8d8609e99f6a1ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return hf_tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define any pre-processing that needs to be done to your datasets (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def preproc_hf_dataset(\n",
    "    # A standard PyTorch Dataset or fast.ai Datasets\n",
    "    dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A Hugging Face model\n",
    "    hf_model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training\n",
    "    libraries\n",
    "    \"\"\"\n",
    "    if (\"label\") in dataset.column_names:\n",
    "        dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    hf_model_fwd_args = list(inspect.signature(hf_model.forward).parameters.keys())\n",
    "    bad_cols = set(dataset.column_names).difference(hf_model_fwd_args)\n",
    "    dataset = dataset.remove_columns(bad_cols)\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Use `BlurrDataLoader` to build Blurr friendly dataloaders from your datasets.\n",
    "\n",
    "Setting the `label_names` argument to a list of label names corresponding to each class's index will ensure the methods like `show_batch` and `show_results` print the name of the class rather than just its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"label\"].names\n",
    "\n",
    "trn_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    label_names=label_names,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    label_names=label_names,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 65])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They include Ask Jeeves Inc., Global Crossing, Aether Systems, Clarent, Copper Mountain Networks and VA Linux, now VA Software. They included Global Crossing, Akamai Technologies, Ask Jeeves, Copper Mountain Networks, Etoys and VA Linux.</td>\n",
       "      <td>equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fanned by the hot, dry Santa Ana winds and minimal humidity, major fires were raging in at least 10 places, having already burned nearly 80 937 hectares. Those hot, dry Santa Ana winds and minimal humidity created optimal conditions for raging fires in at least 10 places that have already burned nearly 200,000 acres.</td>\n",
       "      <td>equivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForSequenceClassification',\n",
       " 'BartForSequenceClassification',\n",
       " 'BertForSequenceClassification',\n",
       " 'BigBirdForSequenceClassification',\n",
       " 'BigBirdPegasusForSequenceClassification',\n",
       " 'CTRLForSequenceClassification',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CanineForSequenceClassification',\n",
       " 'ConvBertForSequenceClassification',\n",
       " 'DebertaForSequenceClassification',\n",
       " 'DebertaV2ForSequenceClassification',\n",
       " 'DistilBertForSequenceClassification',\n",
       " 'ElectraForSequenceClassification',\n",
       " 'FNetForSequenceClassification',\n",
       " 'FlaubertForSequenceClassification',\n",
       " 'FunnelForSequenceClassification',\n",
       " 'GPT2ForSequenceClassification',\n",
       " 'GPTJForSequenceClassification',\n",
       " 'GPTNeoForSequenceClassification',\n",
       " 'HubertForSequenceClassification',\n",
       " 'IBertForSequenceClassification',\n",
       " 'LEDForSequenceClassification',\n",
       " 'LayoutLMForSequenceClassification',\n",
       " 'LayoutLMv2ForSequenceClassification',\n",
       " 'LongformerForSequenceClassification',\n",
       " 'MBartForSequenceClassification',\n",
       " 'MPNetForSequenceClassification',\n",
       " 'MegatronBertForSequenceClassification',\n",
       " 'MobileBertForSequenceClassification',\n",
       " 'OpenAIGPTForSequenceClassification',\n",
       " 'PerceiverForSequenceClassification',\n",
       " 'ReformerForSequenceClassification',\n",
       " 'RemBertForSequenceClassification',\n",
       " 'RoFormerForSequenceClassification',\n",
       " 'RobertaForSequenceClassification',\n",
       " 'SEWDForSequenceClassification',\n",
       " 'SEWForSequenceClassification',\n",
       " 'SqueezeBertForSequenceClassification',\n",
       " 'TransfoXLForSequenceClassification',\n",
       " 'UniSpeechForSequenceClassification',\n",
       " 'UniSpeechSatForSequenceClassification',\n",
       " 'Wav2Vec2ForSequenceClassification',\n",
       " 'XLMForSequenceClassification',\n",
       " 'XLMRobertaForSequenceClassification',\n",
       " 'XLNetForSequenceClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"SequenceClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"albert-base-v1\",\n",
    "    \"facebook/bart-base\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"sshleifer/tiny-ctrl\",\n",
    "    \"camembert-base\",\n",
    "    \"sarnikowski/convbert-medium-small-da-cased\",\n",
    "    \"microsoft/deberta-base\",\n",
    "    \"microsoft/deberta-v2-xlarge\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"monologg/electra-small-finetuned-imdb\",\n",
    "    \"flaubert/flaubert_small_cased\",\n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"gpt2\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    \"allenai/led-base-16384\",\n",
    "    \"microsoft/layoutlm-base-uncased\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"sshleifer/tiny-mbart\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"google/mobilebert-uncased\",\n",
    "    \"openai-gpt\",\n",
    "    #'reformer-enwik8',                  # (see model card; does not work with/require a tokenizer so no bueno here)\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    #'google/tapas-base',                # (requires pip install torch-scatter)\n",
    "    \"transfo-xl-wt103\",\n",
    "    \"xlm-mlm-en-2048\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# for model_name in pretrained_model_names:\n",
    "#     tok = AutoTokenizer.from_pretrained(model_name)\n",
    "#     print(f'=== {model_name} ===')\n",
    "#     print(f'=== {tok.padding_side} ===')\n",
    "#     print(f'=== {tok.pad_token_id} ===')\n",
    "#     print(tok(['hi', 'hello everyone. its good to be here'], ['yo', 'yo'], padding='max_length', max_length=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path(\"models\")\n",
    "imdb_df = pd.read_csv(path / \"texts.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was pleasantly surprised i quite liked this movie. witty writing (some \"inside\" jokes i got, others i didn't - maybe due to actors speaking on top of one another), great acting (notably john cassini), great cameos, interesting and unique directing. i rented it to see jeffrey meek (very disappointed he was in it such a short time, blink and you'll miss him!) but found the movie remarkably entertaining. i'll actually watch it again before i send back to netflix. i think actors and wanna-be actors will thoroughly enjoy this</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after mob boss vic moretti (late great anthony franciosa) kills his lady whom has been cheating on him with derek, their new chauffeur/ vietnam vet, and blames it on the poor guy, derek finds himself in jail where he has to contend with a corrupt warden, vic's prisoner brother who runs the jail, and, oh yeah illegal experiments conducted by a shady cia agent (great genre-mainstay and first time director john saxon) to turn various prisoners into super-human invincible zombies. of course things get out of hand and it's up to derek, and the</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== facebook/bart-base ===\n",
      "\n",
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE DEVIL'S PLAYTHING is my second attempt at a Joseph Sarno production - and although I will say it is far more enjoyable than the painfully dull and unerotic Swedish WILDCATS, it is still a little slow and un-explicit for my taste.&lt;br /&gt;&lt;br /&gt;This one centers around a group of vampire girls who live in a castle, that want to resurrect their previously murdered \"leader\". In order to do so, the girls have to dance around naked and kiss each other and chant weird stuff - and of course drink some blood, too. When a doctor and her brother</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not a big fan of most anime, but Gundam Wing is truly something else. Gundam wing lacks all of that stereotypical melodrama that you might think of when you think of anime, since the number of jokes made over the 17 hours would only be in the double digits, Gundam Wing gets right down to business. &lt;br /&gt;&lt;br /&gt;Gundam Wing is as much of a political thriller as it is an action series. Large parts focus on the diplomatic dealings of a war, not only the battles. Though battle animation lacks extreme detail in cases where it would just be a pain to animate, individual duels</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the views of earth that are claimed in this film to have been faked by nasa have recently been compared with the historical weather data for the time of apollo 11, and show a good match between the cloud patterns in the video sequence and the actual rainfall records on the day. &lt; br / &gt; &lt; br / &gt; this would seem to undermine the entire argument put forward in the film that the \" whole earth \" picture is actually a small part of the planet framed by the spacecraft window. &lt; br / &gt; &lt; br / &gt; i am waiting for bart sibrel to now claim that the historical weather data has been faked by</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why, o'why!... did i pick this one up? well... i needed a no - brainer in the summer heat, and the cover looked cool. &lt; br / &gt; &lt; br / &gt; of course i should've known better. this is a really, really bad movie. and it gets embarasing when the makers know it's bad, and try cover it up by adding some sexy / beautiful women, and some sex - scenes to it. well, folks... it does'nt cut it, does it! &lt; br / &gt; &lt; br / &gt; if you</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pixar has had massive success over the years with the full-length CGI animated movies they have made. \"A Bug's Life\" was the second of a whole bunch of features they have made so far, preceded by the company's feature-length debut, the groundbreaking \"Toy Story\", which was the first ever feature-length CGI movie. I remember when this follow-up was heavily advertised around the time of its release in the late 1990's, but I never actually saw it until November 2006. I watched it twice that month, and over three years later, I have seen it a third time. It has never impressed me</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Written by the writer who penned the excellent Murder Rooms series which chronicled ACD's adventures with Doctor Joseph Bell, I was looking forward to this and I wasn't disappointed. It was quite slow moving, with a lot of emphasis on Doyle's frustration at Sherlock Holmes which was very accurate and excellently portrayed. It was an interesting character study and very well shot ( on digital video, unusual for a period piece ). The acting was excellent all round, particularly Tim McInnery and Brian Cox although the actor who portrayed ACD, whose name I cannot remember impressed me no end. An excellent character study which has about</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-ctrl ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tctrl\n",
      "tokenizer:\tCTRLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film, along with WESTFRONT 1918, are my favorite Pabst-directed films and I enjoyed them more than his much more famous films which starred Louise Brooks (such as PANDORA'S BOX). It's probably because both are very similar to the Neo-Realist films that the Italians perfected in the 1940s and 50s. This style film called for using non-actors (just typical folks) in everyday settings in order to create intensely involving and realistic films.&lt;br /&gt;&lt;br /&gt;In this case, the film is about French and German coal miners, so appropriately, the people in the roles seem like miners--not actors. The central conflict as the film begins is that there is</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ask yourself where she got the gun? Remember what she was taught about the mark's mindset when the con is over? The gun had blanks and it was provided to her from the very beginning.&lt;br /&gt;&lt;br /&gt;When the patient comes back at the end she was SUPPOSED to see him drive away in the red convertible and lead her to the gang splitting up her 80 thousand.&lt;br /&gt;&lt;br /&gt;The patient was in on the con from the beginning.&lt;br /&gt;&lt;br /&gt;Mantegna does not die in the end - the gun had blanks.&lt;br /&gt;&lt;br</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ah, a Kelly/Sinatra sailor-suit musical. So familiar, right? Yes, but this isn't the one you usually hear about. On The Town's that-a-way. But if you stick around, you might learn something. Okay, probably not. Anyway, Anchors Aweigh tells the story of two sailors on a three- or four-day leave. Joe is the \"Sea Wolf\" and Clarence,</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actually... that \"video camera\" effect, is just that, it's an effect, a rather good one.. (u don't know much about directing a film do you?) this film is in fact BETTER than the original, it's great fun to watch, made for TV, doesn't need to follow any rules. I find it hard to watch number 1 because of how he kills the first girl, its disturbing. and all the time we are</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sarnikowski/convbert-medium-small-da-cased ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>... and how they bore you right out of your mind! The Crater Lake Monster is one of the classic BAD films from the 70's made with no actors of any note, an embarrassing script, woeful direction, and a tireless desire to fuse \" horror \" with light comedy. This movie introduces a paleontologist who finds drawings of an aquatic dinosaur underneath Crater Lake... a meteor falls from the sky, and</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ko to tamo peva is the best comedy of all times. Believe me i saw a lot of movies and comedies but tell me which one make you smile every time you watching it. But truth is that the humour in this comedy is special. It is caratherisic for serbia. And all former republic of yugoslavia know it very well!!! So i think the rest of audience ( for example : In Europe ) can't enjoy it so much. Be</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/deberta-base ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A wonder. One of the best musicals ever. The three Busby Berkely numbers that end the movie are spectacular, but what makes this film so wonderful is the incredible non-stop patter and the natural acting of Cagney and Blondell. (Keeler is also lovely, even though she may not have been a great actress). There's a freshness in the movie that you don't see in flicks today, much less in the usually stilted 30s films, even though the plot, involving the setting up of movies prologues, is quite dated.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a really interesting movie. It is an action movie with comedy mixed in. Foxx teams up with comedian Epps in this movie to give it a comedic spin. It will keep you wondering whats going to happen to Foxx next. It was a well shot movie, the director used the right colors in this movie(dark blue colors) to give it the right kind of feel. Kimberly Elise also starred in this movie and it is always a pleasure to see her on the big screen. She plays her role well. Even Jamie Kennedy is in this movie. It's worth seeing it you haven't seen it. It</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/deberta-v2-xlarge ===\n",
      "\n",
      "architecture:\tdeberta_v2\n",
      "tokenizer:\tDebertaV2Tokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carnosaur 3 is bad... awfully bad. Bad to the point where it is funny. How matter how much I try to convince myself, I just can't believe anyone in this world could find this entertaining for serious reasons. I mean, come on, even the cover is bad! OK, the special effects are absolutely ridiculous. Those \"Carnosaurs\" are really ridiculous. A scientist tells the soldiers that they move incredibly fast, yet when you see them run, they run at the speed of... an actor in a rubber suit trying to run as much as he can. And the explosions are funny(there is</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was the first Ewan McGregor movie I ever saw outside of Star Wars. Since then I have become a very big Ewan McGregor fan but I still can't bring myself to forgive this movie's existence.&lt;br /&gt;&lt;br /&gt;My sister has always been a huge Jane Austen fan and because of that, I have been subjected to various of the classics, Emma being one of them. I've always considered them irritating, stupid and boring. However, after watching this terrible rendition, I was forced to admit that the original Emma was delightful and charming. Ewan McGregor scarcely serves a purpose in this film after they hacked and mutilated</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n",
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i remember watching this on prime time when i was about 7 years old. i was a huge comic book reader at the time, and anything relating to superheroes was anticipated heavily. the end result, however, was underwhelming. &lt; br / &gt; &lt; br / &gt; i was aware of the \" emma peel \" diana prince stories, as they had only recently come to an end and diana was returned to her amazonian form. however, there was so little action that i was bored throughout most of the movie. the final costume was an interesting idea, but looked more like a cheerleader than a superhero. &lt; br</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>having read the other comments on this film, i would like to share my own view that this is one tough movie to see unless you are a total brooksophile. i am not. &lt; br / &gt; &lt; br / &gt; when looked at by a purely objective observer, the film is an unbalanced narrative that presents us with more undistilled neuroses than are capable of being absorbed in one sitting. it is quite difficult to watch. the brooks character ( robert cole ) is so unsympathetic and unpleasant that it is hard to relate to him - - - let alone root for him as</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== monologg/electra-small-finetuned-imdb ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the case of the scorpion's tail is a highly stylish giallo directed by sergio martino, who appears to be a giallo master second only to dario argento. &lt; br / &gt; &lt; br / &gt; ernesto gastaldi wrote this fabulous who - dunnit, quite complex but ultimately very satisfying and entertaining murder mystery. it also makes sense in the end, a big plus,'cause that's not always the case for these giallo's, as they tend to stretch credibility with their endless red - herrings and ultimate solutions. here, the less you know about the plot, the better.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the interplay between the characters is a moral disaster. you end up disliking most of the characters and you don't particularly like any of them. &lt; br / &gt; &lt; br / &gt; even the two main characters played by david and gwen are so badly written that you really don't care one bit about them. the movie has no plot, no direction and no purpose. the single redeeming quality of the movie was to treat it as a glimpse into the messed up lives of a few losers - and that's hardly stimulating even as an afternoon waste.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n",
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raising Victor Vargas : A Review &lt; br / &gt; &lt; br / &gt; You know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It' s warm and gooey, but you' re not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn' t quite feel right. Victor Vargas suffers</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" Look, I know this may suck right now, but pain is temporary, film is forever. Whatever you do right now is burned into celluloid for all time and for thousands of years to come \". Robert De Niro &lt; br / &gt; &lt; br / &gt; This was initially a film for Steven Spielberg, the director hiring several screenwriters to adjust the screenplay so that it more suited his themes. And so we have a dysfunctional family that is threatened by a</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>users who have rated this movie so highly simply can't have seen enough good films to compare it with. have they all been brainwashed?? i have rarely felt so disappointed by a film and some of that must be attributable to the ridiculous hype surrounding this movie. &lt; br / &gt; &lt; br / &gt; from the first, bu is just a chase film. we pick it up at the end of one chase and go straight into another. and another. and another. and another. do you see a pattern emerging? there is virtually no time'wasted'on plot, character development, or boring old</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>really good horror flick featuring to of the greatest, boris karloff and bela lugosi. dr. janos rukh ( karloff ) is on an expedition in africa trying to find an ancient meteorite. after finding it, rukh is poisoned by the its radiation. all he touches dies and the dark side of rukh makes him become an egotistic murderer. his friend, dr. felix benet ( lugosi ) finds a limited remedy to the problem and at the same time realizes the radiation could be used for the good of mankind by curing diseases. the two fiends will battle over the</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The real story (took place in Kansas in 1959) of a murder (Perry and Dick, two ex-convicts who broke into a remote house on a rainy night to steal and kill everyone they met). Richard Brooks directed the chilling and disturbing Capote's book about the reasons that drove these kids to the crime (Are they Natural Born Killers?). The crime scenes are very brutal and haunting because of the lack of senses and reasons for what we witnessed. Stunning black &amp; white cinematography from Conrand Hall, excellent country - road music score from Quincy Jones, amazing performances in two principal roles from Robert Blake and Scott</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dig! I would say to anyone even if you don't like Metallica to see'some kind of monster' it is a spinal tap type documentary about one of the biggest bands in the world acting like mental kids during a breakdown of sorts. It's fun and fascinating. Along the same lines comes dig! A film about 'the Dandy Warhol's' and 'the Brian Jonestown massacre' two Portland bands who start off a kind of music scene in there home town only for one of the bands to become huge and one to fall by the wayside into the musical history books. Right from the start the two bands pull</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I should explain why i gave this...\"piece of art\" 1 star rating out of possible 10. Simply because it's hard or next to impossible to rate it unbiased. probably it would have been the same if i had given it 10/10 - explanations anyway would have followed.&lt;br /&gt;&lt;br /&gt;I am not fond of these pointless gore movies like HOSTEL or so - i think that's disgusting and pretty terrible (in all the possible contextual meanings), but as i found out after watching this movie - there is a genre called \"historical drama\" - and probably it would have been the case of 10/10</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This horrendously bad piece of trash manages to be racist, sexist and homophobic all at once, while pretending to be terribly chic and sophisticated. Atrocious performances, a cliche ridden screenplay, and boring direction make this movie one to steer clear of. Two scenes were especially offensive - the one in which Schaech scrubs his tongue after being kissed by another man (could it really have been that gross), and the scene where Eastwood is kissed by Schaech's best friend, who is pretending to be Russian. After he leaves the room she exclaims \"f**king foreigners\"! So much for her being a cultured artist</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/led-base-16384 ===\n",
      "\n",
      "architecture:\tled\n",
      "tokenizer:\tLEDTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Hazzard County, Georgia, cousins Bo and Luke Duke (Scott, Knoxville) and their cousin Daisy Duke (Jessica Simpson) run moonshine made by their Uncle Jesse (Willie Nelson) while avoiding the local authority, Boss Hog (Burt Reynolds). Their problems with the Boss are only beginning as they learn he's been plotting to strip mine the town for valuable ores found below it.&lt;br /&gt;&lt;br /&gt;I have never seen the TV show and after watching the movie, I'm not going to start any time soon. I like stupid comedies but this one didn't offer many laughs. It</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A very realistic portrait of a broken family and the effect it has on the kid caught in between. As a child of divorced parents I was totally relating to events in the film. Also - a really cool zombie twist which I thought was VERY ORIGINAL. I'm tired of the same old stuff in movies. A very realistic portrait of a broken family and the effect it has on the kid caught in between. As a child of divorced parents I was totally relating to events in the film. Also - a really cool zombie twist which I thought was VERY ORIGINAL. I'm tired of the same old stuff in movies. A very</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/layoutlm-base-uncased ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in theory, films should be a form of entertainment. while this excludes documentaries and other experimental forms of film - making ; most movies, specially genre films, must not only tell it's story or message, they must entertain their target audience in some way. all this just to say that in my opinion a bad movie is not a movie with low production values or low - budget, a bad movie is one that is boring. &lt; br / &gt; &lt; br / &gt; \" hellborn \" or \" asylum of the damned \" as is known in the u. s., is a bad movie simply because it is just not</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the interplay between the characters is a moral disaster. you end up disliking most of the characters and you don't particularly like any of them. &lt; br / &gt; &lt; br / &gt; even the two main characters played by david and gwen are so badly written that you really don't care one bit about them. the movie has no plot, no direction and no purpose. the single redeeming quality of the movie was to treat it as a glimpse into the messed up lives of a few losers - and that's hardly stimulating even as an afternoon waste.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Poor Will would be rolling over in his grave if he could this this horiible German-TV adaptaion of his classic play. It's obvious that very little money was spent on it. A stage riser, a catwalk and some randomly placed columns pass off as a set. The movie was ineptly dubbed into English, with the English voice actors occasionally mumbling their lines. The whole production had an incredibly dark and dreary feel to it. And just where was Fonterbras in this movie anyway? MST3K gave this sorry production the treatment it justly deserved.&lt;br /&gt;&lt;</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peeew this stinks! As everyone knows it's based upon some Geico insurance commercials; what no one knows is WHY?! Those commercials were amusing on first viewing at best; hardly fodder for a series. (The talking Geico gecko -- that's another story. Now that would make for an intriguing series!) And why on earth did ABC -- as reported in the press -- actually agree to buy the cavemen character rights from Geico for this? After all, the idea of cavemen struggling in the modern world is hardly unique to TV; Phil Hartman had a recurring Saturday Night Live role as The Unf</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-mbart ===\n",
      "\n",
      "architecture:\tmbart\n",
      "tokenizer:\tMBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jungle Fever is too highly stylized, stereotyped, and comes across as essentially dishonest. Wesley Snipes was wrong for the lead and there was no chemistry between him and Annabella Sciorra. Even though there's plenty of talent in this movie, it's mostly wasted because the parts are reduced to little more than decorative cameos. Also, instead of simply showing racism for the ugly and stupid thing it is, Spike Lee chooses to wave it around like a flag in a most whining and irritating manner. I made</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I should have known I was in trouble with Casper Van Diem as the lead character. Words cannot describe, nor do they do justice to just how terrible this movie was. But please allow me to try to describe it: Horrible acting, terrible dialog, corny situations and through it all you get the feeling that you are being force-fed the beliefs and propeganda from the Trinity Broadcasting Network. Its a weak attempt at trying to show Hollywood that a movie can be entertaining and have a deep, religious message attached to it. They failed miserably. It was clearly the</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as a fan of science - fiction movies, i have been aware of the matrix since its release in 1999. from the little bit i would allow people to tell me about it, i assumed it was highly original and sophisticated. i am also a devotee of alice in wonderland. i could never quite figure out how i missed the matrix when it was released. with the imminent release of the matrix reloaded, it was time to buy the dvd and watch it. &lt; br / &gt; &lt; br / &gt; the disappointment was too great. the premise of the matrix ( the controlling device as opposed to the movie ) was clever. the</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>road to perdition can be summed up by thomas newman's score. it's haunting and beautiful but you're aware that this music is similar to newman's other work and while listening to the soundtrack you're reminded of scent of a woman, meeting joe black and the shawshank redemption you're reminded of other films as the story unfolds on screen. as the sullivans drive round america trying to escape from a psychotic hit man you think of the getaway, irish gangsters is miller's crossing whilst the subtext of guilt and redemption can be summed up by coppola</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i found myself very caught up in this movie, at least at the beginning, and any credit i give to this movie, is lacey chabert, she was fantastic!! but thats where it ends. i seem to be very good at figuring out who the killer is, and i like it when a movie is able to completely baffel me, but i felt out and out lied to, they whole time they lead you in one direction and then suddenly they decided to go in a completely different direction at the end, they gave no hit to it at all, thats not misleading that very bad writing and planning,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i thoroughly enjoyed this film for its humor and pathos. i especially like the way the characters welcomed gina's various suitors. with friends ( and family ) like these anyone would feel nurtured and loved. i found the writing witty and natural and the actors made the material come alive.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai-gpt ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\topenai\n",
      "tokenizer:\tOpenAIGPTTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have seen this movie and the other one. trinity is my name and i find that this one is worse then the first one. i have no idea why they even made another movie it was stupid and pointless sorry to say that i have all of them. i have sat through them number of times and it still drives me to turn it off 5 minutes into the movie. i like terence hill movies and i like bud spencer but this movie just drove me up the wall. if it had a different story line or at least more of a plot and more comedy it might have been funner and worth the 5 dollars i spent buying all</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please, be warned : this movie, though a pretty bad storyline, was one of the most gruesome movies i have seen... ever. just remember that before you settle on your sofa to enjoy the movie. &lt; br / &gt; &lt; br / &gt; so, it officially begins with a party. just your average party but there's some guy there. he's pretty into kate... if you know what i'm saying. memorise his face ; it'll help later. &lt; br / &gt; &lt; br / &gt; so anyway kate goes of to find george clooney ( didn't i say the plot</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another turgid action/adventure flick from the Quinn Martin Productions factory. Roy Thinnes plays undercover agent Diamond Head (Mr. Head, to you), working for his G-Man handler \"Aunt Mary\", looking for \"Tree\", who's on a mission to...well, just watch the movie. &lt;br /&gt;&lt;br /&gt;This one deserved and got the full MST3K sendup. As the boys and various reviewers have pointed out, the movie \"Fargo\" had more Hawaiian locations than this film. Apparently shot on a puny budget, this movie highlights Hawaii's broken-down dive shops,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I have seen this movie many times. At least a Dozen. But unfortunatly not recently. However, Etched in my memory never to leave me is a scene in which Mickey Rooney, -\"Killer Mears\" knows that he is to be executed and it's getting close to the moment of truth, He dances, and cries, and laughs, he vacillates from hesteria to euphoria and runs the gambit of ever emotion. Never have I seen such a brilliant performance by any actor living or dead, past or present. It was then I know for</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have you ever sat watching a movie when 20 or 30 minutes have gone by and suddenly you realize that you have actually seen the movie before? that happened to me with \" the young graduates \". the cover of the video box, if you can find the video, is extremely deceiving. i'd swear that the two women on the cover aren't even in the film. &lt; br / &gt; &lt; br / &gt; anyway, i was either born a decade too late to appreciate the finer points of this film or... it is simply pointless junk. i'm heavily leaning toward the latter but i guess some</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a retired diplomat, played nicely by michael york, goes to russia to get revenge on the russian gangster that murdered the diplomat's policeman son. there the diplomat meets an exceptionally strong and decent russian cop who helps him bring the russian gangster to justice. &lt; br / &gt; &lt; br / &gt; i remembered the old action flicks of the 1980s that always portray the russians as evil bad guys out to undermine the righteous u. s. government. it's interesting to see this time the russian guy as a hero. &lt; br / &gt; &lt; br / &gt; not a great flick, it's really typically a \" b \" action</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== transfo-xl-wt103 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\ttransfo_xl\n",
      "tokenizer:\tTransfoXLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this movie, supporters (either and not-) just lie about a dramatic situation in our country. &lt; br / &gt; &lt; br / &gt; They did not say that the conflict started because of announcement firing a lot of PDVSA best workers just for political issues. &lt; br / &gt; &lt; br / &gt; They did not say anything about more than 96 TV interruptions transmitted by during only 3 days in \"\" (a kind of confiscation o private TV signals). Each one with about 20 minutes of duration. &lt; br / &gt; &lt; br / &gt; They did not tell us anything about The announcement made by General</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am terribly sorry, I know that still is called one of the greatest directors in post-war Germany and that most of his films are considered \"master-pieces,\" but when I see \"Lili\" today, in 2004, I wonder what everyone is up and away about this movie! The acting is simply terrible - Hanna is all the smiling like an idiot! -, the between Nazi-glamour and battlefields are ridiculous, the whole film looks as if it was made within two days in an attic. Probably it was exactly that way and many people seem to take this for \"real art,\" but for me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n",
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so your bairns are away on a sleep-over? the wife is visiting the mother in law? you though are at home. it's a dark and stormy night and there is no football on the telly and the dishwasher needs stacking? so now what are you going to do? &lt; br / &gt; &lt; br / &gt; i will tell you! &lt; br / &gt; &lt; br / &gt; go make an old fashioned cocoa ( frys is best! ) get hold of some ginger nuts and sit down in front of the dvd. now go select and play arthur askeys world war two thriller / horror the</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>despite its stereotypes, virtually'no-name'cast and an obviously low budget i thought this film was alright ; much better than i expected it to be. i was skeptical at first - the idea of a computer virus that can also infect people seemed a little ludicrous to me. but in the end, i thought the film handled the concept well ( even if some scenes were a little cliched ). &lt; br / &gt; &lt; br / &gt; the cast was quite good, and the two leads seemed to take their roles very seriously. i couldn 't help thinking, though, that janine turner is a</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is another one of those 'humans vs insects/eco-horror' features; a theme that was popular in the late 70's. Only you can't really call it horror. There's zero suspense and no gruesome events. In other words: this movie is pretty lame. It's not that it's really bad or something; it's just very boring. A construction site near a hotel uncovers a big nest of ants. Later on we learn that, probably due to different sorts of pesticides used in the past, their bi</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bill Maher's Religulous is not an attack on organized religion. It's an attack on Christianity and Islam. Apart from ridiculing a bunch of Rabbis inventing warped machines to get around Sabbath regulations, he really doesn't attack Judaism and seems enraged when a Rabbi actually challenges the existence of the State of Israel. If Bill Maher followed his hypothesis to its logical conclusion, he would realize that the very creation of Israel in the Palestinian Territories is based on the so called 'holy books' of organized religion. This is</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carnosaur 3 is bad... awfully bad. Bad to the point where it is funny. How matter how much I try to convince myself, I just can't believe anyone in this world could find this entertaining for serious reasons. I mean, come on, even the cover is bad! OK, the special effects are absolutely ridiculous. Those \"Carnosaurs\" are really ridiculous. A scientist tells the soldiers that they move incredibly fast, yet when you see them run, they run at the speed of... an actor in a rubber suit trying to run as much as he can. And</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The fact that most of the budget for this presumably went on the heavy-duty cast list shouldn't have mattered if it had been staged with flair and imagination and some sympathy for the original's satirical intent. Instead we get risibly bad song and dance sequences featuring picturesque beggars and whores, and the final alienation is accomplished by pulling back to reveal the action has taken place on a music-hall stage, appropriately enough for a production that's more Lionel 'Oliver' Blair than Brecht. The acting talent is shamefully misused: Migenes and Walters are good</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_class = RobertaTokenizer if (\"/ibert\" in model_name) else None\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, tokenizer_cls=tok_class)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz), CategoryBlock)\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n",
    "    dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "    b = dls.one_batch()\n",
    "\n",
    "    try:\n",
    "        print(\"*** TESTING DataLoaders ***\\n\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"PASSED\", \"\"))\n",
    "        dls.show_batch(dataloaders=dls, max_n=2, trunc_at=1000)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"FAILED\", err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>albert-base-v1</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizerFast</td>\n",
       "      <td>facebook/bart-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>google/bigbird-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>CTRLTokenizer</td>\n",
       "      <td>sshleifer/tiny-ctrl</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>camembert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>sarnikowski/convbert-medium-small-da-cased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deberta_v2</td>\n",
       "      <td>DebertaV2Tokenizer</td>\n",
       "      <td>microsoft/deberta-v2-xlarge</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizerFast</td>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>monologg/electra-small-finetuned-imdb</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>flaubert/flaubert_small_cased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>huggingface/funnel-small-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>kssteven/ibert-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>led</td>\n",
       "      <td>LEDTokenizerFast</td>\n",
       "      <td>allenai/led-base-16384</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>microsoft/layoutlm-base-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>allenai/longformer-base-4096</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mbart</td>\n",
       "      <td>MBartTokenizerFast</td>\n",
       "      <td>sshleifer/tiny-mbart</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>microsoft/mpnet-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>google/mobilebert-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>openai</td>\n",
       "      <td>OpenAIGPTTokenizerFast</td>\n",
       "      <td>openai-gpt</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>squeezebert/squeezebert-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>TransfoXLTokenizer</td>\n",
       "      <td>transfo-xl-wt103</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>xlm-mlm-en-2048</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>xlnet-base-cased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `blurr.data.core` module contains the fundamental bits for all data preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
