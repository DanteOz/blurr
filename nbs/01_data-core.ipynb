{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.core\n",
    "\n",
    "> This module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to turn your raw datasets into modelable `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, inspect\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce, partial\n",
    "from typing import Any, Callable, List, Optional, Union, Type\n",
    "\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.core import Datasets, DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, ItemGetter, RandomSplitter\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `imdb` to demonstrate how to configure your BLURR for sequence classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234ac47ad76846d1a849a3da7356b24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-65b5588450d6b196.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was horrible. I swear they didn't even write a script they just kinda winged it through out the whole movie. Ice-T was annoying as hell. *SPOILERS Phht more like reasons not to watch it* They sit down and eat breakfast for 20 minutes. he coulda been long gone. The ground was hard it would of been close to impossible to to track him with out dogs. And when ICE-T is on that Hill and uses that Spaz-15 Assault SHOTGUN like its a sniper rifle (and then cuts down a tree with eight shells?? It would take 1000's of shells to cut down a tree that size.) Shotguns and hand guns are conside...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have seen this movie at the cinema many years ago, and one thing surprised me so negatively that I could not see any redeeming virtues in the movies: Dennis Quaid was cast as a policeman that never smiles or grin, while his smile and grin are two of his trademarks. Danny Glover was cast as the bad guy, but - again - most viewers' imagination could not go far enough as to believe him in that role. Also, Jared Leto was not believable as the former medicine student. The tension was just not there, since the killer was known very early. The finale was, again, neither dramatic nor tense: nobo...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a fantastic series first and foremost. It is very well done and very interesting. As a huge WWII buff, I had learned a lot before seeing this series. One of the best things this has going for it is all the interviews with past individuals back when the war was relatively fresh in their minds, comparatively speaking that is. It is nothing against the men that you see getting interviewed in the programs of today, it is just that most of these men weren't really involved in the upper echelons of what was happening then. One of the best parts is the narrating by Sir Laurence Oliver. I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurosawa really blew it on this one. Every genius is allowed a failure. The concept is fine but the execution is badly blurred.&lt;br /&gt;&lt;br /&gt;There is an air of fantasy about this film making it something of an art film. The poverty stricken of Tokyo deserve a fairer and more realistic portrayal. Many of them have interesting stories to tell. A very disappointing film.</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MGM were unsure of how to market Garbo when she first arrived in Hollywood. Mayer had a lot of faith in her and her appearance in \"Torrent\" justified that. She did not speak a word of English so she must have found it difficult to work, also Ricardo Cortez did not make it very easy for her.&lt;br /&gt;&lt;br /&gt;The torrent of the title is the river Juscar that winds through a sleepy little village in Spain. Leonora (Greta Garbo) hopes someday that her voice will bring great wealth and happiness to her struggling parents. Leonora and Don Rafael (Ricardo Cortez) are in love but he is under his mother'...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  This movie was horrible. I swear they didn't even write a script they just kinda winged it through out the whole movie. Ice-T was annoying as hell. *SPOILERS Phht more like reasons not to watch it* They sit down and eat breakfast for 20 minutes. he coulda been long gone. The ground was hard it would of been close to impossible to to track him with out dogs. And when ICE-T is on that Hill and uses that Spaz-15 Assault SHOTGUN like its a sniper rifle (and then cuts down a tree with eight shells?? It would take 1000's of shells to cut down a tree that size.) Shotguns and hand guns are conside...   \n",
       "1  I have seen this movie at the cinema many years ago, and one thing surprised me so negatively that I could not see any redeeming virtues in the movies: Dennis Quaid was cast as a policeman that never smiles or grin, while his smile and grin are two of his trademarks. Danny Glover was cast as the bad guy, but - again - most viewers' imagination could not go far enough as to believe him in that role. Also, Jared Leto was not believable as the former medicine student. The tension was just not there, since the killer was known very early. The finale was, again, neither dramatic nor tense: nobo...   \n",
       "2  This is a fantastic series first and foremost. It is very well done and very interesting. As a huge WWII buff, I had learned a lot before seeing this series. One of the best things this has going for it is all the interviews with past individuals back when the war was relatively fresh in their minds, comparatively speaking that is. It is nothing against the men that you see getting interviewed in the programs of today, it is just that most of these men weren't really involved in the upper echelons of what was happening then. One of the best parts is the narrating by Sir Laurence Oliver. I ...   \n",
       "3                                                                                                                                                                                                                                         Kurosawa really blew it on this one. Every genius is allowed a failure. The concept is fine but the execution is badly blurred.<br /><br />There is an air of fantasy about this film making it something of an art film. The poverty stricken of Tokyo deserve a fairer and more realistic portrayal. Many of them have interesting stories to tell. A very disappointing film.   \n",
       "4  MGM were unsure of how to market Garbo when she first arrived in Hollywood. Mayer had a lot of faith in her and her appearance in \"Torrent\" justified that. She did not speak a word of English so she must have found it difficult to work, also Ricardo Cortez did not make it very easy for her.<br /><br />The torrent of the title is the river Juscar that winds through a sleepy little village in Spain. Leonora (Greta Garbo) hopes someday that her voice will bring great wealth and happiness to her struggling parents. Leonora and Don Rafael (Ricardo Cortez) are in love but he is under his mother'...   \n",
       "\n",
       "   label  is_valid  \n",
       "0      0     False  \n",
       "1      0     False  \n",
       "2      1     False  \n",
       "3      0     False  \n",
       "4      1     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "raw_datasets[0] = raw_datasets[0].add_column(\"is_valid\", [False] * len(raw_datasets[0]))\n",
    "raw_datasets[1] = raw_datasets[1].add_column(\"is_valid\", [True] * len(raw_datasets[1]))\n",
    "\n",
    "final_ds = concatenate_datasets([raw_datasets[0].shuffle().select(range(1000)), raw_datasets[1].shuffle().select(range(200))])\n",
    "imdb_df = pd.DataFrame(final_ds)\n",
    "imdb_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[0].features[\"label\"].names\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"  # \"bert-base-multilingual-cased\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "    pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting with version 2.0, BLURR provides a preprocessing base class that can be used to build task specific pre-processed datasets from pandas DataFrames or Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # The attribute holding the text\n",
    "        text_attr: str = \"text\",\n",
    "        # The attribute holding the text_pair\n",
    "        text_pair_attr: Optional[str] = None,\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer\n",
    "        tok_kwargs: dict = {},\n",
    "    ):\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.text_attr, self.text_pair_attr = text_attr, text_pair_attr\n",
    "        self.tok_kwargs = tok_kwargs\n",
    "\n",
    "    def process_df(self, training_df: pd.DataFrame, validation_df: Optional[pd.DataFrame] = None):\n",
    "        df = training_df.copy()\n",
    "\n",
    "        # concatenate the validation dataset if it is included\n",
    "        if validation_df is not None:\n",
    "            valid_df = validation_df.copy()\n",
    "            # add an `is_valid_col` column to both training/validation DataFrames to indicate what data is part of the validation set\n",
    "            if self.is_valid_attr:\n",
    "                valid_df[self.is_valid_attr] = True\n",
    "                df[self.is_valid_attr] = False\n",
    "\n",
    "            df = pd.concat([df, valid_df])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_hf_dataset(self, training_ds: Dataset, validation_ds: Optional[Dataset] = None):\n",
    "        ds = training_ds\n",
    "\n",
    "        # concatenate the validation dataset if it is included\n",
    "        if validation_ds is not None:\n",
    "            # add an `is_valid_col` column to both training/validation DataFrames to indicate what data is part of\n",
    "            # the validation set\n",
    "            if self.is_valid_attr:\n",
    "                validation_ds = validation_ds.add_column(self.is_valid_attr, [True] * len(validation_ds))\n",
    "                training_ds = training_ds.add_column(self.is_valid_attr, [False] * len(training_ds))\n",
    "\n",
    "            ds = concatenate_datasets([training_ds, validation_ds])\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def _tokenize_function(self, example):\n",
    "        truncation = self.tok_kwargs.pop(\"truncation\", True)\n",
    "\n",
    "        txts = example[self.text_attr]\n",
    "        txt_pairs = example[self.text_pair_attr] if self.text_pair_attr else None\n",
    "        \n",
    "        return self.hf_tokenizer(txts, txt_pairs, truncation=truncation, **self.tok_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ClassificationPreprocessor`\n",
    "\n",
    "Starting with version 2.0, BLURR provides a sequence classification preprocessing class that can be used to preprocess DataFrames or Hugging Face Datasets.\n",
    "\n",
    "This class can be used for preprocessing both multiclass and multilabel classification datasets, and includes a `proc_{your_text_attr}` and `proc_{your_text_pair_attr}` (optional) attributes containing your modified text as a result of tokenization (e.g., if you specify a `max_length` the `proc_{your_text_attr}` may contain truncated text). \n",
    "\n",
    "**Note**: This class works for both slow and fast tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ClassificationPreprocessor(Preprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # Whether the dataset should be processed for multi-label; if True, will ensure `label_attrs` are\n",
    "        # converted to a value of either 0 or 1 indiciating the existence of the class in the example\n",
    "        is_multilabel: bool = False,\n",
    "        # The unique identifier in the dataset\n",
    "        id_attr: Optional[str] = None,\n",
    "        # The attribute holding the text\n",
    "        text_attr: str = \"text\",\n",
    "        # The attribute holding the text_pair\n",
    "        text_pair_attr: Optional[str] = None,\n",
    "        # The attribute holding the label(s) of the example\n",
    "        label_attrs: Union[str, List[str]] = \"label\",\n",
    "        # The attribute that should be created if your are processing individual training and validation\n",
    "        # datasets into a single dataset, and will indicate to which each example is associated\n",
    "        is_valid_attr: Optional[str] = \"is_valid\",\n",
    "        # A list indicating the valid labels for the dataset (optional, defaults to the unique set of labels\n",
    "        # found in the full dataset)\n",
    "        label_mapping: Optional[List[str]] = None,\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer\n",
    "        tok_kwargs: dict = {},\n",
    "    ):\n",
    "        tok_kwargs = {**tok_kwargs, \"return_offsets_mapping\": True}\n",
    "        super().__init__(hf_tokenizer, batch_size, text_attr, text_pair_attr, tok_kwargs)\n",
    "\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.id_attr = id_attr\n",
    "        self.label_attrs = label_attrs\n",
    "        self.is_valid_attr = is_valid_attr\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def process_df(self, training_df: pd.DataFrame, validation_df: Optional[pd.DataFrame] = None):\n",
    "        df = super().process_df(training_df, validation_df)\n",
    "\n",
    "        # convert even single \"labels\" to a list to make things easier\n",
    "        label_cols = listify(self.label_attrs)\n",
    "\n",
    "        # if `is_multilabel`, convert all targets to an int, 0 or 1, rounding floats if necessary\n",
    "        if self.is_multilabel:\n",
    "            for label_col in label_cols:\n",
    "                df[label_col] = df[label_col].apply(lambda v: int(bool(max(0, round(v)))))\n",
    "\n",
    "        # if a `label_mapping` is included, add a \"[label_col]_name\" field with the label Ids converted to their label names\n",
    "        if self.label_mapping:\n",
    "            for label_col in label_cols:\n",
    "                df[f\"{label_col}_name\"] = df[label_col].apply(lambda v: self.label_mapping[v])\n",
    "\n",
    "        # process df in mini-batches\n",
    "        final_df = pd.DataFrame()\n",
    "        for g, batch_df in df.groupby(np.arange(len(df)) // self.batch_size):\n",
    "            final_df = final_df.append(self._process_df_batch(batch_df))\n",
    "\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "        return final_df\n",
    "\n",
    "    def process_hf_dataset(self, training_ds: Dataset, validation_ds: Optional[Dataset] = None):\n",
    "        ds = super().process_hf_dataset(training_ds, validation_ds)\n",
    "        return Dataset.from_pandas(self.process_df(pd.DataFrame(ds)))\n",
    "\n",
    "    # ----- utility methods -----\n",
    "    def _process_df_batch(self, batch_df):\n",
    "        batch_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # grab our inputs\n",
    "        inputs = self._tokenize_function(batch_df.to_dict(orient=\"list\"))\n",
    "\n",
    "        for txt_seq_idx, txt_attr in enumerate([self.text_attr, self.text_pair_attr]):\n",
    "            if txt_attr is None:\n",
    "                break\n",
    "\n",
    "            char_idxs = []\n",
    "            for idx, offset_mapping in enumerate(inputs[\"offset_mapping\"]):\n",
    "                text_offsets = [offset_mapping[i] for i, seq_id in enumerate(inputs.sequence_ids(idx)) if seq_id == txt_seq_idx]\n",
    "                char_idxs.append([min(text_offsets)[0], max(text_offsets)[1]])\n",
    "\n",
    "            batch_df = pd.concat(\n",
    "                [batch_df, pd.DataFrame(char_idxs, columns=[f\"{txt_attr}_start_char_idx\", f\"{txt_attr}_end_char_idx\"])], axis=1\n",
    "            )\n",
    "            batch_df.insert(\n",
    "                0,\n",
    "                f\"proc_{txt_attr}\",\n",
    "                batch_df.apply(lambda r: r[txt_attr][r[f\"{txt_attr}_start_char_idx\"] : r[f\"{txt_attr}_end_char_idx\"] + 1], axis=1),\n",
    "            )\n",
    "\n",
    "            return batch_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proc_text</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>label_name</th>\n",
       "      <th>text_start_char_idx</th>\n",
       "      <th>text_end_char_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was horrible. I swear they didn't even write a script they just kinda winged it through out</td>\n",
       "      <td>This movie was horrible. I swear they didn't even write a script they just kinda winged it through out the whole movie. Ice-T was annoying as hell. *SPOILERS Phht more like reasons not to watch it* They sit down and eat breakfast for 20 minutes. he coulda been long gone. The ground was hard it would of been close to impossible to to track him with out dogs. And when ICE-T is on that Hill and uses that Spaz-15 Assault SHOTGUN like its a sniper rifle (and then cuts down a tree with eight shells?? It would take 1000's of shells to cut down a tree that size.) Shotguns and hand guns are conside...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have seen this movie at the cinema many years ago, and one thing surprised me so negatively that I could</td>\n",
       "      <td>I have seen this movie at the cinema many years ago, and one thing surprised me so negatively that I could not see any redeeming virtues in the movies: Dennis Quaid was cast as a policeman that never smiles or grin, while his smile and grin are two of his trademarks. Danny Glover was cast as the bad guy, but - again - most viewers' imagination could not go far enough as to believe him in that role. Also, Jared Leto was not believable as the former medicine student. The tension was just not there, since the killer was known very early. The finale was, again, neither dramatic nor tense: nobo...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     proc_text  \\\n",
       "0      This movie was horrible. I swear they didn't even write a script they just kinda winged it through out    \n",
       "1  I have seen this movie at the cinema many years ago, and one thing surprised me so negatively that I could    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  This movie was horrible. I swear they didn't even write a script they just kinda winged it through out the whole movie. Ice-T was annoying as hell. *SPOILERS Phht more like reasons not to watch it* They sit down and eat breakfast for 20 minutes. he coulda been long gone. The ground was hard it would of been close to impossible to to track him with out dogs. And when ICE-T is on that Hill and uses that Spaz-15 Assault SHOTGUN like its a sniper rifle (and then cuts down a tree with eight shells?? It would take 1000's of shells to cut down a tree that size.) Shotguns and hand guns are conside...   \n",
       "1  I have seen this movie at the cinema many years ago, and one thing surprised me so negatively that I could not see any redeeming virtues in the movies: Dennis Quaid was cast as a policeman that never smiles or grin, while his smile and grin are two of his trademarks. Danny Glover was cast as the bad guy, but - again - most viewers' imagination could not go far enough as to believe him in that role. Also, Jared Leto was not believable as the former medicine student. The tension was just not there, since the killer was known very early. The finale was, again, neither dramatic nor tense: nobo...   \n",
       "\n",
       "   label  is_valid label_name  text_start_char_idx  text_end_char_idx  \n",
       "0      0     False        neg                    0                102  \n",
       "1      0     False        neg                    0                106  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ClassificationPreprocessor(hf_tokenizer, label_mapping=labels, tok_kwargs={\"max_length\": 24})\n",
    "proc_df = preprocessor.process_df(imdb_df)\n",
    "proc_df.columns, len(proc_df)\n",
    "proc_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Hugging Face `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['proc_text', 'text', 'label', 'is_valid', 'label_name', 'text_start_char_idx', 'text_end_char_idx'],\n",
       "    num_rows: 1200\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ClassificationPreprocessor(hf_tokenizer, label_mapping=labels)\n",
    "proc_ds = preprocessor.process_hf_dataset(final_ds)\n",
    "proc_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextInput`\n",
    "\n",
    "A `TextInput` object is returned from the decodes method of `BatchDecodeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TextInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchTokenizeTransform` \n",
    "\n",
    "Inspired by this [article](https://docs.fast.ai/tutorial.transformers.html), inputs can come in as raw **text**, **a list of words** (e.g., tasks like Named Entity Recognition (NER), where you want to predict the label of each token), or as a **dictionary** that includes extra information you want to use during post-processing.\n",
    "\n",
    "**On-the-fly Batch-Time Tokenization**: \n",
    "\n",
    "Part of the inspiration for this derives from the mechanics of Hugging Face tokenizers, in particular it can return a collated mini-batch of data given a list of sequences. As such, the collating required for our inputs can be done during tokenization ***before*** our batch transforms run in a `before_batch_tfms` transform (where we get a list of examples)! This allows users of BLURR to have everything done dynamically at batch-time without prior preprocessing with at least four potential benefits:\n",
    "1. Less code\n",
    "2. Faster mini-batch creation\n",
    "3. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "4. Flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchTokenizeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Handles everything you need to assemble a mini-batch of inputs and targets, as well as\n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        \"\"\"\n",
    "        This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        is_dict = isinstance(samples[0][0], dict) \n",
    "        test_inp = samples[0][0][\"text\"] if is_dict else samples[0][0]\n",
    "\n",
    "        if is_listy(test_inp) and not self.is_split_into_words:\n",
    "            if is_dict:\n",
    "                inps = [(item[\"text\"][0], item[\"text\"][1]) for item in samples.itemgot(0).items]\n",
    "            else:\n",
    "                inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = [item[\"text\"] for item in samples.itemgot(0).items] if is_dict else samples.itemgot(0).items\n",
    "\n",
    "        inputs = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs\n",
    "        )\n",
    "\n",
    "        d_keys = inputs.keys()\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), as well as extra information\n",
    "        # if the inputs is a dictionary.\n",
    "        # (< 2.0.0): updated_samples = [(*[{k: inputs[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "        updated_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            inps = {k: inputs[k][idx] for k in d_keys}\n",
    "            if is_dict:\n",
    "                inps = {**inps, **{k: v for k,v in sample[0].items() if k not in ['text']}}\n",
    "\n",
    "            trgs = sample[1:]\n",
    "            if self.include_labels:\n",
    "                inps[\"labels\"] = [trg.clone() for trg in trgs]\n",
    "\n",
    "            updated_samples.append((*[inps], *trgs))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchDecodeTransform`\n",
    "\n",
    "As of fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here (one that occurs \"after\" the batch has been created) that will do the decoding for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchDecodeTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs as `input_return_type` for fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(self, input_return_type: Type = TextInput, **kwargs):\n",
    "        store_attr()\n",
    "\n",
    "    def decodes(self, items: dict):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        return self.input_return_type(items[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextBlock`\n",
    "\n",
    "A basic `DataBlock` for our inputs, `TextBlock` is designed with sensible defaults to minimize user effort in defining their transforms pipeline. It handles setting up your `BatchTokenizeTransform` and `BatchDecodeTransform` transforms regardless of data source (e.g., this will work with files, DataFrames, whatever). \n",
    "\n",
    "**Note**: You must either pass in your own instance of a `BatchTokenizeTransform` class or the Hugging Face objects returned from `BLURR.get_hf_objects` (e.g.,architecture, config, tokenizer, and model). The other args are optional.\n",
    "\n",
    "We also include a `blurr_sort_func` that works with `SortedDL` to properly sort based on the number of tokens in each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def blurr_sort_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "    # if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    txt = example[0][\"text\"] if isinstance(example[0], dict) else example[0]\n",
    "    return len(txt) if is_split_into_words else len(hf_tokenizer.tokenize(txt, **tok_kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your inputs for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: Optional[str] = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: Optional[PretrainedConfig] = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: Optional[PreTrainedModel] = None,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "        # The before_batch_tfm you want to use to tokenize your raw data on the fly\n",
    "        # (defaults to an instance of `BatchTokenizeTransform`)\n",
    "        batch_tokenize_tfm: Optional[BatchTokenizeTransform] = None,\n",
    "        # The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods,\n",
    "        # (defaults to BatchDecodeTransform)\n",
    "        batch_decode_tfm: Optional[BatchDecodeTransform] = None,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: Optional[int] = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = TextInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: Optional[DataLoader] = None,\n",
    "        # Any keyword arguments you want applied to your `batch_tokenize_tfm`\n",
    "        batch_tokenize_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to your `batch_decode_tfm` (will be set as a fastai `batch_tfms`)\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `TextBlock`\n",
    "        **kwargs\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and batch_tokenize_tfm is None:\n",
    "            raise ValueError(\"You must supply an hf_arch, hf_config, hf_tokenizer, hf_model -or- a BatchTokenizeTransform\")\n",
    "\n",
    "        if batch_tokenize_tfm is None:\n",
    "            batch_tokenize_tfm = BatchTokenizeTransform(\n",
    "                hf_arch,\n",
    "                hf_config,\n",
    "                hf_tokenizer,\n",
    "                hf_model,\n",
    "                include_labels=include_labels,\n",
    "                ignore_token_id=ignore_token_id,\n",
    "                max_length=max_length,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                is_split_into_words=is_split_into_words,\n",
    "                tok_kwargs=tok_kwargs.copy(),\n",
    "                **batch_tokenize_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if batch_decode_tfm is None:\n",
    "            batch_decode_tfm = BatchDecodeTransform(input_return_type=input_return_type, **batch_decode_kwargs.copy())\n",
    "\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                blurr_sort_func,\n",
    "                hf_tokenizer=batch_tokenize_tfm.hf_tokenizer,\n",
    "                is_split_into_words=batch_tokenize_tfm.is_split_into_words,\n",
    "                tok_kwargs=batch_tokenize_tfm.tok_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "            dl_type = partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs={\"before_batch\": batch_tokenize_tfm}, batch_tfms=batch_decode_tfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level API\n",
    "\n",
    "For working with PyTorch and/or fast.ai Datasets & DataLoaders, the low-level API allows you to get back fast.ai specific features such as `show_batch`, `show_results`, etc... when using plain ol' PyTorch Datasets, Hugging Face Datasets, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class BlurrBatchCreator:\n",
    "    \"\"\"\n",
    "    A class that can be assigned to a `TfmdDL.create_batch` method; used to in Blurr's low-level API\n",
    "    to create batches that can be used in the Blurr library\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Your Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator: Type = None,\n",
    "    ):\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.data_collator = data_collator if (data_collator) else DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "\n",
    "    def __call__(self, features):  # A mini-batch (list of examples to run through your model)\n",
    "        \"\"\"This method will collate your data using `self.data_collator` and add a target element to the\n",
    "        returned tuples if `labels` are defined as is the case when most Hugging Face datasets\n",
    "        \"\"\"\n",
    "        batch = self.data_collator(features)\n",
    "        if isinstance(features[0], dict):\n",
    "            return dict(batch), batch[\"labels\"] if (\"labels\" in features[0]) else dict(batch)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BlurrBatchDecodeTransform(BatchDecodeTransform):\n",
    "    \"\"\"A class used to cast your inputs into something understandable in fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: Optional[str] = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: Optional[PretrainedConfig] = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: Optional[PreTrainedModel] = None,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID to ignore when calculating loss/metrics\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any text generation keyword arguments\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = TextInput,\n",
    "        # Any other keyword arguments you need to pass to `BatchDecodeTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(input_return_type=input_return_type)\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates()\n",
    "class BlurrDataLoader(TfmdDL):\n",
    "    \"\"\"A class that makes creating a fast.ai `DataLoader` that works with Blurr\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A standard PyTorch Dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `BatchTokenizeTransform`\n",
    "        # to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel,\n",
    "        # An instance of `BlurrBatchCreator` or equivalent (defaults to `BlurrBatchCreator`)\n",
    "        batch_creator: Optional[BlurrBatchCreator] = None,\n",
    "        # The batch_tfm used to decode Blurr batches (defaults to `BlurrBatchDecodeTransform`)\n",
    "        batch_decode_tfm: Optional[BlurrBatchDecodeTransform] = None,\n",
    "        # (optional) A preprocessing function that will be applied to your dataset\n",
    "        preproccesing_func: Callable[\n",
    "            [Union[torch.utils.data.dataset.Dataset, Datasets], PreTrainedTokenizerBase, PreTrainedModel],\n",
    "            Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        ] = None,\n",
    "        # Keyword arguments to be applied to your `batch_decode_tfm`\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Keyword arguments to be applied to `BlurrDataLoader`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if preproccesing_func:\n",
    "            dataset = preproccesing_func(dataset, hf_tokenizer, hf_model)\n",
    "\n",
    "        if \"create_batch\" in kwargs:\n",
    "            kwargs.pop(\"create_batch\")\n",
    "        if not batch_creator:\n",
    "            batch_creator = BlurrBatchCreator(hf_tokenizer=hf_tokenizer)\n",
    "\n",
    "        if \"after_batch\" in kwargs:\n",
    "            kwargs.pop(\"after_batch\")\n",
    "        if not batch_decode_tfm:\n",
    "            batch_decode_tfm = BlurrBatchDecodeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, **batch_decode_kwargs.copy())\n",
    "\n",
    "        super().__init__(dataset=dataset, create_batch=batch_creator, after_batch=batch_decode_tfm, **kwargs)\n",
    "        store_attr(names=\"hf_arch, hf_config, hf_tokenizer, hf_model\")\n",
    "\n",
    "    def new(\n",
    "        self,\n",
    "        # A standard PyTorch and fastai dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets] = None,\n",
    "        # The class you want to create an instance of (will be \"self\" if None)\n",
    "        cls: Type = None,\n",
    "        #  Any additional keyword arguments you want to pass to the __init__ method of `cls`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"We have to override the new method in order to add back the Hugging Face objects in this factory\n",
    "        method (called for example in places like `show_results`). With the exception of the additions to the kwargs\n",
    "        dictionary, the code below is pulled from the `DataLoaders.new` method as is.\n",
    "        \"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "        if cls is None:\n",
    "            cls = type(self)\n",
    "\n",
    "        cur_kwargs = dict(\n",
    "            dataset=dataset,\n",
    "            num_workers=self.fake_l.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            timeout=self.timeout,\n",
    "            bs=self.bs,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last,\n",
    "            indexed=self.indexed,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        for n in self._methods:\n",
    "            o = getattr(self, n)\n",
    "            if not isinstance(o, MethodType):\n",
    "                cur_kwargs[n] = o\n",
    "\n",
    "        # we need to add these arguments back in (these, after_batch, and create_batch will go in as kwargs)\n",
    "        kwargs[\"hf_arch\"] = self.hf_arch\n",
    "        kwargs[\"hf_config\"] = self.hf_config\n",
    "        kwargs[\"hf_tokenizer\"] = self.hf_tokenizer\n",
    "        kwargs[\"hf_model\"] = self.hf_model\n",
    "\n",
    "        return cls(**merge(cur_kwargs, kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility classes and methods \n",
    "\n",
    "These methods are use internally for getting blurr transforms associated to your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = BatchTokenizeTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_blurr_tfm\" class=\"doc_header\"><code>get_blurr_tfm</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_blurr_tfm</code>(**`tfms_list`**:`Pipeline`, **`tfm_class`**:`Transform`=*`BatchTokenizeTransform`*)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`tfms_list`** : *`<class 'fastcore.transform.Pipeline'>`*\t<p>A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)</p>\n",
       "\n",
       "\n",
       " - **`tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The transform to find</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_blurr_tfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_blurr_tfm(\n",
    "    # Your fast.ai `DataLoaders\n",
    "    dls: DataLoaders,  \n",
    "    # The Blurr transforms to look for in order\n",
    "    tfms: List[Transform] = [BatchTokenizeTransform, BatchDecodeTransform, BlurrBatchDecodeTransform]\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as\n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    for tfm in tfms:\n",
    "        found_tfm = get_blurr_tfm(dls.before_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n",
    "\n",
    "        found_tfm = get_blurr_tfm(dls.after_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"first_blurr_tfm\" class=\"doc_header\"><code>first_blurr_tfm</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>first_blurr_tfm</code>(**`dls`**:`DataLoaders`, **`tfms`**:`List`\\[`Transform`\\]=*`[<class '__main__.BatchTokenizeTransform'>, <class '__main__.BatchDecodeTransform'>, <class '__main__.BlurrBatchDecodeTransform'>]`*)\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`dls`** : *`<class 'fastai.data.core.DataLoaders'>`*\t<p>Your fast.ai `DataLoaders</p>\n",
       "\n",
       "\n",
       " - **`tfms`** : *`typing.List[fastcore.transform.Transform]`*, *optional*\t<p>The Blurr transforms to look for in order</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(first_blurr_tfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `show_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our labels list, we'll use it to look up the value of our target(s)\n",
    "    trg_labels = tfm.kwargs[\"labels\"] if (\"labels\" in tfm.kwargs) else None\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    for idx, (input_ids, label, sample) in enumerate(zip(x, y, samples)):\n",
    "        if idx >= max_n:\n",
    "            break\n",
    "\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "        for item in sample[n_inp:]:\n",
    "            if not torch.is_tensor(item):\n",
    "                trg = trg_labels[int(item)] if trg_labels else item\n",
    "            elif is_listy(item.tolist()):\n",
    "                trg = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.item()\n",
    "            else:\n",
    "                trg = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "The following eamples demonstrate several approaches to construct your `DataBlock` for sequence classication tasks using the mid-level API, and also an example on how to accomplish the same using the low-level API and standard PyTorch/Hugging Face/fast.ai Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Get your Hugging Face objects.\n",
    "\n",
    "There are a bunch of ways we can get at the four Hugging Face elements we need (e.g., architecture name, tokenizer, config, and model).  We can just create them directly, or we can use one of the helper methods available via `BLURR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"  # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 512]), 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anyone who visited drive-ins in the 1950s, 60s, and 70s, must have seen a film or two by American International Pictures, a distributor that resembled 1980s giant Cannon Films. Wherever movie-goers ventured, AIP would be right there to supply the latest en vogue titles - in the 50s came horror movies like 'Voodoo Woman' and 'The Undead;' in the 60s were Frankie Avalon-Annette Funicello beach comedies and biker flicks like 'The Glory Stompers;' and into the 70s, AIP churned out grindhouse-level</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Was this a comedy or was it a drama? I begin this review by asking this question because the film that I just witnessed, Hollywood Shuffle, was neither funny or rather dramatic. While it tried so hard to make a point, because of this lack of definition (comedy or drama), the clever themes and pointed remarks were lost. While I am a strong believer that there is too much racial profiling happening in Hollywood, even today, I do not believe that Townsend's directorial debut did much to stop it. I</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a preprocessed dataset\n",
    "\n",
    "Preprocessing your raw data is the more traditional approach to using Transformers. It is required, for example, when you want to work with documents longer than your model will allow. A preprocessed dataset is used in the same way a non-preprocessed dataset is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1a: Get your Hugging Face objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1b. Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['proc_text', 'text', 'label', 'is_valid', 'label_name', 'text_start_char_idx', 'text_end_char_idx'],\n",
       "    num_rows: 1200\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ClassificationPreprocessor(hf_tokenizer, label_mapping=labels)\n",
    "proc_ds = preprocessor.process_hf_dataset(final_ds)\n",
    "proc_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ItemGetter(\"proc_text\"), get_y=ItemGetter(\"label\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(proc_ds, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film is bundled along with \"Gli fumavano le Colt... lo chiamavano Camposanto\" and both films leave a lot to be desired in the way of their DVD prints. First, both films are very dark--occasionally making it hard to see exactly what's happening. Second, neither film has subtitles and you are forced to watch a dubbed film--though \"Il Prezzo del Potere\" does seem to have a better dub. Personally, I always prefer subtitles but for the non-purists out there this isn't a problem. These DVD probl</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Written and directed by Steve Gordon. Running time: 97 minutes. Classified PG.&lt;br /&gt;&lt;br /&gt;It was the quintessential comedy of the decade. It won Sir John Gielgud the Academy Award. It was even featured in VH1's \"I Love the 80's.\" And it looks just as good today as it did upon it's initial release. Arthur is the acclaimed comedy classic about a drunken millionaire (played with enthusiasm and wit by Dudley Moore in an Oscar-nominated performance) who must choose between the woman he loves and the</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing extra information\n",
    "\n",
    "As of v.2, BLURR now also allows you to pass extra information alongside your inputs in the form of a dictionary.  If you use this approach, you must assign your text(s) to the `text` attribute of the dictionary.  This is a useful approach when splitting long documents into chunks, but wanting to score/predict by example rather than chunk (for example in extractive question answering tasks).\n",
    "\n",
    "**Note**: A good place to access to this extra information during training/validation is in the `before_batch` method of a `Callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "\n",
    "def get_x(item):\n",
    "    return {\"text\": item.text, \"another_val\": \"testing123\" }\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=ColReader(\"label\"), splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 512]), 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anyone who visited drive-ins in the 1950s, 60s, and 70s, must have seen a film or two by American International Pictures, a distributor that resembled 1980s giant Cannon Films. Wherever movie-goers ventured, AIP would be right there to supply the latest en vogue titles - in the 50s came horror movies like 'Voodoo Woman' and 'The Undead;' in the 60s were Frankie Avalon-Annette Funicello beach comedies and biker flicks like 'The Glory Stompers;' and into the 70s, AIP churned out grindhouse-level</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Jacksons version(s) are better films overall from objective point of view. That being said, they are not my favorite screen versions of Lord Of The Rings, and let me explain why. &lt;br /&gt;&lt;br /&gt;Firstly, the acting of the on-screen characters is just too ordinary and uninspiring with Jackson's LOTR. The whole cast is too run of the mill. \"Are you claiming that those silly cartoon characters of Ralph Bakshi version are better actors than real people?\" one could ask. Well, they are not really s</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Build your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0826e1d54e27409fa661c7db68301d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6e3b62256f7dcee8.arrow\n",
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a37d5e554eb61497.arrow\n",
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-83f60d5306f74c84.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return hf_tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Dataset pre-processing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def preproc_hf_dataset(\n",
    "    # A standard PyTorch Dataset or fast.ai Datasets\n",
    "    dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A Hugging Face model\n",
    "    hf_model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training\n",
    "    libraries\n",
    "    \"\"\"\n",
    "    if (\"label\") in dataset.column_names:\n",
    "        dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    hf_model_fwd_args = list(inspect.signature(hf_model.forward).parameters.keys())\n",
    "    bad_cols = set(dataset.column_names).difference(hf_model_fwd_args)\n",
    "    dataset = dataset.remove_columns(bad_cols)\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build your `DataLoaders`.\n",
    "\n",
    "Use `BlurrDataLoader` to build Blurr friendly dataloaders from your datasets. Passing `{'labels': label_names}` to your `batch_tfm_kwargs` will ensure that your lable/target names will be displayed in methods like `show_batch` and `show_results` (just as it works with the mid-level API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"label\"].names\n",
    "\n",
    "trn_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 71])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sony's portfolio gives subscribers a variety of personalized services including the ability to download and experience images, ring tones, music videos and other music entertainment services. Sony Music's portfolio of products and services currently enable carriers to offer subscribers the ability to download images, ring tones, music videos and other entertainment services.</td>\n",
       "      <td>equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The goal will be to reduce congestion by upgrading road, building tollways, facilitating public transit or any other means to improve mobility. The money can be used for road upgrades, tollways, public transit or any other means to improve mobility.</td>\n",
       "      <td>equivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForSequenceClassification',\n",
       " 'BartForSequenceClassification',\n",
       " 'BertForSequenceClassification',\n",
       " 'BigBirdForSequenceClassification',\n",
       " 'BigBirdPegasusForSequenceClassification',\n",
       " 'CTRLForSequenceClassification',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CanineForSequenceClassification',\n",
       " 'ConvBertForSequenceClassification',\n",
       " 'DebertaForSequenceClassification',\n",
       " 'DebertaV2ForSequenceClassification',\n",
       " 'DistilBertForSequenceClassification',\n",
       " 'ElectraForSequenceClassification',\n",
       " 'FNetForSequenceClassification',\n",
       " 'FlaubertForSequenceClassification',\n",
       " 'FunnelForSequenceClassification',\n",
       " 'GPT2ForSequenceClassification',\n",
       " 'GPTJForSequenceClassification',\n",
       " 'GPTNeoForSequenceClassification',\n",
       " 'HubertForSequenceClassification',\n",
       " 'IBertForSequenceClassification',\n",
       " 'LEDForSequenceClassification',\n",
       " 'LayoutLMForSequenceClassification',\n",
       " 'LayoutLMv2ForSequenceClassification',\n",
       " 'LongformerForSequenceClassification',\n",
       " 'MBartForSequenceClassification',\n",
       " 'MPNetForSequenceClassification',\n",
       " 'MegatronBertForSequenceClassification',\n",
       " 'MobileBertForSequenceClassification',\n",
       " 'NystromformerForSequenceClassification',\n",
       " 'OpenAIGPTForSequenceClassification',\n",
       " 'PerceiverForSequenceClassification',\n",
       " 'ReformerForSequenceClassification',\n",
       " 'RemBertForSequenceClassification',\n",
       " 'RoFormerForSequenceClassification',\n",
       " 'RobertaForSequenceClassification',\n",
       " 'SEWDForSequenceClassification',\n",
       " 'SEWForSequenceClassification',\n",
       " 'SqueezeBertForSequenceClassification',\n",
       " 'TransfoXLForSequenceClassification',\n",
       " 'UniSpeechForSequenceClassification',\n",
       " 'UniSpeechSatForSequenceClassification',\n",
       " 'Wav2Vec2ForSequenceClassification',\n",
       " 'WavLMForSequenceClassification',\n",
       " 'XLMForSequenceClassification',\n",
       " 'XLMRobertaForSequenceClassification',\n",
       " 'XLNetForSequenceClassification',\n",
       " 'YosoForSequenceClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"SequenceClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-random-bart\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"google/bigbird-pegasus-large-arxiv\",\n",
    "    \"hf-internal-testing/tiny-random-ctrl\",\n",
    "    \"camembert-base\",\n",
    "    \"hf-internal-testing/tiny-random-canine\",\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    \"hf-internal-testing/tiny-random-deberta-v2\",\n",
    "    \"hf-internal-testing/tiny-random-distilbert\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    \"google/fnet-base\",\n",
    "    \"hf-internal-testing/tiny-random-flaubert\",\n",
    "    \"hf-internal-testing/tiny-random-funnel\",\n",
    "    \"hf-internal-testing/tiny-random-gpt2\",\n",
    "    \"anton-l/gpt-j-tiny-random\",\n",
    "    \"hf-internal-testing/tiny-random-gpt_neo\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    \"hf-internal-testing/tiny-random-led\",\n",
    "    \"hf-internal-testing/tiny-random-longformer\",\n",
    "    \"hf-internal-testing/tiny-random-mbart\",\n",
    "    \"hf-internal-testing/tiny-random-mpnet\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                 could not test\n",
    "    \"hf-internal-testing/tiny-random-mobilebert\",\n",
    "    \"openai-gpt\",\n",
    "    \"google/reformer-crime-and-punishment\",\n",
    "    \"google/rembert\",\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    \"hf-internal-testing/tiny-random-transfo-xl\",\n",
    "    \"xlm-mlm-en-2048\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# for model_name in pretrained_model_names:\n",
    "#     tok = AutoTokenizer.from_pretrained(model_name)\n",
    "#     print(f'=== {model_name} ===')\n",
    "#     print(f'=== {tok.padding_side} ===')\n",
    "#     print(f'=== {tok.pad_token_id} ===')\n",
    "#     print(tok(['hi', 'hello everyone. its good to be here'], ['yo', 'yo'], padding='max_length', max_length=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9031fee112b94678be973396cee3e352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "raw_datasets = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "raw_datasets[0] = raw_datasets[0].add_column(\"is_valid\", [False] * len(raw_datasets[0]))\n",
    "raw_datasets[1] = raw_datasets[1].add_column(\"is_valid\", [True] * len(raw_datasets[1]))\n",
    "\n",
    "final_ds = concatenate_datasets([raw_datasets[0].shuffle().select(range(1000)), raw_datasets[1].shuffle().select(range(200))])\n",
    "imdb_df = pd.DataFrame(final_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. they're like the equivalent of a carton of mcdonald's fries--they taste really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is mostly a story about the growing relationship between jeff webster(jimmy stewart) and ronda castle(ruth roman). she takes an instant liking to jeff in a brief encounter on the deck of the steamer to skagway, and a longer look when he hides in her cabin while authorities seek him on a charge of murder. they find out they have some things in common besides an animal attraction. neither trusts a member of the opposite sex, apparently because both have been married to spouses who cheated on them. gra</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-bart ===\n",
      "\n",
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We all know that some of the greatest movies of all time were based on books. While not particularly accurate adaptations, these movies were nonetheless excellent films. Some great examples are the Harry Potter series, the Lord of the Rings trilogy, and, to a lesser extent, almost every Disney film ever made. However, I must regretfully announ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don't take a long time to develop ; basically lean and mean - - he sets them up quickly in a scarce number of pages, you read'em, and you're finished before you know you've begun. they're like the equivalent of a carton of mcdonald's fries - - they taste really good and you know there's not much nutritional value in them ( re : from a literary standpoint, they don't say much about the universal human condition ), but you're still gonna scarf '</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well, what can i say. &lt; br / &gt; &lt; br / &gt; \" what the bleep do we know \" has achieved the nearly impossible - leaving behind such masterpieces of the genre as \" the postman \", \" the dungeon master \", \" merlin \", and so fourth, it will go down in history as the single worst movie i have ever seen in its entirety. and that, ladies and gentlemen, is impressive indeed, for i have seen many a bad movie. &lt; br / &gt; &lt; br / &gt; this masterpiece of modern cinema consists of two interwoven parts, alternating between a silly and con</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf 'em down, just don't be a pig and go for the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WWE has produced some of the worst pay-per-views in its history over the past few months. Cyber Sunday, Survivor Series and December to Dismember were appalling to say the least and so it was relying on its B brand show, Smackdown! to attempt to end the year on a high note. Armageddon had two major gimmick matches in the Last Ride and Inferno matches, three Championships were on the line and an interesting main event in the shape of a tag team war featuring Batista and John Cena against King Booker and Finlay. However, it was an amendment to one of those Championship matches that brought us not only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-pegasus-large-arxiv ===\n",
      "\n",
      "architecture:\tbigbird_pegasus\n",
      "tokenizer:\tPegasusTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf 'em down, just</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"THINGS TO COME\" Movie Review by kWRicebr /&gt;br /&gt;Here is another wonder filled science fiction film from a different time and place. It is a film I've only seen in truncated parts, but Art should be taken as a whole. I experienced this film as it was designed, in a darkened theatre, on a silver screen, with whirling reels of film and an audience to share it with. That audience and myself were effected by this film! One woman who lived through WW II was choked up by the remembered ravages of war and replenished by the positive, albeit corny, ending.br /&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-ctrl ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/transformers/models/ctrl/modeling_ctrl.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model_size)\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tctrl\n",
      "tokenizer:\tCTRLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf 'em down, just don't be a pig and go for the extra-super-sized portion and fill up on too much grease (\"too much grease\" is a metaphor for the prose in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Following the appalling Attack Force, chances were that Seagal could only have a step up with Flight Of Fury. To out-stink Attack Force would take some doing. Flight Of Fury is a marked improvement overall, but still in the grand scheme of thinks, mediocre. Mediocrity is seemingly an achievement for Seagal these days, a sad insight into his movie career's decline. Where Attack Force was a hodge-podge of plot lines altered drastically from conception, to filming, to post production, Flight Of Fury keeps the plot line more simple. Someone steals a high-tech stealth fighter, planning to use it to fire chemical weapons (which we later, bizarrely discover, will destroy the whole world in 48 hrs).</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I cannot comment on this film without discussing its significance to me personally. As a child bad health prevented me from ever going to a cinema. I first encountered movies at the end of WWII through Roger Manvilles splendid Penguin book \"Film\", which brought me so much pleasure as my health began to improve that I wish I could buy another copy to re-read today. My introduction to many classic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-canine ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcanine\n",
      "tokenizer:\tCanineTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cartoon Network seems to be desperate for ratings. Beginning with the cancellation of Samurai Jack, the network seemed hellben</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don't take a long time to develop ; basically lean and mean - - he sets them up quickly in a scarce number of pages, you read'em, and you're finished before you know you've begun. they're like the equivalent of a carton of mcdonald's fries - - they taste really good and you know there's not much nutritional value in them ( re : from a literary standpoint, they don't say much about the universal human condition ), but you're still gonna scarf '</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we all know that some of the greatest movies of all time were based on books. while not particularly accurate adaptations, these movies were nonetheless excellent films. some great examples are the harry potter series, the lord of the rings trilogy, and, to a lesser extent, almost every disney film ever made. however, i must regretfully announce that a wrinkle in time is not one of those movies. not only does it fail to meet some of the most basic expectations of madeleine l'engle's fan base, it manages to defy the standards of scriptwriting, acting, special effects and, ultimately, respect for</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I cannot comment on this film without discussing its significance to me personally. As a child bad health prevented me from ever going to a cinema. I first encountered movies at the end of WWII through Roger Manvilles splendid Penguin book \"Film\", which brought me so much pleasure as my health began to improve that I wish I could</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-deberta-v2 ===\n",
      "\n",
      "architecture:\tdeberta_v2\n",
      "tokenizer:\tDebertaV2Tokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf 'em down,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The summer has been so full of Blockbusters and comebacks of films, and not to mention some of the disappointments of those comebacks, that I was woe to find a film I could just sit down and enjoy. &lt;br /&gt;&lt;br /&gt;In case you don't want to read further down the page (there aren't any spoilers), I'll sum it up here: It's more mature than Ella Enchanted (there are some questionably violent parts, plenty of death, and a handful of scenes with a little blood, not for small children), but doesn't try to be overly corny or overstep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-distilbert ===\n",
      "\n",
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don't take a long time to develop ; basically lean and mean - - he s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is mostly a story about the growing relationship between jeff webster ( jimmy stewart ) and ronda castle ( ruth roman ). she takes an instant liking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don't take a long time to develop ; basically lean and mean - - he sets them up quickly in a scarce number of pages, you read'em, and you're finished before you know you've begun. they're like the equivalent of a carton of mcdonald's fries - - they taste really good and you know there's not much nutritional value in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>michelle rodriguez is a well - built high - school senior who discovers that she has a powerful punch and begins amateur training at a brooklyn gym. santiago douglas is a a handsome young man, barely older than she, who also trains there. they meet after class, so to speak, and feel attracted to each other. no sex. santiago has been instructed to save it for his next bout.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/fnet-base ===\n",
      "\n",
      "architecture:\tfnet\n",
      "tokenizer:\tFNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone with a young boy in the house who won't watch black &amp; white movies should put this on their television set. When the child walks by, wondering what all the on screen shouting and shooting's about, tell him this is a picture for adults and that he isn't big enough to watch it yet. That'll hold him there for a few minutes; director George Stevens and his team will keep him to the end.&lt;br /&gt;&lt;br /&gt;I think my father did that to me, anyway, and I'm the better man for it. This classic adventure yarn, set</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-flaubert ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_class = RobertaTokenizer if (\"/ibert\" in model_name) else None\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, tokenizer_cls=tok_class)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz), CategoryBlock)\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n",
    "        dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\\n\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"PASSED\", \"\"))\n",
    "        dls.show_batch(dataloaders=dls, max_n=2, trunc_at=1000)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"FAILED\", err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `blurr.data.core` module contains the fundamental bits for all data preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
