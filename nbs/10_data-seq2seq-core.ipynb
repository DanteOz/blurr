{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.seq2seq.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.seq2seq.core\n",
    "\n",
    "> This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import reduce\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock, ColReader, ColSplitter\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, logging,\n",
    "    PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    ")\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import (\n",
    "    HF_TextBlock, HF_BaseInput, HF_BeforeBatchTransform, HF_AfterBatchTransform, first_blurr_tfm\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.7.1\n",
      "fastai: 2.5.2\n",
      "transformers: 4.9.2\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "import pdb\n",
    "\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import *\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.data.core import HF_TextBlock\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions('torch fastai transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bart',\n",
       " transformers.models.bart.configuration_bart.BartConfig,\n",
       " transformers.models.bart.tokenization_bart_fast.BartTokenizerFast,\n",
       " transformers.models.bart.modeling_bart.BartForConditionalGeneration)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=BartForConditionalGeneration)\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq tasks are essentially conditional generation tasks, this applies to specific derived tasks such as summarization and translation.  Given this, we can use the *same* HF_Seq2Seq transforms, `HF_Seq2SeqInput`, and `HF_Seq2SeqBlock` for these tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Seq2SeqInput(HF_BaseInput): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a subclass of `HF_BeforeBatchTransform` for summarization tasks to add `decoder_input_ids` and `labels` to our inputs during training, which will in turn allow the Hugging Face model to calculate the loss for us.  See [here](https://huggingface.co/transformers/glossary.html#labels) and [here](https://huggingface.co/transformers/glossary.html#decoder-input-ids) for more information on these additional inputs used in summarization, translation, and conversational training tasks. How they should look for particular architectures can be found by looking at those model's `forward` function's docs (See [here](https://huggingface.co/transformers/model_doc/bart.html#transformers.BartModel.forward) for BART for example)\n",
    "\n",
    "Note also that `labels` is simply target_ids shifted to the right by one since the task to is to predict the next token based on the current (and all previous) `decoder_input_ids`.\n",
    "\n",
    "And lastly, we also update our targets to just be the `input_ids` of our target sequence so that fastai's `Learner.show_results` works (again, almost all the fastai bits require returning a single tensor to work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def default_text_gen_kwargs(hf_config, hf_model, task=None):\n",
    "    text_gen_kwargs = {}\n",
    "    hf_config_dict = hf_config.to_dict()\n",
    "\n",
    "    generate_func_args = list(inspect.signature(hf_model.generate).parameters.keys())\n",
    "    for k in generate_func_args:\n",
    "        if (k in hf_config_dict): text_gen_kwargs.update({k: hf_config_dict[k]})\n",
    "            \n",
    "    # not all configs even have a task_specific_params property\n",
    "    if (task is not None):\n",
    "        try:\n",
    "            text_gen_kwargs = { **text_gen_kwargs, **hf_config.task_specific_params[task] }\n",
    "        except: pass\n",
    "        \n",
    "    return text_gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 142,\n",
       " 'min_length': 56,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': True,\n",
       " 'num_beams': 4,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'bad_words_ids': None,\n",
       " 'bos_token_id': 0,\n",
       " 'pad_token_id': 1,\n",
       " 'eos_token_id': 2,\n",
       " 'length_penalty': 2.0,\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'num_return_sequences': 1,\n",
       " 'decoder_start_token_id': 2,\n",
       " 'use_cache': True,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': 0,\n",
       " 'forced_eos_token_id': 2,\n",
       " 'remove_invalid_values': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_text_gen_kwargs(hf_config, hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5688, -0.4156],\n",
       "        [ 1.0000, -0.1378, -0.5449],\n",
       "        [ 1.0000,  0.1534,  0.0900]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "t = torch.randn((3,3));\n",
    "\n",
    "F.pad(t, pad=(1,0), value=1)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Seq2SeqBeforeBatchTransform(HF_BeforeBatchTransform):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch:str,   \n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config:PretrainedConfig,   \n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer:PreTrainedTokenizerBase,  \n",
    "        # A Hugging Face model\n",
    "        hf_model:PreTrainedModel,      \n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id:int=CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation of the input sequence. It can be an integer or None, \n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no \n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length:int=None, \n",
    "        # To control the length of the padding/truncation of the target sequence. It can be an integer or None, \n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no \n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_target_length:int=None, \n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to \n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding:Union[bool, str]=True, \n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation:Union[bool, str]=True, \n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words:bool=False, \n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs={}, \n",
    "        # Any keyword arguments to pass to the `hf_model.generate` method\n",
    "        text_gen_kwargs={}, \n",
    "        # Keyword arguments to apply to `HF_BeforeBatchTransform`\n",
    "        **kwargs\n",
    "    ):     \n",
    "        super().__init__(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                         max_length=max_length, padding=padding, truncation=truncation, is_split_into_words=False, \n",
    "                         tok_kwargs=tok_kwargs.copy(), **kwargs)\n",
    "        \n",
    "        store_attr(self=self, names='text_gen_kwargs, max_target_length, ignore_token_id')\n",
    "    \n",
    "    def encodes(self, samples): \n",
    "        samples = L(samples)\n",
    "        \n",
    "        # tokenize\n",
    "        src_texts=samples.itemgot(0).items\n",
    "        tgt_texts=samples.itemgot(1).items if (len(samples[0]) > 1) else None\n",
    "        \n",
    "        tok_d = self.hf_tokenizer(src_texts, max_length=self.max_length, padding=self.padding, \n",
    "                                  truncation=self.truncation, return_tensors='pt', **self.tok_kwargs)\n",
    "\n",
    "        if (tgt_texts):\n",
    "            with self.hf_tokenizer.as_target_tokenizer():\n",
    "                tok_d_targs = self.hf_tokenizer(tgt_texts, max_length=self.max_target_length, padding=self.padding, \n",
    "                                      truncation=self.truncation, return_tensors='pt', **self.tok_kwargs)\n",
    "\n",
    "                tok_d['labels'] = tok_d_targs['input_ids']\n",
    "        \n",
    "        # add in target ids for us to use if fastai is calculating the loss\n",
    "        targ_ids = [[]] * len(samples)\n",
    "        if ('labels' in tok_d):\n",
    "            tok_d['labels'].masked_fill_(tok_d['labels'] == self.ignore_token_id, self.hf_tokenizer.pad_token_id)\n",
    "            targ_ids = tok_d['labels'].clone()\n",
    "\n",
    "        # update samples with tokenized inputs (e.g. input_ids, attention_mask, etc...)\n",
    "        d_keys = tok_d.keys()\n",
    "        updated_samples= [ (*[{k: tok_d[k][idx] for k in d_keys}], *tuplify(targ_ids[idx]), *sample[2:]) \n",
    "                          for idx, sample in enumerate(samples) ]\n",
    "        \n",
    "        return updated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include a new AFTER batch `Transform` and `TransformBlock` specific to text-2-text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Seq2SeqAfterBatchTransform(HF_AfterBatchTransform):\n",
    "    def decodes(self, encoded_samples):\n",
    "        input_ids = encoded_samples['input_ids'] if (isinstance(encoded_samples, dict)) else encoded_samples\n",
    "        return self.input_return_type(input_ids, hf_tokenizer=self.hf_tokenizer)\n",
    "    \n",
    "    \n",
    "class HF_Seq2SeqBlock(HF_TextBlock):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch:str=None,          \n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_config:PretrainedConfig=None,    \n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer:PreTrainedTokenizerBase=None,  \n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_model:PreTrainedModel=None,                     \n",
    "        # The before batch transform you want to use to tokenize your raw data on the fly \n",
    "        # (defaults to an instance of `HF_BeforeBatchTransform` created using the Hugging Face objects defined above)\n",
    "        before_batch_tfm:HF_BeforeBatchTransform=None,             \n",
    "        # The batch_tfms to apply to the creation of your DataLoaders, \n",
    "        # (defaults to HF_AfterBatchTransform created using the Hugging Face objects defined above)\n",
    "        after_batch_tfm:HF_AfterBatchTransform=None,   \n",
    "        # To control the length of the padding/truncation for the input sequence. It can be an integer or None, \n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no \n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length:int=None,\n",
    "        # To control the length of the padding/truncation for the target sequence. It can be an integer or None, \n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no \n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-y\n",
    "        max_target_length=None, \n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to \n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding:Union[bool, str]=True, \n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation:Union[bool, str]=True, \n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type=HF_Seq2SeqInput, \n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type=SortedDL, \n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs={}, \n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        # (default: default_text_gen_kwargs)\n",
    "        text_gen_kwargs={},\n",
    "        # Any keyword arguments you want applied to `HF_TextBlock`\n",
    "        # Any keyword arguments you want applied to your before batch tfm\n",
    "        before_batch_kwargs={}, \n",
    "        # Any keyword arguments you want applied to your after batch tfm (or referred to in fastai as `batch_tfms`)\n",
    "        after_batch_kwargs={}, \n",
    "        # Any keyword arguments you want applied to `HF_TextBlock`\n",
    "        **kwargs\n",
    "    ):    \n",
    "        # we need to pass text_gen_kwargs into our HF_Seq2SeqBeforeBatchTransform (use default unless specified)\n",
    "        if (len(text_gen_kwargs) == 0): \n",
    "            if (hf_config is None): hf_config = before_batch_tfm.hf_config\n",
    "            if (hf_model is None): hf_model = before_batch_tfm.hf_model\n",
    "            self.text_gen_kwargs = default_text_gen_kwargs(hf_config, hf_model)\n",
    "        else:\n",
    "            self.text_gen_kwargs = text_gen_kwargs.copy()\n",
    "            \n",
    "        # construct our before_batch and after_batch tfms as usual\n",
    "        if (before_batch_tfm is None): \n",
    "            before_batch_tfm = HF_Seq2SeqBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                              max_length=max_length, \n",
    "                                                              max_target_length=max_target_length,\n",
    "                                                              padding=padding, \n",
    "                                                              truncation=truncation,\n",
    "                                                              tok_kwargs=tok_kwargs.copy(), \n",
    "                                                              text_gen_kwargs=text_gen_kwargs, \n",
    "                                                              **before_batch_kwargs.copy())\n",
    "\n",
    "        if (after_batch_tfm is None): \n",
    "            hf_tokenizer = hf_tokenizer if (hf_tokenizer is not None) else before_batch_tfm.hf_tokenizer\n",
    "            after_batch_tfm = HF_Seq2SeqAfterBatchTransform(hf_tokenizer, input_return_type,\n",
    "                                                            **after_batch_kwargs.copy())\n",
    "                \n",
    "        return super().__init__(before_batch_tfm=before_batch_tfm, after_batch_tfm=after_batch_tfm,\n",
    "                                max_length=max_length, padding=padding, truncation=truncation, \n",
    "                                is_split_into_words=False, \n",
    "                                input_return_type=input_return_type, dl_type=dl_type, \n",
    "                                tok_kwargs=tok_kwargs, \n",
    "                                before_batch_kwargs=before_batch_kwargs, \n",
    "                                after_batch_kwargs=after_batch_kwargs, \n",
    "                                **kwargs)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a `DataLoaders.show_batch` for seq2seq tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `HF_Seq2SeqInput` typed inputs\n",
    "    x:HF_Seq2SeqInput, \n",
    "    # Your targets\n",
    "    y,              \n",
    "    # Your raw inputs/targets\n",
    "    samples,        \n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for \n",
    "    # decoding them into something understandable\n",
    "    dataloaders,    \n",
    "    # Your `show_batch` context\n",
    "    ctxs=None, \n",
    "    # The maximum number of items to show\n",
    "    max_n=6, \n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    input_trunc_at=None, \n",
    "    # Any truncation your want applied to your decoded targets\n",
    "    target_trunc_at=None, \n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs\n",
    "):  \n",
    "    # grab our tokenizer and ignore token to decode\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    \n",
    "    res = L([ (hf_tokenizer.decode(s[0], skip_special_tokens=False)[:input_trunc_at], \n",
    "               hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:target_trunc_at])\n",
    "             for s in samples ])      \n",
    "    \n",
    "    display_df(pd.DataFrame(res, columns=['text', 'target'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module includes the fundamental bits to all Seq2Seq transformers data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
