{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.token_classification\n",
    "\n",
    "> This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import TransformBlock, Category, CategoryMap\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import AutoModelForTokenClassification, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import HF_BaseInput, HF_BeforeBatchTransform, first_blurr_tfm\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from fastai.data.block import DataBlock, ColReader, ColSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import *\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.data.core import HF_TextBlock\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get a list of the distinct entities we want to predict. If they are represented as list in their raw/readable form in another attribute/column in our dataset, we could use something like this to build a sorted list of distinct values as such: `labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))`.\n",
    "\n",
    "Fortunately, the `conll2003` dataset allows us to get at this list directly using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[('EU', 'B-ORG'), ('rejects', 'O'), ('German', 'B-MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('British', 'B-MISC'), ('lamb', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])\n",
    "print([(word, labels[label_idx]) for word, label_idx in zip(raw_datasets[\"train\"][0][\"tokens\"], raw_datasets[\"train\"][0][\"ner_tags\"])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"bert-base-multilingual-cased\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "    pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility methods\n",
    "\n",
    "These methods allow us to align labels to tokens and words without having to resort to fast tokenizer-only methods like `word_ids`.  Thus, blurr supports all token classification friendly Transformers models regardless of the type of tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def align_labels_with_tokens(\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # List of input_ids for the tokens in a single piece of processed text\n",
    "    input_ids: List[int], \n",
    "    # List of label indexs for each token\n",
    "    token_label_ids, \n",
    "    # List of label names from witch the `label` indicies can be used to find the name of the label\n",
    "    vocab\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
    "    each tuple defines the \"token\" and its label name. For example: \n",
    "    [('ĠWay', B-PER), ('de', B-PER), ('ĠGill', I-PER), ('iam', I-PER), ('Ġloves'), ('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG)]\n",
    "    \"\"\"\n",
    "    # convert ids to tokens\n",
    "    toks = hf_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # align \"tokens\" with labels\n",
    "    tok_labels = [(tok, vocab[label_id]) for tok_id, tok, label_id in zip(input_ids, toks, token_label_ids) if tok_id not in hf_tokenizer.all_special_ids]\n",
    "    return tok_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for align_labels_with_tokens()\n",
    "for idx in range(3):\n",
    "    raw_word_list = conll2003_df.iloc[idx]['tokens']\n",
    "    raw_label_list = conll2003_df.iloc[idx]['ner_tags']\n",
    "\n",
    "    be = hf_tokenizer(raw_word_list, is_split_into_words=True)\n",
    "    input_ids = be['input_ids']\n",
    "    targ_ids = [-100 if (word_id == None) else raw_label_list[word_id] for word_id in be.word_ids()]\n",
    "\n",
    "    tok_labels = align_labels_with_tokens(hf_tokenizer, input_ids, targ_ids, labels)\n",
    "\n",
    "    for tok_label, targ_id in zip (tok_labels, [label_id for label_id in targ_ids if label_id != -100]):\n",
    "        test_eq(tok_label[1], labels[targ_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"align_labels_with_tokens\" class=\"doc_header\"><code>align_labels_with_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>align_labels_with_tokens</code>(**`hf_tokenizer`**:`PreTrainedTokenizerBase`, **`input_ids`**:`List`\\[`int`\\], **`token_label_ids`**, **`vocab`**)\n",
       "\n",
       "Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
       "each tuple defines the \"token\" and its label name. For example: \n",
       "[('ĠWay', B-PER), ('de', B-PER), ('ĠGill', I-PER), ('iam', I-PER), ('Ġloves'), ('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG)]\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`hf_tokenizer`** : *`<class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'>`*\t<p>A Hugging Face tokenizer</p>\n",
       "\n",
       "\n",
       " - **`input_ids`** : *`typing.List[int]`*\t<p>List of input_ids for the tokens in a single piece of processed text</p>\n",
       "\n",
       "\n",
       " - **`token_label_ids`** : *`<class 'inspect._empty'>`*\t<p>List of label indexs for each token</p>\n",
       "\n",
       "\n",
       " - **`vocab`** : *`<class 'inspect._empty'>`*\t<p>List of label names from witch the `label` indicies can be used to find the name of the label</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[typing.Tuple[str, str]]`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(align_labels_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def align_labels_with_words(\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A list of tuples, where each represents a token and its label (e.g., [('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG), ...])\n",
    "    tok_labels\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
    "    \"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
    "    allows the user to reconstruct the orginal raw inputs and labels.\n",
    "    \"\"\"\n",
    "    # recreate raw words list (we assume for token classification that the input is a list of words)\n",
    "    words = hf_tokenizer.convert_tokens_to_string([tok_label[0] for tok_label in tok_labels]).split()\n",
    "    word_list = [word for word in words]\n",
    "    # align \"words\" with labels\n",
    "    word_labels, idx = [], 0\n",
    "    for word in word_list:\n",
    "        word_labels.append((word, tok_labels[idx][1]))\n",
    "        idx += len(hf_tokenizer.tokenize(word))\n",
    "\n",
    "    return word_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for align_labels_with_words()\n",
    "for idx in range(5):\n",
    "    raw_word_list = conll2003_df.iloc[idx]['tokens']\n",
    "    raw_label_list = conll2003_df.iloc[idx]['ner_tags']\n",
    "\n",
    "    be = hf_tokenizer(raw_word_list, is_split_into_words=True)\n",
    "    input_ids = be['input_ids']\n",
    "    targ_ids = [-100 if (word_id == None) else raw_label_list[word_id] for word_id in be.word_ids()]\n",
    "\n",
    "    tok_labels = align_labels_with_tokens(hf_tokenizer, input_ids, targ_ids, labels)\n",
    "    word_labels = align_labels_with_words(hf_tokenizer, tok_labels)\n",
    "\n",
    "    for word_label, raw_word, raw_label_id in zip (word_labels, raw_word_list, raw_label_list):\n",
    "        test_eq(word_label[0], raw_word)\n",
    "        test_eq(word_label[1], labels[raw_label_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"align_labels_with_words\" class=\"doc_header\"><code>align_labels_with_words</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>align_labels_with_words</code>(**`hf_tokenizer`**:`PreTrainedTokenizerBase`, **`tok_labels`**)\n",
       "\n",
       "Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
       "\"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the [`align_labels_with_tokens`](/blurr/data-token-classification.html#align_labels_with_tokens) method,\n",
       "allows the user to reconstruct the orginal raw inputs and labels.\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`hf_tokenizer`** : *`<class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'>`*\t<p>A Hugging Face tokenizer</p>\n",
       "\n",
       "\n",
       " - **`tok_labels`** : *`<class 'inspect._empty'>`*\t<p>A list of tuples, where each represents a token and its label (e.g., [('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG), ...])</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.List[typing.Tuple[str, str]]`*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(align_labels_with_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_slow_word_ids(hf_tokenizer, input_ids, special_tokens_mask):\n",
    "\n",
    "    toks = hf_tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
    "    word_list = [word for word in hf_tokenizer.convert_tokens_to_string([tok for tok in toks]).split() ]\n",
    "    n_tokens_per_word = [len(hf_tokenizer.tokenize(word)) for word in word_list]\n",
    "\n",
    "    word_ids, word_idx, tok_idx = [], 0, 0\n",
    "    while tok_idx < len(special_tokens_mask):\n",
    "        if (special_tokens_mask[tok_idx] == 1):\n",
    "            word_ids.append(None)\n",
    "            tok_idx += 1\n",
    "        else:\n",
    "            n_tokens = n_tokens_per_word[word_idx]\n",
    "            word_ids += [word_idx] * n_tokens\n",
    "            tok_idx += n_tokens\n",
    "            word_idx += 1\n",
    "\n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for get_slow_word_ids()\n",
    "_, _, test_tokenizer, _ = BLURR.get_hf_objects(\n",
    "    \"roberta-base\", model_cls=AutoModelForTokenClassification, config_kwargs={\"num_labels\": len(labels)}\n",
    ")\n",
    "\n",
    "for idx in range(5):\n",
    "    raw_word_list = conll2003_df.iloc[idx][\"tokens\"]\n",
    "    raw_label_list = conll2003_df.iloc[idx][\"ner_tags\"]\n",
    "\n",
    "    be = test_tokenizer(raw_word_list, is_split_into_words=True, return_special_tokens_mask=True)\n",
    "    input_ids, spec_tok_mask = be[\"input_ids\"], be[\"special_tokens_mask\"]\n",
    "\n",
    "    slow_word_ids = get_slow_word_ids(test_tokenizer, input_ids, spec_tok_mask)\n",
    "    if test_tokenizer.is_fast:\n",
    "        test_eq(be.word_ids(), slow_word_ids)\n",
    "    else:\n",
    "        print(input_ids)\n",
    "        print(slow_word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_slow_word_ids\" class=\"doc_header\"><code>get_slow_word_ids</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_slow_word_ids</code>(**`hf_tokenizer`**, **`input_ids`**, **`special_tokens_mask`**)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`hf_tokenizer`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`input_ids`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`special_tokens_mask`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_slow_word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TokenTensorCategory(TensorBase):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TokenCategorize(Transform):\n",
    "    \"\"\"Reversible transform of a list of category string to `vocab` id\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))\n",
    "        vocab=None,\n",
    "        # The token used to identifiy ignored tokens (default: xIGNx)\n",
    "        ignore_token=None,\n",
    "        # The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)\n",
    "        ignore_token_id=None,\n",
    "    ):\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab, sort=False)\n",
    "        self.ignore_token = \"[xIGNx]\" if ignore_token is None else ignore_token\n",
    "        self.ignore_token_id = CrossEntropyLossFlat().ignore_index if ignore_token_id is None else ignore_token_id\n",
    "\n",
    "        self.loss_func, self.order = CrossEntropyLossFlat(ignore_index=self.ignore_token_id), 1\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None:\n",
    "            self.vocab = CategoryMap(dsets)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, labels):\n",
    "        # if `val` is the label name (e.g., B-PER, I-PER, etc...), lookup the corresponding index in the vocab using\n",
    "        # `self.vocab.o2i`\n",
    "        ids = [val if (isinstance(val, int)) else self.vocab.o2i[val] for val in labels]\n",
    "        return HF_TokenTensorCategory(ids)\n",
    "\n",
    "    def decodes(self, encoded_labels):\n",
    "        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_TokenCategorize` modifies the fastai `Categorize` transform in a couple of ways.  \n",
    "\n",
    "First, it allows your targets to consist of a `Category` ***per*** token, and second, it uses the idea of an `ignore_token_id` to mask subtokens that don't need a prediction.  For example, the target of special tokens (e.g., pad, cls, sep) are set to `ignore_token_id` as are subsequent sub-tokens of a given token should more than 1 sub-token make it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def HF_TokenCategoryBlock(\n",
    "    # The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))\n",
    "    vocab=None,\n",
    "    # The token used to identifiy ignored tokens (default: xIGNx)\n",
    "    ignore_token=None,\n",
    "    # The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)\n",
    "    ignore_token_id=None,\n",
    "):\n",
    "    \"\"\"`TransformBlock` for per-token categorical targets\"\"\"\n",
    "    return TransformBlock(type_tfms=HF_TokenCategorize(vocab=vocab, ignore_token=ignore_token, ignore_token_id=ignore_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HF_TokenCategoryBlock\" class=\"doc_header\"><code>HF_TokenCategoryBlock</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HF_TokenCategoryBlock</code>(**`vocab`**=*`None`*, **`ignore_token`**=*`None`*, **`ignore_token_id`**=*`None`*)\n",
       "\n",
       "`TransformBlock` for per-token categorical targets\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`vocab`** : *`<class 'NoneType'>`*, *optional*\t<p>The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))</p>\n",
       "\n",
       "\n",
       " - **`ignore_token`** : *`<class 'NoneType'>`*, *optional*\t<p>The token used to identifiy ignored tokens (default: xIGNx)</p>\n",
       "\n",
       "\n",
       " - **`ignore_token_id`** : *`<class 'NoneType'>`*, *optional*\t<p>The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HF_TokenCategoryBlock)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a custom class, `HF_TokenClassInput`, for the @typedispatched methods to use so that we can override how token classification inputs/targets are assembled, as well as, how the data is shown via methods like `show_batch` and `show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TokenClassInput(HF_BaseInput):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TokenClassBeforeBatchTransform(HF_BeforeBatchTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "        # Labeling strategy (defaults to replacing each token with it's related entity's label)\n",
    "        label_strategy: str = \"replace_tokens_with_same_label\",\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = True,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs={},\n",
    "        # Keyword arguments to apply to `HF_TokenClassBeforeBatchTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        tok_kwargs = {**tok_kwargs, **{\"return_special_tokens_mask\": True}}\n",
    "\n",
    "        super().__init__(\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            is_split_into_words=is_split_into_words,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if label_strategy == \"replace_tokens_with_same_label\":\n",
    "            self.label_strategy_func = self._replace_tokens_with_same_label_strategy\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        samples, batch_encoding = super().encodes(samples, return_batch_encoding=True)\n",
    "\n",
    "        # if there are no targets (e.g., when used for inference), there is no need to do any post-processing on the labels\n",
    "        if len(samples[0]) == 1:\n",
    "            return samples\n",
    "\n",
    "        # get the type of our targets (by default will be HF_TokenTensorCategory)\n",
    "        target_cls = type(samples[0][1])\n",
    "\n",
    "        # we assume that first target = the categories we want to predict for each token\n",
    "        updated_samples = []\n",
    "        for idx, s in enumerate(samples):\n",
    "            word_ids = (\n",
    "                batch_encoding.word_ids(idx)\n",
    "                if self.hf_tokenizer.is_fast\n",
    "                else get_slow_word_ids(hf_tokenizer, s[0][\"input_ids\"], s[0][\"special_tokens_mask\"])\n",
    "            )\n",
    "            targ_ids = self.label_strategy_func(s[1], word_ids, target_cls)\n",
    "            updated_samples.append((s[0], targ_ids))\n",
    "\n",
    "        return updated_samples\n",
    "\n",
    "    def _replace_tokens_with_same_label_strategy(self, tok_label_ids, word_ids, trg_class=HF_TokenTensorCategory):\n",
    "        try:\n",
    "            targ_ids = trg_class([self.ignore_token_id if (word_id == None) else tok_label_ids[word_id] for word_id in word_ids])\n",
    "            return targ_ids\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_TokenClassBeforeBatchTransform` is used to exclude any of the target's tokens we don't want to include in the loss calcuation (e.g. padding, cls, sep, etc...). Notice also that we default `is_split_into_words = True` since for most token classification problems, each example comes in the form as a list of words and a list of entity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), HF_TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# dblock.summary(conll2003_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 32]), torch.Size([4, 32]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0][\"input_ids\"].shape, b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `HF_TokenClassInput` typed inputs\n",
    "    x: HF_TokenClassInput,\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    vocab = dataloaders.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample in zip(x, y, samples):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = align_labels_with_tokens(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = align_labels_with_words(hf_tokenizer, tok_labels)\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append([f\"{[ word_targ for idx, word_targ in enumerate(word_labels) if (trunc_at is None or idx < trunc_at) ]}\"])\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"word / target label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Witnesses', 'O'), ('had', 'O'), ('reported', 'O'), ('seeing', 'O'), ('many', 'O'), ('young', 'O'), ('women', 'O'), ('and', 'O'), ('some', 'O'), ('girls', 'O'), ('who', 'O'), ('looked', 'O'), ('clearly', 'O'), ('underage', 'O'), ('at', 'O'), ('parties', 'O'), ('around', 'O'), ('the', 'O'), ('boat', 'O'), ('in', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Amtrak', 'B-ORG'), ('train', 'O'), ('derails', 'O'), (',', 'O'), ('three', 'O'), ('injured', 'O'), ('-', 'O'), ('officials', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[('It', 'O'), ('will', 'O'), ('have', 'O'), ('no', 'O'), ('duty', 'O'), ('to', 'O'), ('contribute', 'O'), ('any', 'O'), ('taxes', 'O'), ('to', 'O'), ('Beijing', 'B-LOC'), (',', 'O'), ('Tsang', 'B-PER'), ('said', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[('Quarracino', 'B-PER'), ('and', 'O'), ('other', 'O'), ('Church', 'O'), ('leaders', 'O'), ('are', 'O'), ('regular', 'O'), ('critics', 'O'), ('of', 'O'), ('the', 'O'), ('government', 'O'), (\"'s\", 'O'), ('free-market', 'O'), ('economic', 'O'), ('policy', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=5, trunc_at=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    # \"hf-internal-testing/tiny-albert\",\n",
    "    # \"hf-internal-testing/tiny-bert\",\n",
    "    # \"google/bigbird-roberta-base\",\n",
    "    # \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    # \"YituTech/conv-bert-base\",\n",
    "    # \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    # \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    # \"hf-internal-testing/tiny-electra\",\n",
    "    # # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids \n",
    "    # \"huggingface/funnel-small-base\",\n",
    "    # \"sshleifer/tiny-gpt2\",\n",
    "    # \"hf-internal-testing/tiny-layoutlm\",\n",
    "    # \"allenai/longformer-base-4096\",\n",
    "    # \"microsoft/mpnet-base\",\n",
    "    # \"kssteven/ibert-roberta-base\",\n",
    "    # # \"nvidia/megatron-bert-cased-345m\",                # could not test           \n",
    "    # \"google/mobilebert-uncased\",\n",
    "    # 'google/rembert',\n",
    "    # \"junnyu/roformer_chinese_sim_char_ft_small\",                 \n",
    "    # \"roberta-base\",\n",
    "    # \"squeezebert/squeezebert-uncased\",\n",
    "    \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    # \"xlm-roberta-base\",\n",
    "    # \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n",
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if 'deberta' in model_name else {}\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, tokenizer_kwargs=tok_kwargs)\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    \n",
    "    before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz)\n",
    "    blocks = (HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), HF_TokenCategoryBlock(vocab=labels))\n",
    "    dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "    dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "    b = dls.one_batch()\n",
    "\n",
    "    print(\"*** TESTING DataLoaders ***\\n\")\n",
    "    test_eq(len(b), 2)\n",
    "    test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "    test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "    test_eq(len(b[1]), bsz)\n",
    "\n",
    "    if hasattr(hf_tokenizer, \"add_prefix_space\"):\n",
    "        test_eq(hf_tokenizer.add_prefix_space, True)\n",
    "    try:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"PASSED\", \"\"))\n",
    "        dls.show_batch(dataloaders=dls, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"FAILED\", err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>xlm-mlm-en-2048</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module includes all the low, mid, and high-level API bits for token classification tasks data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'], dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>[11, 12, 12, 21, 11, 12, 12, 11, 12, 13, 11, 12, 13, 11, 12, 3, 11, 12, 12, 12, 0, 11, 12, 12, 0, 21, 11, 12, 12, 13, 11, 0]</td>\n",
       "      <td>356</td>\n",
       "      <td>[0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 22, 38, 29, 16, 21, 11, 21, 15, 11, 24, 15, 12, 21, 46, 29, 16, 21, 22, 6, 12, 16, 21, 6, 38, 12, 22, 23, 15, 22, 7]</td>\n",
       "      <td>[Sir, Mark, Prescott, landed, his, first, group, one, victory, in, 25, years, as, a, trainer, when, his, top, sprinter, Pivotal, ,, a, 100-30, chance, ,, won, the, Nunthorpe, Stakes, on, Thursday, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>[11, 13, 11, 12, 0, 11, 0, 11, 12, 13, 11, 12, 12, 21, 13, 11, 12, 12, 21, 22, 22, 22, 11, 0, 0, 0, 11, 13, 11, 13, 11, 12, 0, 11, 12, 12, 12, 13, 11, 0]</td>\n",
       "      <td>1013</td>\n",
       "      <td>[0, 0, 7, 8, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 35, 22, 22, 4, 22, 5, 12, 11, 15, 11, 22, 11, 39, 35, 12, 39, 21, 35, 37, 35, 37, 24, 10, 34, 10, 24, 15, 24, 15, 16, 21, 4, 22, 12, 22, 11, 15, 11, 5]</td>\n",
       "      <td>[Corrigendum, to, Commission, Regulation, (, EC, ), No, 1464/96, of, 25, July, 1996, relating, to, a, standing, invitation, to, tender, to, determine, levies, and, /, or, refunds, on, exports, of, white, sugar, (, OJ, No, L, 187, of, 26.7.1996, )]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>[11, 21, 13, 11, 11, 21, 22, 11, 12, 12, 0, 11, 12, 12, 0, 13, 11, 12, 13, 11, 12, 12, 13, 11, 0, 3, 11, 13, 11, 12, 12, 13, 11, 12, 21, 22, 22, 22, 11, 12, 0]</td>\n",
       "      <td>1100</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 38, 15, 22, 28, 20, 37, 11, 11, 24, 4, 3, 11, 11, 5, 15, 21, 21, 35, 12, 22, 21, 15, 22, 6, 46, 16, 15, 12, 21, 21, 15, 12, 21, 42, 40, 35, 37, 12, 21, 7]</td>\n",
       "      <td>[Britain, said, on, Thursday, it, would, give, 25, million, pounds, (, $, 39, million, ), of, development, aid, to, the, Caribbean, island, of, Montserrat, ,, where, much, of, the, population, living, in, the, south, has, fled, to, avoid, a, volcano, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>[11, 11, 12, 12, 21, 11, 3, 4, 11, 12, 0, 11, 12, 12, 13, 11, 13, 11, 13, 11, 0]</td>\n",
       "      <td>1309</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 24, 41, 21, 15, 18, 11, 24, 10, 3, 11, 11, 15, 21, 35, 21, 15, 11, 7]</td>\n",
       "      <td>[Peru, 's, guerrilla, conflicts, have, cost, at, least, 30,000, lives, and, $, 25, billion, in, damage, to, infrastructure, since, 1980, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>[0, 17, 11, 21, 22, 11, 12, 12, 12, 0, 3, 11, 12, 12, 21, 22, 22, 11, 12, 12, 12, 0, 0, 21, 11, 12, 12, 12, 11, 21, 22, 22, 22, 0]</td>\n",
       "      <td>1680</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 15, 28, 41, 39, 15, 18, 11, 21, 6, 30, 29, 16, 24, 41, 30, 39, 11, 10, 11, 21, 6, 0, 38, 11, 22, 24, 21, 44, 38, 35, 37, 40, 7]</td>\n",
       "      <td>[\", If, they, 're, saying, at, least, 20, percent, ,, then, their, internal, forecasts, are, probably, saying, 25, or, 30, percent, ,, \", said, one, Sydney, media, analyst, who, declined, to, be, named, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>[11, 12, 12, 13, 11, 21, 22, 13, 11, 11, 12, 12, 3, 13, 11, 12, 11, 12, 0]</td>\n",
       "      <td>2261</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 16, 21, 15, 11, 38, 40, 15, 22, 27, 22, 22, 30, 15, 22, 11, 16, 21, 7]</td>\n",
       "      <td>[The, previous, mark, of, 2:29.34, was, set, by, Mozambique, 's, Maria, Mutola, here, on, August, 25, last, year, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>[11, 21, 11, 12, 0, 11, 12, 13, 11, 13, 11, 12, 13, 11, 12, 12, 0, 11, 12, 13, 11, 12, 13, 11, 12, 13, 11, 12, 13, 11, 12, 0, 11, 0]</td>\n",
       "      <td>2568</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0]</td>\n",
       "      <td>[28, 38, 16, 24, 6, 22, 11, 15, 21, 15, 16, 24, 35, 12, 16, 21, 6, 22, 11, 15, 12, 21, 15, 12, 21, 15, 12, 22, 15, 22, 22, 10, 22, 7]</td>\n",
       "      <td>[He, proposed, definite, dates, ,, August, 25, for, return, of, Unita, generals, to, the, joint, army, ,, September, 5, for, the, beginning, of, the, formation, of, the, Government, of, National, Unity, and, Reconciliation, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>[11, 12, 13, 11, 13, 11, 12, 13, 11, 12, 21, 22, 13, 11, 12, 0, 11, 12, 0, 11, 12, 12, 12, 21, 13, 11, 0]</td>\n",
       "      <td>2784</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 24, 15, 24, 15, 12, 22, 15, 22, 22, 41, 40, 15, 22, 11, 10, 22, 11, 6, 12, 22, 22, 22, 38, 15, 22, 7]</td>\n",
       "      <td>[No, closures, of, airports, in, the, Commonwealth, of, Independent, States, are, expected, on, August, 24, and, August, 25, ,, the, Russian, Weather, Service, said, on, Friday, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 21, 22, 11, 0, 11, 12, 13, 11, 12, 13, 11, 0, 11, 12, 21, 13, 11, 0]</td>\n",
       "      <td>2810</td>\n",
       "      <td>[5, 6, 0, 0, 1, 2, 0, 0, 5, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 22, 22, 22, 22, 20, 37, 22, 10, 22, 22, 15, 22, 11, 35, 11, 6, 12, 21, 38, 15, 22, 7]</td>\n",
       "      <td>[Hong, Kong, Financial, Secretary, Donald, Tsang, will, visit, Indonesia, and, New, Zealand, from, August, 25, to, 31, ,, the, government, said, on, Friday, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>[11, 0, 11, 0, 11, 12, 12, 12, 12, 0, 11, 12, 13, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 0, 21, 22, 13, 11, 12, 12, 13, 11, 12, 12, 12, 0]</td>\n",
       "      <td>3312</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 3, 0, 3, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[21, 8, 22, 10, 21, 24, 21, 22, 22, 6, 12, 21, 15, 21, 24, 21, 22, 22, 22, 4, 22, 5, 22, 6, 38, 40, 15, 12, 22, 22, 15, 22, 11, 6, 11, 7]</td>\n",
       "      <td>[NOTE, -, Marble, and, granite, products, distributor, Companion, Marble, ,, a, spinoff, of, construction, materials, concern, Companion, Building, Material, (, Holdings, ), Ltd, ,, was, listed, on, the, Stock, Exchange, on, April, 25, ,, 1996, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>[13, 11, 12, 11, 0, 11, 0, 0, 0, 11, 21, 11, 12, 1, 13, 11, 12, 12, 0, 21, 3, 13, 11, 12, 0, 11, 12, 0, 0]</td>\n",
       "      <td>4353</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[15, 11, 21, 22, 4, 11, 22, 5, 6, 22, 38, 11, 24, 16, 15, 12, 22, 23, 10, 39, 30, 15, 11, 21, 4, 11, 21, 5, 7]</td>\n",
       "      <td>[At, 11, a.m., EDT, (, 1500, GMT, ), ,, Edouard, was, 1,130, miles, east, of, the, Lesser, Antilles, and, moving, west, at, 14, mph, (, 25, kph, ), .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>[11, 12, 12, 21, 22, 13, 11, 3, 11, 21, 11, 12, 11, 12, 12, 12, 0, 11, 12, 11, 12, 12, 0, 17, 11, 12, 13, 11, 13, 11, 12, 0]</td>\n",
       "      <td>4481</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 16, 21, 38, 40, 35, 11, 46, 22, 38, 12, 21, 27, 16, 16, 21, 6, 12, 21, 11, 16, 21, 6, 15, 16, 21, 15, 21, 15, 11, 24, 7]</td>\n",
       "      <td>[Mickelson, three-stroke, lead, was, cut, to, two, when, Mayfair, birdied, the, course, 's, only, easy, hole, ,, the, par, five, second, hole, ,, while, Mickelson, three-putted, for, par, from, 25, feet, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>[11, 12, 12]</td>\n",
       "      <td>4581</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>[11, 23, 11]</td>\n",
       "      <td>[4., Theybers, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>[11, 0, 11, 0, 21, 11, 13, 14, 11, 12, 21, 22, 11, 12, 12, 12, 12, 0, 11, 12, 11, 12, 12, 0, 13, 11, 13, 11, 12, 12, 21, 0]</td>\n",
       "      <td>4870</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 3, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 6, 11, 6, 38, 21, 30, 15, 29, 24, 35, 37, 22, 22, 21, 22, 22, 6, 16, 21, 27, 21, 21, 6, 15, 11, 15, 12, 11, 24, 38, 7]</td>\n",
       "      <td>[Takeda, ,, 18, ,, showed, poise, far, beyond, his, years, to, overtake, Australian, Ducati, rider, Troy, Corser, ,, last, year, 's, championship, runner-up, ,, with, four, of, the, 25, laps, left, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>4891</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[London, 22, 12, 1, 9, 611, 462, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>[11, 0, 11, 12, 12, 0, 11]</td>\n",
       "      <td>5000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[24, 4, 11, 11, 11, 5, 11]</td>\n",
       "      <td>[Extras, (, b-4, lb-5, nb-16, ), 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5197</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Alaniya, Vladikavkaz, 24, 16, 5, 3, 48, 25, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5198</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Dynamo, Moscow, 25, 15, 7, 3, 43, 21, 52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5200</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Spartak, Moscow, 25, 14, 7, 4, 48, 24, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 11]</td>\n",
       "      <td>5201</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[CSKA, Moscow, 25, 13, 6, 6, 40, 27, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5202</td>\n",
       "      <td>[3, 4, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Lokomotiv, Nizhny, Novgorod, 25, 11, 4, 10, 27, 35, 37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 11, 12, 12]</td>\n",
       "      <td>5203</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Lokomotiv, Moscow, 25, 9, 9, 7, 30, 24, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 11, 12]</td>\n",
       "      <td>5204</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Baltika, Kaliningrad, 25, 8, 10, 7, 29, 26, 34]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5205</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Torpedo, Moscow, 25, 8, 9, 8, 31, 33, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5207</td>\n",
       "      <td>[3, 4, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Krylya, Sovetov, Samara, 25, 8, 7, 10, 19, 29, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 11, 12, 12]</td>\n",
       "      <td>5208</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Zhemchuzhina, Sochi, 25, 8, 4, 13, 26, 38, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 11]</td>\n",
       "      <td>5210</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[24, 21, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Chernomorets, Novorossiisk, 25, 7, 5, 13, 25, 38, 26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5211</td>\n",
       "      <td>[3, 4, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Kamaz, Naberezhnye, Chelny, 24, 5, 4, 15, 25, 42, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 11, 12]</td>\n",
       "      <td>5213</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Tekstilshchik, Kamyshin, 25, 3, 9, 13, 15, 30, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5274</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Canberra, 21, 12, 1, 8, 502, 374, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5275</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[St, George, 21, 12, 1, 8, 421, 344, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5517</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Molde, 19, 8, 3, 8, 36, 25, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>[11, 0, 11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5518</td>\n",
       "      <td>[3, 4, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 34, 21, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Bodo, /, Glimt, 20, 7, 4, 9, 33, 41, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>[11, 12, 12, 12, 12, 11, 12, 12]</td>\n",
       "      <td>5519</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Kongsvinger, 20, 7, 4, 9, 26, 38, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5520</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[21, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Stromsgodset, 20, 7, 4, 9, 27, 40, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5800</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Canberra, 21, 12, 1, 8, 502, 374, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>5801</td>\n",
       "      <td>[3, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[St, George, 21, 12, 1, 8, 421, 344, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>[11, 0, 11, 12, 12, 12, 12, 12, 0, 11, 12, 12, 21, 11, 12, 12, 11, 21, 22, 11, 13, 11, 12, 12, 13, 11, 11, 12, 12, 12, 13, 11, 12, 0, 11, 21, 0]</td>\n",
       "      <td>5817</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 3, 0, 0]</td>\n",
       "      <td>[22, 6, 22, 11, 8, 22, 22, 22, 10, 29, 12, 23, 40, 12, 21, 21, 21, 35, 41, 12, 15, 12, 21, 24, 15, 21, 27, 16, 16, 21, 15, 22, 22, 6, 22, 38, 7]</td>\n",
       "      <td>[PRETORIA, ,, Aug, 25, -, Captain, Sean, Fitzpatrick, and, his, All, Blacks, revisited, the, test, venue, today, to, relive, some, of, the, magic, moments, of, yesterday, 's, momentous, rugby, victory, over, South, Africa, ,, NZPA, reported, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>[11, 12, 0, 11, 13, 11, 12, 12, 0, 21, 11, 12, 11, 12, 12, 21, 11, 12, 0, 11, 13, 11, 21, 11, 12, 0, 13, 11, 12, 21, 13, 11, 12, 17, 21, 13, 11, 0, 11, 12, 12, 12, 0, 11, 12, 0, 3, 13, 11, 0]</td>\n",
       "      <td>6008</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]</td>\n",
       "      <td>[22, 22, 6, 21, 15, 12, 22, 22, 6, 38, 22, 21, 12, 22, 21, 38, 12, 24, 6, 12, 15, 43, 41, 16, 24, 6, 15, 12, 21, 40, 15, 12, 24, 15, 39, 15, 22, 6, 12, 21, 11, 21, 4, 11, 24, 5, 30, 15, 22, 7]</td>\n",
       "      <td>[Mohammed, Saleh, ,, director, of, the, Egyptian, Museum, ,, told, Reuters, television, a, U.S., team, found, the, pots, ,, some, of, which, contain, human, intestines, ,, in, a, tomb, built, into, the, rocks, while, digging, in, Dahshour, ,, a, village, 40, km, (, 25, miles, ), south, of, Cairo, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6019</th>\n",
       "      <td>[11, 12, 0, 11, 0, 11, 12, 0]</td>\n",
       "      <td>6019</td>\n",
       "      <td>[0, 0, 0, 5, 0, 0, 0, 0]</td>\n",
       "      <td>[24, 22, 8, 22, 8, 22, 11, 7]</td>\n",
       "      <td>[PRESS, DIGEST, -, Jordan, -, Aug, 25, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>[11, 21, 0, 11, 0, 11, 12, 0]</td>\n",
       "      <td>6048</td>\n",
       "      <td>[0, 0, 0, 5, 0, 0, 0, 0]</td>\n",
       "      <td>[24, 37, 8, 22, 8, 22, 11, 7]</td>\n",
       "      <td>[PRESS, DIGEST, -, Israel, -, Aug, 25, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6214</th>\n",
       "      <td>[11, 12, 0, 11, 21, 0, 21, 13, 11, 12, 13, 11, 13, 11, 12, 12, 13, 11, 12, 13, 11, 11, 12, 12, 11, 12, 0, 13, 11, 12, 13, 11, 0, 11, 12, 13, 11, 12, 13, 11, 0]</td>\n",
       "      <td>6214</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[16, 24, 6, 28, 38, 6, 40, 15, 16, 21, 15, 24, 15, 21, 10, 24, 15, 11, 21, 15, 22, 27, 16, 21, 15, 21, 6, 15, 11, 21, 15, 11, 10, 11, 21, 15, 12, 21, 15, 11, 7]</td>\n",
       "      <td>[Further, plans, ,, he, said, ,, called, for, full, provision, of, resources, like, fertiliser, and, herbicides, for, 25, percent, of, Ukraine, 's, arable, land, next, year, ,, for, 50, percent, in, 1998, and, 100, percent, of, the, land, in, 1999, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>6517</td>\n",
       "      <td>[3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[London, 22, 12, 1, 9, 611, 462, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7265</th>\n",
       "      <td>[11, 0, 11, 12, 12, 12, 0, 0, 11, 12, 3, 21, 22, 11, 21, 11, 21, 22, 11, 12, 3, 11, 12, 11, 12, 12, 21, 11, 12, 12, 0]</td>\n",
       "      <td>7265</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[24, 6, 16, 16, 16, 24, 6, 10, 39, 24, 30, 20, 37, 24, 39, 28, 35, 30, 11, 24, 46, 12, 21, 27, 21, 21, 42, 3, 11, 11, 7]</td>\n",
       "      <td>[Bondholders, ,, other, unsecured, non-trade, creditors, ,, and, existing, shareholders, also, will, receive, warrants, entitling, them, to, roughly, 800,000, shares, when, the, company, 's, market, capitalization, reaches, $, 25, million, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7408</th>\n",
       "      <td>[11, 12, 12, 0, 11, 0, 0, 11, 12, 0, 11, 0, 21, 22, 13, 11, 12, 13, 11, 12, 12, 12, 21, 22, 0]</td>\n",
       "      <td>7408</td>\n",
       "      <td>[1, 2, 2, 0, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 22, 6, 11, 6, 10, 22, 23, 6, 11, 6, 38, 40, 15, 11, 24, 15, 12, 3, 11, 21, 38, 40, 7]</td>\n",
       "      <td>[Regula, Susana, Siegfried, ,, 50, ,, and, Nicola, Fleuchaus, ,, 25, ,, were, released, after, 71, days, after, a, $, 200,000, ransom, was, paid, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>[11, 12, 12, 12, 13, 11, 21, 11, 0, 11, 21, 13, 21, 11, 13, 11, 0, 21, 13, 11, 0, 21, 11, 13, 11, 0, 11, 12, 13, 11, 0, 3, 11, 12, 12, 21, 22, 13, 11, 12, 12, 12, 13, 11, 12, 0, 0, 11, 0]</td>\n",
       "      <td>7534</td>\n",
       "      <td>[0, 5, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]</td>\n",
       "      <td>[12, 22, 21, 21, 15, 22, 38, 22, 6, 44, 38, 15, 39, 22, 15, 22, 10, 38, 15, 22, 6, 38, 24, 15, 22, 10, 22, 24, 15, 22, 6, 46, 11, 22, 24, 38, 40, 15, 12, 21, 21, 21, 15, 22, 11, 6, 10, 22, 7]</td>\n",
       "      <td>[A, U.S., embassy, spokesman, in, Riyadh, said, Specter, ,, who, arrived, from, neighbouring, Oman, on, Sunday, and, left, on, Monday, ,, had, talks, with, Saudi, and, American, officials, in, Dhahran, ,, where, 19, U.S., airmen, were, killed, by, a, fuel, truck, bomb, on, June, 25, ,, and, Riyadh, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8314</th>\n",
       "      <td>[11, 12, 12, 12, 12, 11, 12, 12]</td>\n",
       "      <td>8314</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Mark, Ealham, 1, 2, 0, 30, 25, 15.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8353</th>\n",
       "      <td>[11, 12, 12, 12, 12, 12, 12]</td>\n",
       "      <td>8353</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 22, 11, 11, 11, 11, 11]</td>\n",
       "      <td>[Waqar, Younis, 125, 25, 431, 16, 26.93]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8879</th>\n",
       "      <td>[0, 13, 11, 12, 12, 12, 12, 11, 21, 22, 3, 13, 11, 0, 0, 21, 22, 11, 12, 0]</td>\n",
       "      <td>8879</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 15, 11, 6, 11, 10, 11, 28, 20, 37, 30, 15, 28, 6, 30, 39, 40, 16, 24, 7]</td>\n",
       "      <td>[\", At, 24, ,, 25, or, 26, you, could, get, away, with, it, ,, not, having, played, first-team, games, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>[21, 11, 12, 12, 12]</td>\n",
       "      <td>9533</td>\n",
       "      <td>[5, 0, 0, 0, 0]</td>\n",
       "      <td>[37, 11, 22, 11, 11]</td>\n",
       "      <td>[Indore, 25, Yellow, 12,750-12,950, 12,900-13,100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           chunk_tags  \\\n",
       "356                                                                      [11, 12, 12, 21, 11, 12, 12, 11, 12, 13, 11, 12, 13, 11, 12, 3, 11, 12, 12, 12, 0, 11, 12, 12, 0, 21, 11, 12, 12, 13, 11, 0]   \n",
       "1013                                        [11, 13, 11, 12, 0, 11, 0, 11, 12, 13, 11, 12, 12, 21, 13, 11, 12, 12, 21, 22, 22, 22, 11, 0, 0, 0, 11, 13, 11, 13, 11, 12, 0, 11, 12, 12, 12, 13, 11, 0]   \n",
       "1100                                  [11, 21, 13, 11, 11, 21, 22, 11, 12, 12, 0, 11, 12, 12, 0, 13, 11, 12, 13, 11, 12, 12, 13, 11, 0, 3, 11, 13, 11, 12, 12, 13, 11, 12, 21, 22, 22, 22, 11, 12, 0]   \n",
       "1309                                                                                                                 [11, 11, 12, 12, 21, 11, 3, 4, 11, 12, 0, 11, 12, 12, 13, 11, 13, 11, 13, 11, 0]   \n",
       "1680                                                               [0, 17, 11, 21, 22, 11, 12, 12, 12, 0, 3, 11, 12, 12, 21, 22, 22, 11, 12, 12, 12, 0, 0, 21, 11, 12, 12, 12, 11, 21, 22, 22, 22, 0]   \n",
       "2261                                                                                                                       [11, 12, 12, 13, 11, 21, 22, 13, 11, 11, 12, 12, 3, 13, 11, 12, 11, 12, 0]   \n",
       "2568                                                             [11, 21, 11, 12, 0, 11, 12, 13, 11, 13, 11, 12, 13, 11, 12, 12, 0, 11, 12, 13, 11, 12, 13, 11, 12, 13, 11, 12, 13, 11, 12, 0, 11, 0]   \n",
       "2784                                                                                        [11, 12, 13, 11, 13, 11, 12, 13, 11, 12, 21, 22, 13, 11, 12, 0, 11, 12, 0, 11, 12, 12, 12, 21, 13, 11, 0]   \n",
       "2810                                                                                                    [11, 12, 12, 12, 12, 12, 21, 22, 11, 0, 11, 12, 13, 11, 12, 13, 11, 0, 11, 12, 21, 13, 11, 0]   \n",
       "3312                                                      [11, 0, 11, 0, 11, 12, 12, 12, 12, 0, 11, 12, 13, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 0, 21, 22, 13, 11, 12, 12, 13, 11, 12, 12, 12, 0]   \n",
       "4353                                                                                       [13, 11, 12, 11, 0, 11, 0, 0, 0, 11, 21, 11, 12, 1, 13, 11, 12, 12, 0, 21, 3, 13, 11, 12, 0, 11, 12, 0, 0]   \n",
       "4481                                                                     [11, 12, 12, 21, 22, 13, 11, 3, 11, 21, 11, 12, 11, 12, 12, 12, 0, 11, 12, 11, 12, 12, 0, 17, 11, 12, 13, 11, 13, 11, 12, 0]   \n",
       "4581                                                                                                                                                                                     [11, 12, 12]   \n",
       "4870                                                                      [11, 0, 11, 0, 21, 11, 13, 14, 11, 12, 21, 22, 11, 12, 12, 12, 12, 0, 11, 12, 11, 12, 12, 0, 13, 11, 13, 11, 12, 12, 21, 0]   \n",
       "4891                                                                                                                                                                 [11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5000                                                                                                                                                                       [11, 0, 11, 12, 12, 0, 11]   \n",
       "5197                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5198                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5200                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5201                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 11]   \n",
       "5202                                                                                                                                                         [11, 12, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5203                                                                                                                                                             [11, 12, 12, 12, 12, 12, 11, 12, 12]   \n",
       "5204                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 11, 12]   \n",
       "5205                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5207                                                                                                                                                         [11, 12, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5208                                                                                                                                                             [11, 12, 12, 12, 12, 12, 11, 12, 12]   \n",
       "5210                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 11]   \n",
       "5211                                                                                                                                                         [11, 12, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5213                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 11, 12]   \n",
       "5274                                                                                                                                                                 [11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5275                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5517                                                                                                                                                                 [11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5518                                                                                                                                                          [11, 0, 11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5519                                                                                                                                                                 [11, 12, 12, 12, 12, 11, 12, 12]   \n",
       "5520                                                                                                                                                                 [11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5800                                                                                                                                                                 [11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5801                                                                                                                                                             [11, 12, 12, 12, 12, 12, 12, 12, 12]   \n",
       "5817                                                 [11, 0, 11, 12, 12, 12, 12, 12, 0, 11, 12, 12, 21, 11, 12, 12, 11, 21, 22, 11, 13, 11, 12, 12, 13, 11, 11, 12, 12, 12, 13, 11, 12, 0, 11, 21, 0]   \n",
       "6008  [11, 12, 0, 11, 13, 11, 12, 12, 0, 21, 11, 12, 11, 12, 12, 21, 11, 12, 0, 11, 13, 11, 21, 11, 12, 0, 13, 11, 12, 21, 13, 11, 12, 17, 21, 13, 11, 0, 11, 12, 12, 12, 0, 11, 12, 0, 3, 13, 11, 0]   \n",
       "6019                                                                                                                                                                    [11, 12, 0, 11, 0, 11, 12, 0]   \n",
       "6048                                                                                                                                                                    [11, 21, 0, 11, 0, 11, 12, 0]   \n",
       "6214                                  [11, 12, 0, 11, 21, 0, 21, 13, 11, 12, 13, 11, 13, 11, 12, 12, 13, 11, 12, 13, 11, 11, 12, 12, 11, 12, 0, 13, 11, 12, 13, 11, 0, 11, 12, 13, 11, 12, 13, 11, 0]   \n",
       "6517                                                                                                                                                                 [11, 12, 12, 12, 12, 12, 12, 12]   \n",
       "7265                                                                           [11, 0, 11, 12, 12, 12, 0, 0, 11, 12, 3, 21, 22, 11, 21, 11, 21, 22, 11, 12, 3, 11, 12, 11, 12, 12, 21, 11, 12, 12, 0]   \n",
       "7408                                                                                                   [11, 12, 12, 0, 11, 0, 0, 11, 12, 0, 11, 0, 21, 22, 13, 11, 12, 13, 11, 12, 12, 12, 21, 22, 0]   \n",
       "7534      [11, 12, 12, 12, 13, 11, 21, 11, 0, 11, 21, 13, 21, 11, 13, 11, 0, 21, 13, 11, 0, 21, 11, 13, 11, 0, 11, 12, 13, 11, 0, 3, 11, 12, 12, 21, 22, 13, 11, 12, 12, 12, 13, 11, 12, 0, 0, 11, 0]   \n",
       "8314                                                                                                                                                                 [11, 12, 12, 12, 12, 11, 12, 12]   \n",
       "8353                                                                                                                                                                     [11, 12, 12, 12, 12, 12, 12]   \n",
       "8879                                                                                                                      [0, 13, 11, 12, 12, 12, 12, 11, 21, 22, 3, 13, 11, 0, 0, 21, 22, 11, 12, 0]   \n",
       "9533                                                                                                                                                                             [21, 11, 12, 12, 12]   \n",
       "\n",
       "        id  \\\n",
       "356    356   \n",
       "1013  1013   \n",
       "1100  1100   \n",
       "1309  1309   \n",
       "1680  1680   \n",
       "2261  2261   \n",
       "2568  2568   \n",
       "2784  2784   \n",
       "2810  2810   \n",
       "3312  3312   \n",
       "4353  4353   \n",
       "4481  4481   \n",
       "4581  4581   \n",
       "4870  4870   \n",
       "4891  4891   \n",
       "5000  5000   \n",
       "5197  5197   \n",
       "5198  5198   \n",
       "5200  5200   \n",
       "5201  5201   \n",
       "5202  5202   \n",
       "5203  5203   \n",
       "5204  5204   \n",
       "5205  5205   \n",
       "5207  5207   \n",
       "5208  5208   \n",
       "5210  5210   \n",
       "5211  5211   \n",
       "5213  5213   \n",
       "5274  5274   \n",
       "5275  5275   \n",
       "5517  5517   \n",
       "5518  5518   \n",
       "5519  5519   \n",
       "5520  5520   \n",
       "5800  5800   \n",
       "5801  5801   \n",
       "5817  5817   \n",
       "6008  6008   \n",
       "6019  6019   \n",
       "6048  6048   \n",
       "6214  6214   \n",
       "6517  6517   \n",
       "7265  7265   \n",
       "7408  7408   \n",
       "7534  7534   \n",
       "8314  8314   \n",
       "8353  8353   \n",
       "8879  8879   \n",
       "9533  9533   \n",
       "\n",
       "                                                                                                                                                    ner_tags  \\\n",
       "356                                                         [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0]   \n",
       "1013                                [0, 0, 7, 8, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1100                             [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1309                                                                                         [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1680                                                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2261                                                                                               [0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2568                                                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0]   \n",
       "2784                                                                       [0, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0]   \n",
       "2810                                                                                [5, 6, 0, 0, 1, 2, 0, 0, 5, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3312                                            [0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 3, 0, 3, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0]   \n",
       "4353                                                                 [0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4481                                                        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4581                                                                                                                                               [0, 1, 0]   \n",
       "4870                                                        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 3, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4891                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5000                                                                                                                                   [0, 0, 0, 0, 0, 0, 0]   \n",
       "5197                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5198                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5200                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5201                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5202                                                                                                                          [3, 4, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5203                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5204                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5205                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5207                                                                                                                          [3, 4, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5208                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5210                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5211                                                                                                                          [3, 4, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5213                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5274                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5275                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5517                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5518                                                                                                                          [3, 4, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5519                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5520                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5800                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5801                                                                                                                             [3, 4, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5817                                         [5, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 3, 0, 0]   \n",
       "6008  [1, 2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]   \n",
       "6019                                                                                                                                [0, 0, 0, 5, 0, 0, 0, 0]   \n",
       "6048                                                                                                                                [0, 0, 0, 5, 0, 0, 0, 0]   \n",
       "6214                             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6517                                                                                                                                [3, 0, 0, 0, 0, 0, 0, 0]   \n",
       "7265                                                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "7408                                                                             [1, 2, 2, 0, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "7534     [0, 5, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]   \n",
       "8314                                                                                                                                [1, 2, 0, 0, 0, 0, 0, 0]   \n",
       "8353                                                                                                                                   [1, 2, 0, 0, 0, 0, 0]   \n",
       "8879                                                                                            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9533                                                                                                                                         [5, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                                                                                              pos_tags  \\\n",
       "356                                                                      [22, 22, 22, 38, 29, 16, 21, 11, 21, 15, 11, 24, 15, 12, 21, 46, 29, 16, 21, 22, 6, 12, 16, 21, 6, 38, 12, 22, 23, 15, 22, 7]   \n",
       "1013                                      [22, 35, 22, 22, 4, 22, 5, 12, 11, 15, 11, 22, 11, 39, 35, 12, 39, 21, 35, 37, 35, 37, 24, 10, 34, 10, 24, 15, 24, 15, 16, 21, 4, 22, 12, 22, 11, 15, 11, 5]   \n",
       "1100                                   [22, 38, 15, 22, 28, 20, 37, 11, 11, 24, 4, 3, 11, 11, 5, 15, 21, 21, 35, 12, 22, 21, 15, 22, 6, 46, 16, 15, 12, 21, 21, 15, 12, 21, 42, 40, 35, 37, 12, 21, 7]   \n",
       "1309                                                                                                                [22, 27, 21, 24, 41, 21, 15, 18, 11, 24, 10, 3, 11, 11, 15, 21, 35, 21, 15, 11, 7]   \n",
       "1680                                                               [0, 15, 28, 41, 39, 15, 18, 11, 21, 6, 30, 29, 16, 24, 41, 30, 39, 11, 10, 11, 21, 6, 0, 38, 11, 22, 24, 21, 44, 38, 35, 37, 40, 7]   \n",
       "2261                                                                                                                       [12, 16, 21, 15, 11, 38, 40, 15, 22, 27, 22, 22, 30, 15, 22, 11, 16, 21, 7]   \n",
       "2568                                                             [28, 38, 16, 24, 6, 22, 11, 15, 21, 15, 16, 24, 35, 12, 16, 21, 6, 22, 11, 15, 12, 21, 15, 12, 21, 15, 12, 22, 15, 22, 22, 10, 22, 7]   \n",
       "2784                                                                                        [12, 24, 15, 24, 15, 12, 22, 15, 22, 22, 41, 40, 15, 22, 11, 10, 22, 11, 6, 12, 22, 22, 22, 38, 15, 22, 7]   \n",
       "2810                                                                                                    [22, 22, 22, 22, 22, 22, 20, 37, 22, 10, 22, 22, 15, 22, 11, 35, 11, 6, 12, 21, 38, 15, 22, 7]   \n",
       "3312                                                         [21, 8, 22, 10, 21, 24, 21, 22, 22, 6, 12, 21, 15, 21, 24, 21, 22, 22, 22, 4, 22, 5, 22, 6, 38, 40, 15, 12, 22, 22, 15, 22, 11, 6, 11, 7]   \n",
       "4353                                                                                    [15, 11, 21, 22, 4, 11, 22, 5, 6, 22, 38, 11, 24, 16, 15, 12, 22, 23, 10, 39, 30, 15, 11, 21, 4, 11, 21, 5, 7]   \n",
       "4481                                                                     [22, 16, 21, 38, 40, 35, 11, 46, 22, 38, 12, 21, 27, 16, 16, 21, 6, 12, 21, 11, 16, 21, 6, 15, 16, 21, 15, 21, 15, 11, 24, 7]   \n",
       "4581                                                                                                                                                                                      [11, 23, 11]   \n",
       "4870                                                                       [22, 6, 11, 6, 38, 21, 30, 15, 29, 24, 35, 37, 22, 22, 21, 22, 22, 6, 16, 21, 27, 21, 21, 6, 15, 11, 15, 12, 11, 24, 38, 7]   \n",
       "4891                                                                                                                                                                  [22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5000                                                                                                                                                                        [24, 4, 11, 11, 11, 5, 11]   \n",
       "5197                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5198                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5200                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5201                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5202                                                                                                                                                          [22, 22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5203                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5204                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5205                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5207                                                                                                                                                          [22, 22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5208                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5210                                                                                                                                                              [24, 21, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5211                                                                                                                                                          [22, 22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5213                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5274                                                                                                                                                                  [22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5275                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5517                                                                                                                                                                  [22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5518                                                                                                                                                          [22, 34, 21, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5519                                                                                                                                                                  [22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5520                                                                                                                                                                  [21, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5800                                                                                                                                                                  [22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5801                                                                                                                                                              [22, 22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "5817                                                  [22, 6, 22, 11, 8, 22, 22, 22, 10, 29, 12, 23, 40, 12, 21, 21, 21, 35, 41, 12, 15, 12, 21, 24, 15, 21, 27, 16, 16, 21, 15, 22, 22, 6, 22, 38, 7]   \n",
       "6008  [22, 22, 6, 21, 15, 12, 22, 22, 6, 38, 22, 21, 12, 22, 21, 38, 12, 24, 6, 12, 15, 43, 41, 16, 24, 6, 15, 12, 21, 40, 15, 12, 24, 15, 39, 15, 22, 6, 12, 21, 11, 21, 4, 11, 24, 5, 30, 15, 22, 7]   \n",
       "6019                                                                                                                                                                     [24, 22, 8, 22, 8, 22, 11, 7]   \n",
       "6048                                                                                                                                                                     [24, 37, 8, 22, 8, 22, 11, 7]   \n",
       "6214                                  [16, 24, 6, 28, 38, 6, 40, 15, 16, 21, 15, 24, 15, 21, 10, 24, 15, 11, 21, 15, 22, 27, 16, 21, 15, 21, 6, 15, 11, 21, 15, 11, 10, 11, 21, 15, 12, 21, 15, 11, 7]   \n",
       "6517                                                                                                                                                                  [22, 11, 11, 11, 11, 11, 11, 11]   \n",
       "7265                                                                          [24, 6, 16, 16, 16, 24, 6, 10, 39, 24, 30, 20, 37, 24, 39, 28, 35, 30, 11, 24, 46, 12, 21, 27, 21, 21, 42, 3, 11, 11, 7]   \n",
       "7408                                                                                                    [22, 22, 22, 6, 11, 6, 10, 22, 23, 6, 11, 6, 38, 40, 15, 11, 24, 15, 12, 3, 11, 21, 38, 40, 7]   \n",
       "7534   [12, 22, 21, 21, 15, 22, 38, 22, 6, 44, 38, 15, 39, 22, 15, 22, 10, 38, 15, 22, 6, 38, 24, 15, 22, 10, 22, 24, 15, 22, 6, 46, 11, 22, 24, 38, 40, 15, 12, 21, 21, 21, 15, 22, 11, 6, 10, 22, 7]   \n",
       "8314                                                                                                                                                                  [22, 22, 11, 11, 11, 11, 11, 11]   \n",
       "8353                                                                                                                                                                      [22, 22, 11, 11, 11, 11, 11]   \n",
       "8879                                                                                                                      [0, 15, 11, 6, 11, 10, 11, 28, 20, 37, 30, 15, 28, 6, 30, 39, 40, 16, 24, 7]   \n",
       "9533                                                                                                                                                                              [37, 11, 22, 11, 11]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              tokens  \n",
       "356                                                                                                          [Sir, Mark, Prescott, landed, his, first, group, one, victory, in, 25, years, as, a, trainer, when, his, top, sprinter, Pivotal, ,, a, 100-30, chance, ,, won, the, Nunthorpe, Stakes, on, Thursday, .]  \n",
       "1013                                                         [Corrigendum, to, Commission, Regulation, (, EC, ), No, 1464/96, of, 25, July, 1996, relating, to, a, standing, invitation, to, tender, to, determine, levies, and, /, or, refunds, on, exports, of, white, sugar, (, OJ, No, L, 187, of, 26.7.1996, )]  \n",
       "1100                                                   [Britain, said, on, Thursday, it, would, give, 25, million, pounds, (, $, 39, million, ), of, development, aid, to, the, Caribbean, island, of, Montserrat, ,, where, much, of, the, population, living, in, the, south, has, fled, to, avoid, a, volcano, .]  \n",
       "1309                                                                                                                                                                     [Peru, 's, guerrilla, conflicts, have, cost, at, least, 30,000, lives, and, $, 25, billion, in, damage, to, infrastructure, since, 1980, .]  \n",
       "1680                                                                                                   [\", If, they, 're, saying, at, least, 20, percent, ,, then, their, internal, forecasts, are, probably, saying, 25, or, 30, percent, ,, \", said, one, Sydney, media, analyst, who, declined, to, be, named, .]  \n",
       "2261                                                                                                                                                                                            [The, previous, mark, of, 2:29.34, was, set, by, Mozambique, 's, Maria, Mutola, here, on, August, 25, last, year, .]  \n",
       "2568                                                                              [He, proposed, definite, dates, ,, August, 25, for, return, of, Unita, generals, to, the, joint, army, ,, September, 5, for, the, beginning, of, the, formation, of, the, Government, of, National, Unity, and, Reconciliation, .]  \n",
       "2784                                                                                                                            [No, closures, of, airports, in, the, Commonwealth, of, Independent, States, are, expected, on, August, 24, and, August, 25, ,, the, Russian, Weather, Service, said, on, Friday, .]  \n",
       "2810                                                                                                                                                 [Hong, Kong, Financial, Secretary, Donald, Tsang, will, visit, Indonesia, and, New, Zealand, from, August, 25, to, 31, ,, the, government, said, on, Friday, .]  \n",
       "3312                                                         [NOTE, -, Marble, and, granite, products, distributor, Companion, Marble, ,, a, spinoff, of, construction, materials, concern, Companion, Building, Material, (, Holdings, ), Ltd, ,, was, listed, on, the, Stock, Exchange, on, April, 25, ,, 1996, .]  \n",
       "4353                                                                                                                                                          [At, 11, a.m., EDT, (, 1500, GMT, ), ,, Edouard, was, 1,130, miles, east, of, the, Lesser, Antilles, and, moving, west, at, 14, mph, (, 25, kph, ), .]  \n",
       "4481                                                                                                  [Mickelson, three-stroke, lead, was, cut, to, two, when, Mayfair, birdied, the, course, 's, only, easy, hole, ,, the, par, five, second, hole, ,, while, Mickelson, three-putted, for, par, from, 25, feet, .]  \n",
       "4581                                                                                                                                                                                                                                                                                              [4., Theybers, 25]  \n",
       "4870                                                                                                        [Takeda, ,, 18, ,, showed, poise, far, beyond, his, years, to, overtake, Australian, Ducati, rider, Troy, Corser, ,, last, year, 's, championship, runner-up, ,, with, four, of, the, 25, laps, left, .]  \n",
       "4891                                                                                                                                                                                                                                                                            [London, 22, 12, 1, 9, 611, 462, 25]  \n",
       "5000                                                                                                                                                                                                                                                                            [Extras, (, b-4, lb-5, nb-16, ), 25]  \n",
       "5197                                                                                                                                                                                                                                                                [Alaniya, Vladikavkaz, 24, 16, 5, 3, 48, 25, 53]  \n",
       "5198                                                                                                                                                                                                                                                                      [Dynamo, Moscow, 25, 15, 7, 3, 43, 21, 52]  \n",
       "5200                                                                                                                                                                                                                                                                     [Spartak, Moscow, 25, 14, 7, 4, 48, 24, 49]  \n",
       "5201                                                                                                                                                                                                                                                                        [CSKA, Moscow, 25, 13, 6, 6, 40, 27, 45]  \n",
       "5202                                                                                                                                                                                                                                                        [Lokomotiv, Nizhny, Novgorod, 25, 11, 4, 10, 27, 35, 37]  \n",
       "5203                                                                                                                                                                                                                                                                    [Lokomotiv, Moscow, 25, 9, 9, 7, 30, 24, 36]  \n",
       "5204                                                                                                                                                                                                                                                                [Baltika, Kaliningrad, 25, 8, 10, 7, 29, 26, 34]  \n",
       "5205                                                                                                                                                                                                                                                                      [Torpedo, Moscow, 25, 8, 9, 8, 31, 33, 33]  \n",
       "5207                                                                                                                                                                                                                                                             [Krylya, Sovetov, Samara, 25, 8, 7, 10, 19, 29, 31]  \n",
       "5208                                                                                                                                                                                                                                                                 [Zhemchuzhina, Sochi, 25, 8, 4, 13, 26, 38, 28]  \n",
       "5210                                                                                                                                                                                                                                                          [Chernomorets, Novorossiisk, 25, 7, 5, 13, 25, 38, 26]  \n",
       "5211                                                                                                                                                                                                                                                          [Kamaz, Naberezhnye, Chelny, 24, 5, 4, 15, 25, 42, 19]  \n",
       "5213                                                                                                                                                                                                                                                             [Tekstilshchik, Kamyshin, 25, 3, 9, 13, 15, 30, 18]  \n",
       "5274                                                                                                                                                                                                                                                                          [Canberra, 21, 12, 1, 8, 502, 374, 25]  \n",
       "5275                                                                                                                                                                                                                                                                        [St, George, 21, 12, 1, 8, 421, 344, 25]  \n",
       "5517                                                                                                                                                                                                                                                                                [Molde, 19, 8, 3, 8, 36, 25, 27]  \n",
       "5518                                                                                                                                                                                                                                                                       [Bodo, /, Glimt, 20, 7, 4, 9, 33, 41, 25]  \n",
       "5519                                                                                                                                                                                                                                                                          [Kongsvinger, 20, 7, 4, 9, 26, 38, 25]  \n",
       "5520                                                                                                                                                                                                                                                                         [Stromsgodset, 20, 7, 4, 9, 27, 40, 25]  \n",
       "5800                                                                                                                                                                                                                                                                          [Canberra, 21, 12, 1, 8, 502, 374, 25]  \n",
       "5801                                                                                                                                                                                                                                                                        [St, George, 21, 12, 1, 8, 421, 344, 25]  \n",
       "5817                                                            [PRETORIA, ,, Aug, 25, -, Captain, Sean, Fitzpatrick, and, his, All, Blacks, revisited, the, test, venue, today, to, relive, some, of, the, magic, moments, of, yesterday, 's, momentous, rugby, victory, over, South, Africa, ,, NZPA, reported, .]  \n",
       "6008    [Mohammed, Saleh, ,, director, of, the, Egyptian, Museum, ,, told, Reuters, television, a, U.S., team, found, the, pots, ,, some, of, which, contain, human, intestines, ,, in, a, tomb, built, into, the, rocks, while, digging, in, Dahshour, ,, a, village, 40, km, (, 25, miles, ), south, of, Cairo, .]  \n",
       "6019                                                                                                                                                                                                                                                                       [PRESS, DIGEST, -, Jordan, -, Aug, 25, .]  \n",
       "6048                                                                                                                                                                                                                                                                       [PRESS, DIGEST, -, Israel, -, Aug, 25, .]  \n",
       "6214                                                     [Further, plans, ,, he, said, ,, called, for, full, provision, of, resources, like, fertiliser, and, herbicides, for, 25, percent, of, Ukraine, 's, arable, land, next, year, ,, for, 50, percent, in, 1998, and, 100, percent, of, the, land, in, 1999, .]  \n",
       "6517                                                                                                                                                                                                                                                                            [London, 22, 12, 1, 9, 611, 462, 25]  \n",
       "7265                                                              [Bondholders, ,, other, unsecured, non-trade, creditors, ,, and, existing, shareholders, also, will, receive, warrants, entitling, them, to, roughly, 800,000, shares, when, the, company, 's, market, capitalization, reaches, $, 25, million, .]  \n",
       "7408                                                                                                                                                            [Regula, Susana, Siegfried, ,, 50, ,, and, Nicola, Fleuchaus, ,, 25, ,, were, released, after, 71, days, after, a, $, 200,000, ransom, was, paid, .]  \n",
       "7534  [A, U.S., embassy, spokesman, in, Riyadh, said, Specter, ,, who, arrived, from, neighbouring, Oman, on, Sunday, and, left, on, Monday, ,, had, talks, with, Saudi, and, American, officials, in, Dhahran, ,, where, 19, U.S., airmen, were, killed, by, a, fuel, truck, bomb, on, June, 25, ,, and, Riyadh, .]  \n",
       "8314                                                                                                                                                                                                                                                                          [Mark, Ealham, 1, 2, 0, 30, 25, 15.00]  \n",
       "8353                                                                                                                                                                                                                                                                        [Waqar, Younis, 125, 25, 431, 16, 26.93]  \n",
       "8879                                                                                                                                                                                                       [\", At, 24, ,, 25, or, 26, you, could, get, away, with, it, ,, not, having, played, first-team, games, .]  \n",
       "9533                                                                                                                                                                                                                                                              [Indore, 25, Yellow, 12,750-12,950, 12,900-13,100]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_df[conll2003_df.apply(lambda r: '25' in r['tokens'], axis=1)][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xlm',\n",
       " transformers.models.xlm.configuration_xlm.XLMConfig,\n",
       " transformers.models.xlm.tokenization_xlm.XLMTokenizer,\n",
       " transformers.models.xlm.modeling_xlm.XLMForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "\n",
    "pretrained_model_name = \"xlm-mlm-en-2048\" # \"bert-base-multilingual-cased\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "_, _, test_tokenizer, _ = BLURR.get_hf_objects(\n",
    "    pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '4</w>', '.</w>', 'they', 'bers</w>', '25</w>', '</s>']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_encoding = test_tokenizer(['4.', 'Theybers', '25'], is_split_into_words=True)\n",
    "test_tokenizer.convert_ids_to_tokens(test_encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '4', '.</w>', 'they', 'bers</w>', '25</w>', '</s>']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoding = test_tokenizer('4. Theybers 25', is_split_into_words=True)\n",
    "test_tokenizer.convert_ids_to_tokens(test_encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>4 . theybers 25 </s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.convert_tokens_to_string(['<s>', '4</w>', '.</w>', 'they', 'bers</w>', '25</w>', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '.</w>', 'they', 'bers</w>', '25</w>']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = test_tokenizer.convert_ids_to_tokens(test_encoding['input_ids'], skip_special_tokens=True)\n",
    "toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.', 'theybers', '25']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = [word for word in test_tokenizer.convert_tokens_to_string([tok for tok in toks]).split() ]\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
