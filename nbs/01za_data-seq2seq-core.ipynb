{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.seq2seq.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.seq2seq.core\n",
    "\n",
    "> This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by huggingface transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.7.1+cu110\n",
      "Using fastai 2.2.5\n",
      "Using transformers 4.2.1\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bart',\n",
       " transformers.models.bart.configuration_bart.BartConfig,\n",
       " transformers.models.bart.tokenization_bart_fast.BartTokenizerFast,\n",
       " transformers.models.bart.modeling_bart.BartForConditionalGeneration)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               model_cls=BartForConditionalGeneration)\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq tasks are essentially conditional generation tasks, this applies to specific derived tasks such as summarization and translation.  Given this, we can use the *same* HF_Seq2Seq transforms, `HF_Seq2SeqInput`, and `HF_Seq2SeqBlock` for these tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Seq2SeqInput(HF_BaseInput): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a subclass of `HF_BeforeBatchTransform` for summarization tasks to add `decoder_input_ids` and `labels` to our inputs during training, which will in turn allow the huggingface model to calculate the loss for us.  See [here](https://huggingface.co/transformers/glossary.html#labels) and [here](https://huggingface.co/transformers/glossary.html#decoder-input-ids) for more information on these additional inputs used in summarization, translation, and conversational training tasks. How they should look for particular architectures can be found by looking at those model's `forward` function's docs (See [here](https://huggingface.co/transformers/model_doc/bart.html#transformers.BartModel.forward) for BART for example)\n",
    "\n",
    "Note also that `labels` is simply target_ids shifted to the right by one since the task to is to predict the next token based on the current (and all previous) `decoder_input_ids`.\n",
    "\n",
    "And lastly, we also update our targets to just be the `input_ids` of our target sequence so that fastai's `Learner.show_results` works (again, almost all the fastai bits require returning a single tensor to work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def default_text_gen_kwargs(hf_config, hf_model, task=None):\n",
    "    text_gen_kwargs = {}\n",
    "    hf_config_dict = hf_config.to_dict()\n",
    "\n",
    "    generate_func_args = list(inspect.signature(hf_model.generate).parameters.keys())\n",
    "    for k in generate_func_args:\n",
    "        if (k in hf_config_dict): text_gen_kwargs.update({k: hf_config_dict[k]})\n",
    "            \n",
    "    # not all configs even have a task_specific_params property\n",
    "    if (task is not None):\n",
    "        try:\n",
    "            text_gen_kwargs = { **text_gen_kwargs, **hf_config.task_specific_params[task] }\n",
    "        except: pass\n",
    "        \n",
    "    return text_gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 142,\n",
       " 'min_length': 56,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': True,\n",
       " 'num_beams': 4,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'bad_words_ids': None,\n",
       " 'bos_token_id': 0,\n",
       " 'pad_token_id': 1,\n",
       " 'eos_token_id': 2,\n",
       " 'length_penalty': 2.0,\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'num_return_sequences': 1,\n",
       " 'decoder_start_token_id': 2,\n",
       " 'use_cache': True,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_text_gen_kwargs(hf_config, hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.3607, -1.2068],\n",
       "        [ 1.0000, -0.9912, -0.3943],\n",
       "        [ 1.0000,  1.2608, -0.7091]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "t = torch.randn((3,3));\n",
    "\n",
    "F.pad(t, pad=(1,0), value=1)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Seq2SeqBeforeBatchTransform(HF_BeforeBatchTransform):\n",
    "    \n",
    "    def __init__(self, hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "                 ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "                 max_length=None, max_target_length=None, padding=True, truncation=True,\n",
    "                 tok_kwargs={}, text_gen_kwargs={}, **kwargs):\n",
    "                 \n",
    "        super().__init__(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                         max_length=max_length, padding=padding, truncation=truncation, is_split_into_words=False, \n",
    "                         tok_kwargs=tok_kwargs.copy(), **kwargs)\n",
    "        \n",
    "        store_attr(self=self, names='text_gen_kwargs, max_target_length, ignore_token_id')\n",
    "    \n",
    "    def encodes(self, samples): \n",
    "        samples = L(samples)\n",
    "        \n",
    "        # tokenize\n",
    "        src_texts=samples.itemgot(0).items\n",
    "        tgt_texts=samples.itemgot(1).items if (len(samples[0]) > 1) else None\n",
    "        \n",
    "        try:\n",
    "            tok_d = self.hf_tokenizer.prepare_seq2seq_batch(src_texts=src_texts, tgt_texts=tgt_texts, \n",
    "                                                            max_length=self.max_length, \n",
    "                                                            max_target_length=self.max_target_length,\n",
    "                                                            padding=self.padding, \n",
    "                                                            truncation=self.truncation, \n",
    "                                                            return_tensors='pt', \n",
    "                                                            **self.tok_kwargs)\n",
    "        except NotImplementedError as err:\n",
    "            # not all seq2seq models implement \"prepare_seq2seq_batch\" (i.e., blenderbot)\n",
    "            tok_d = self.hf_tokenizer(src_texts, max_length=self.max_length, padding=self.padding, \n",
    "                                      truncation=self.truncation, return_tensors='pt', **self.tok_kwargs)\n",
    "            \n",
    "            if (tgt_texts):\n",
    "                tok_d_targs = self.hf_tokenizer(tgt_texts, max_length=self.max_target_length, padding=self.padding, \n",
    "                                      truncation=self.truncation, return_tensors='pt', **self.tok_kwargs)\n",
    "            \n",
    "                tok_d['labels'] = tok_d_targs['input_ids']\n",
    "        \n",
    "        # add in target ids for us to use if fastai is calculating the loss\n",
    "        targ_ids = [[]] * len(samples)\n",
    "        if ('labels' in tok_d):\n",
    "            tok_d['labels'].masked_fill_(tok_d['labels'] == self.ignore_token_id, self.hf_tokenizer.pad_token_id)\n",
    "            targ_ids = tok_d['labels'].clone()\n",
    "\n",
    "        # update samples with tokenized inputs (e.g. input_ids, attention_mask, etc...)\n",
    "        d_keys = tok_d.keys()\n",
    "        updated_samples= [ (*[{k: tok_d[k][idx] for k in d_keys}], *tuplify(targ_ids[idx]), *sample[2:]) \n",
    "                          for idx, sample in enumerate(samples) ]\n",
    "        \n",
    "        return updated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include a new AFTER batch `Transform` and `TransformBlock` specific to text-2-text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Seq2SeqAfterBatchTransform(HF_AfterBatchTransform):\n",
    "    def decodes(self, encoded_samples):\n",
    "        input_ids = encoded_samples['input_ids'] if (isinstance(encoded_samples, dict)) else encoded_samples\n",
    "        return self.input_return_type(input_ids, hf_tokenizer=self.hf_tokenizer)\n",
    "    \n",
    "    \n",
    "class HF_Seq2SeqBlock(HF_TextBlock):\n",
    "    \n",
    "    def __init__(self, hf_arch=None, hf_config=None, hf_tokenizer=None, hf_model=None,\n",
    "                 before_batch_tfm=None, after_batch_tfm=None,\n",
    "                 max_length=None, max_target_length=None, padding=True, truncation=True, \n",
    "                 input_return_type=HF_Seq2SeqInput, dl_type=SortedDL, \n",
    "                 tok_kwargs={}, text_gen_kwargs={}, before_batch_kwargs={}, after_batch_kwargs={}, **kwargs):\n",
    "        \n",
    "        # we need to pass text_gen_kwargs into our HF_Seq2SeqBeforeBatchTransform (use default unless specified)\n",
    "        if (len(text_gen_kwargs) == 0): \n",
    "            if (hf_config is None): hf_config = before_batch_tfm.hf_config\n",
    "            if (hf_model is None): hf_model = before_batch_tfm.hf_model\n",
    "            self.text_gen_kwargs = default_text_gen_kwargs(hf_config, hf_model)\n",
    "        else:\n",
    "            self.text_gen_kwargs = text_gen_kwargs.copy()\n",
    "            \n",
    "        # construct our before_batch and after_batch tfms as usual\n",
    "        if (before_batch_tfm is None): \n",
    "            before_batch_tfm = HF_Seq2SeqBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                              max_length=max_length, \n",
    "                                                              max_target_length=max_target_length,\n",
    "                                                              padding=padding, \n",
    "                                                              truncation=truncation,\n",
    "                                                              tok_kwargs=tok_kwargs.copy(), \n",
    "                                                              text_gen_kwargs=text_gen_kwargs, \n",
    "                                                              **before_batch_kwargs.copy())\n",
    "\n",
    "        if (after_batch_tfm is None): \n",
    "            hf_tokenizer = hf_tokenizer if (hf_tokenizer is not None) else before_batch_tfm.hf_tokenizer\n",
    "            after_batch_tfm = HF_Seq2SeqAfterBatchTransform(hf_tokenizer, input_return_type,\n",
    "                                                            **after_batch_kwargs.copy())\n",
    "                \n",
    "        return super().__init__(before_batch_tfm=before_batch_tfm, after_batch_tfm=after_batch_tfm,\n",
    "                                max_length=max_length, padding=padding, truncation=truncation, \n",
    "                                is_split_into_words=False, \n",
    "                                input_return_type=input_return_type, dl_type=dl_type, \n",
    "                                tok_kwargs=tok_kwargs, \n",
    "                                before_batch_kwargs=before_batch_kwargs, \n",
    "                                after_batch_kwargs=after_batch_kwargs, \n",
    "                                **kwargs)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a `DataLoaders.show_batch` for seq2seq tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:HF_Seq2SeqInput, y, samples, dataloaders, ctxs=None, max_n=6, \n",
    "               input_trunc_at=None, target_trunc_at=None, **kwargs):  \n",
    "    # grab our tokenizer and ignore token to decode\n",
    "    hf_before_batch_tfm = get_blurr_tfm(dataloaders.before_batch)\n",
    "    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer\n",
    "    ignore_token_id = hf_before_batch_tfm.ignore_token_id\n",
    "    \n",
    "    res = L([ (hf_tokenizer.decode(s[0], skip_special_tokens=True)[:input_trunc_at], \n",
    "               hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:target_trunc_at])\n",
    "             for s in samples ])      \n",
    "    \n",
    "    display_df(pd.DataFrame(res, columns=['text', 'target'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01za_data-seq2seq-core.ipynb.\n",
      "Converted 01zb_data-seq2seq-language-modeling.ipynb.\n",
      "Converted 01zc_data-seq2seq-summarization.ipynb.\n",
      "Converted 01zd_data-seq2seq-translation.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02za_modeling-seq2seq-core.ipynb.\n",
      "Converted 02zb_modeling-seq2seq-language-modeling.ipynb.\n",
      "Converted 02zc_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 02zc_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-multilabel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
