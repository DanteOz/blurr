{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp text.data.core\n",
    "#|default_cls_lvl 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| nbflags skip_exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text.data.core\n",
    "\n",
    "> This module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to turn your raw datasets into modelable `DataLoaders` for text/NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os, inspect\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce, partial\n",
    "from typing import Any, Callable, List, Optional, Union, Type\n",
    "\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.core import Datasets, DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from blurr.text.utils import get_hf_objects\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.12.0\n",
      "fastai: 2.7.7\n",
      "transformers: 4.20.1\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "import pdb\n",
    "\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, ItemGetter, RandomSplitter\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n",
    "\n",
    "from blurr.text.utils import BlurrText\n",
    "from blurr.utils import print_versions\n",
    "\n",
    "NLP = BlurrText()\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `imdb` to demonstrate how to configure your BLURR for sequence classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/dev/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37865c042b5b45749b935a7de3caa686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess when \"Beat Street\" made a national appearance, \"Flashdance\" came at the same time. The problem with \"Flashdance\" is that there was only one break dancing scene and the rest was jazz dance and ballet. That was one of the reasons why \"Beat Street\" was better. The only movie that could rival \"Beat Street\" seems to be \"Footloose\", because both movies focused on how dance had been used by people to express their utmost feelings.&lt;br /&gt;&lt;br /&gt;The break-dance scenes in \"Beat Street\" come just before the middle and at the end of the flick. And I loved all of them. Almost all of the break tri...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was lucky enough to watch this without any pre viewing hype. I was surprised at the resilience of the ghost's image in my mind the next day, and the day after that. I've watched it 3-4 times, and each time I appreciate it even more. The settings are gorgeous, the town at dusk has beautiful lighting effects, the marsh long shots, and the house itself is sufficiently grown with moss. The main hero is so likable and good natured, that he is easily sympathized with. To the person who complained that there wasn't enough 'spark' in this film, I'd say that it's because the whole fight against t...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really bad. Why anyone thinks this is a good film let alone funny is a true mystery. I like comedies as much as the next man and I LOVED \"A Christmas Story.\" The fact that it has the same director and was based on the same writer's memoirs has me completely puzzled as to why this film is such a complete failure on every level. Charles Grodin is woefully miscast as the father for starters. For another it does not seem to have the same pacing -- it just doesn't flow well. Everything seems tired and forced. The joy of life that permeated the first film is completely absent here -- you just wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I remember the original series vividly mostly due to it's unique blend of wry humor and macabre subject matter. Kolchak was hard-bitten newsman from the Ben Hecht school of big-city reporting, and his gritty determination and wise-ass demeanor made even the most mundane episode eminently watchable. My personal fave was \"The Spanish Moss Murders\" due to it's totally original storyline. A poor,troubled Cajun youth from Louisiana bayou country, takes part in a sleep research experiment, for the purpose of dream analysis. Something goes inexplicably wrong, and he literally dreams to life a swa...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  I guess when \"Beat Street\" made a national appearance, \"Flashdance\" came at the same time. The problem with \"Flashdance\" is that there was only one break dancing scene and the rest was jazz dance and ballet. That was one of the reasons why \"Beat Street\" was better. The only movie that could rival \"Beat Street\" seems to be \"Footloose\", because both movies focused on how dance had been used by people to express their utmost feelings.<br /><br />The break-dance scenes in \"Beat Street\" come just before the middle and at the end of the flick. And I loved all of them. Almost all of the break tri...   \n",
       "1  I was lucky enough to watch this without any pre viewing hype. I was surprised at the resilience of the ghost's image in my mind the next day, and the day after that. I've watched it 3-4 times, and each time I appreciate it even more. The settings are gorgeous, the town at dusk has beautiful lighting effects, the marsh long shots, and the house itself is sufficiently grown with moss. The main hero is so likable and good natured, that he is easily sympathized with. To the person who complained that there wasn't enough 'spark' in this film, I'd say that it's because the whole fight against t...   \n",
       "2  Really bad. Why anyone thinks this is a good film let alone funny is a true mystery. I like comedies as much as the next man and I LOVED \"A Christmas Story.\" The fact that it has the same director and was based on the same writer's memoirs has me completely puzzled as to why this film is such a complete failure on every level. Charles Grodin is woefully miscast as the father for starters. For another it does not seem to have the same pacing -- it just doesn't flow well. Everything seems tired and forced. The joy of life that permeated the first film is completely absent here -- you just wa...   \n",
       "3  I remember the original series vividly mostly due to it's unique blend of wry humor and macabre subject matter. Kolchak was hard-bitten newsman from the Ben Hecht school of big-city reporting, and his gritty determination and wise-ass demeanor made even the most mundane episode eminently watchable. My personal fave was \"The Spanish Moss Murders\" due to it's totally original storyline. A poor,troubled Cajun youth from Louisiana bayou country, takes part in a sleep research experiment, for the purpose of dream analysis. Something goes inexplicably wrong, and he literally dreams to life a swa...   \n",
       "4                                                                                                                                          I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.   \n",
       "\n",
       "   label  is_valid  \n",
       "0      1     False  \n",
       "1      1     False  \n",
       "2      0     False  \n",
       "3      1     False  \n",
       "4      1     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "raw_datasets[0] = raw_datasets[0].add_column(\"is_valid\", [False] * len(raw_datasets[0]))\n",
    "raw_datasets[1] = raw_datasets[1].add_column(\"is_valid\", [True] * len(raw_datasets[1]))\n",
    "\n",
    "final_ds = concatenate_datasets([raw_datasets[0].shuffle().select(range(1000)), raw_datasets[1].shuffle().select(range(200))])\n",
    "imdb_df = pd.DataFrame(final_ds)\n",
    "imdb_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[0].features[\"label\"].names\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"  # \"bert-base-multilingual-cased\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "    pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting with version 2.0, `BLURR` provides a preprocessing base class that can be used to build task specific pre-processed datasets from pandas DataFrames or Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Preprocessor` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # The attribute holding the text\n",
    "        text_attr: str = \"text\",\n",
    "        # The attribute holding the text_pair\n",
    "        text_pair_attr: Optional[str] = None,\n",
    "        # The attribute that should be created if your are processing individual training and validation\n",
    "        # datasets into a single dataset, and will indicate to which each example is associated\n",
    "        is_valid_attr: Optional[str] = \"is_valid\",\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer\n",
    "        tok_kwargs: dict = {},\n",
    "    ):\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.text_attr, self.text_pair_attr = text_attr, text_pair_attr\n",
    "        self.is_valid_attr = is_valid_attr\n",
    "        self.tok_kwargs = tok_kwargs\n",
    "\n",
    "        if \"truncation\" not in self.tok_kwargs:\n",
    "            self.tok_kwargs[\"truncation\"] = True\n",
    "\n",
    "    def process_df(self, training_df: pd.DataFrame, validation_df: Optional[pd.DataFrame] = None):\n",
    "        df = training_df.copy()\n",
    "\n",
    "        # concatenate the validation dataset if it is included\n",
    "        if validation_df is not None:\n",
    "            valid_df = validation_df.copy()\n",
    "            # add an \"is_valid_col\" column to both training/validation DataFrames to indicate what data is part of the validation set\n",
    "            if self.is_valid_attr:\n",
    "                valid_df[self.is_valid_attr] = True\n",
    "                df[self.is_valid_attr] = False\n",
    "\n",
    "            df = pd.concat([df, valid_df])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_hf_dataset(self, training_ds: Dataset, validation_ds: Optional[Dataset] = None):\n",
    "        ds = training_ds\n",
    "\n",
    "        # concatenate the validation dataset if it is included\n",
    "        if validation_ds is not None:\n",
    "            # add an \"is_valid_col\" column to both training/validation DataFrames to indicate what data is part of\n",
    "            # the validation set\n",
    "            if self.is_valid_attr:\n",
    "                validation_ds = validation_ds.add_column(self.is_valid_attr, [True] * len(validation_ds))\n",
    "                training_ds = training_ds.add_column(self.is_valid_attr, [False] * len(training_ds))\n",
    "\n",
    "            ds = concatenate_datasets([training_ds, validation_ds])\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def _tokenize_function(self, example):\n",
    "        txts = example[self.text_attr]\n",
    "        txt_pairs = example[self.text_pair_attr] if self.text_pair_attr else None\n",
    "\n",
    "        return self.hf_tokenizer(txts, txt_pairs, **self.tok_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ClassificationPreprocessor` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ClassificationPreprocessor(Preprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # Whether the dataset should be processed for multi-label; if True, will ensure `label_attrs` are\n",
    "        # converted to a value of either 0 or 1 indiciating the existence of the class in the example\n",
    "        is_multilabel: bool = False,\n",
    "        # The unique identifier in the dataset\n",
    "        id_attr: Optional[str] = None,\n",
    "        # The attribute holding the text\n",
    "        text_attr: str = \"text\",\n",
    "        # The attribute holding the text_pair\n",
    "        text_pair_attr: Optional[str] = None,\n",
    "        # The attribute holding the label(s) of the example\n",
    "        label_attrs: Union[str, List[str]] = \"label\",\n",
    "        # The attribute that should be created if your are processing individual training and validation\n",
    "        # datasets into a single dataset, and will indicate to which each example is associated\n",
    "        is_valid_attr: Optional[str] = \"is_valid\",\n",
    "        # A list indicating the valid labels for the dataset (optional, defaults to the unique set of labels\n",
    "        # found in the full dataset)\n",
    "        label_mapping: Optional[List[str]] = None,\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer\n",
    "        tok_kwargs: dict = {},\n",
    "    ):\n",
    "        tok_kwargs = {**tok_kwargs, \"return_offsets_mapping\": True}\n",
    "        super().__init__(hf_tokenizer, batch_size, text_attr, text_pair_attr, is_valid_attr, tok_kwargs)\n",
    "\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.id_attr = id_attr\n",
    "        self.label_attrs = label_attrs\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def process_df(self, training_df: pd.DataFrame, validation_df: Optional[pd.DataFrame] = None):\n",
    "        df = super().process_df(training_df, validation_df)\n",
    "\n",
    "        # convert even single \"labels\" to a list to make things easier\n",
    "        label_cols = listify(self.label_attrs)\n",
    "\n",
    "        # if \"is_multilabel\", convert all targets to an int, 0 or 1, rounding floats if necessary\n",
    "        if self.is_multilabel:\n",
    "            for label_col in label_cols:\n",
    "                df[label_col] = df[label_col].apply(lambda v: int(bool(max(0, round(v)))))\n",
    "\n",
    "        # if a \"label_mapping\" is included, add a \"[label_col]_name\" field with the label Ids converted to their label names\n",
    "        if self.label_mapping:\n",
    "            for label_col in label_cols:\n",
    "                df[f\"{label_col}_name\"] = df[label_col].apply(lambda v: self.label_mapping[v])\n",
    "\n",
    "        # process df in mini-batches\n",
    "        final_df = pd.DataFrame()\n",
    "        for g, batch_df in df.groupby(np.arange(len(df)) // self.batch_size):\n",
    "            final_df = final_df.append(self._process_df_batch(batch_df))\n",
    "\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "        return final_df\n",
    "\n",
    "    def process_hf_dataset(self, training_ds: Dataset, validation_ds: Optional[Dataset] = None):\n",
    "        ds = super().process_hf_dataset(training_ds, validation_ds)\n",
    "        return Dataset.from_pandas(self.process_df(pd.DataFrame(ds)))\n",
    "\n",
    "    # ----- utility methods -----\n",
    "    def _process_df_batch(self, batch_df):\n",
    "        batch_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # grab our inputs\n",
    "        inputs = self._tokenize_function(batch_df.to_dict(orient=\"list\"))\n",
    "\n",
    "        for txt_seq_idx, txt_attr in enumerate([self.text_attr, self.text_pair_attr]):\n",
    "            if txt_attr is None:\n",
    "                break\n",
    "\n",
    "            char_idxs = []\n",
    "            for idx, offset_mapping in enumerate(inputs[\"offset_mapping\"]):\n",
    "                text_offsets = [offset_mapping[i] for i, seq_id in enumerate(inputs.sequence_ids(idx)) if seq_id == txt_seq_idx]\n",
    "                char_idxs.append([min(text_offsets)[0], max(text_offsets)[1]])\n",
    "\n",
    "            batch_df = pd.concat(\n",
    "                [batch_df, pd.DataFrame(char_idxs, columns=[f\"{txt_attr}_start_char_idx\", f\"{txt_attr}_end_char_idx\"])], axis=1\n",
    "            )\n",
    "            batch_df.insert(\n",
    "                0,\n",
    "                f\"proc_{txt_attr}\",\n",
    "                batch_df.apply(lambda r: r[txt_attr][r[f\"{txt_attr}_start_char_idx\"] : r[f\"{txt_attr}_end_char_idx\"] + 1], axis=1),\n",
    "            )\n",
    "\n",
    "        return batch_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with version 2.0, `BLURR` provides a sequence classification preprocessing class that can be used to preprocess DataFrames or Hugging Face Datasets.\n",
    "\n",
    "This class can be used for preprocessing both multiclass and multilabel classification datasets, and includes a `proc_{your_text_attr}` and `proc_{your_text_pair_attr}` (optional) attributes containing your modified text as a result of tokenization (e.g., if you specify a `max_length` the `proc_{your_text_attr}` may contain truncated text). \n",
    "\n",
    "**Note**: This class works for both slow and fast tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232318/683270517.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "/tmp/ipykernel_232318/683270517.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proc_text</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>label_name</th>\n",
       "      <th>text_start_char_idx</th>\n",
       "      <th>text_end_char_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I guess when \"Beat Street\" made a national appearance, \"Flashdance\" came at the same time.</td>\n",
       "      <td>I guess when \"Beat Street\" made a national appearance, \"Flashdance\" came at the same time. The problem with \"Flashdance\" is that there was only one break dancing scene and the rest was jazz dance and ballet. That was one of the reasons why \"Beat Street\" was better. The only movie that could rival \"Beat Street\" seems to be \"Footloose\", because both movies focused on how dance had been used by people to express their utmost feelings.&lt;br /&gt;&lt;br /&gt;The break-dance scenes in \"Beat Street\" come just before the middle and at the end of the flick. And I loved all of them. Almost all of the break tri...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was lucky enough to watch this without any pre viewing hype. I was surprised at the resilience of the ghost'</td>\n",
       "      <td>I was lucky enough to watch this without any pre viewing hype. I was surprised at the resilience of the ghost's image in my mind the next day, and the day after that. I've watched it 3-4 times, and each time I appreciate it even more. The settings are gorgeous, the town at dusk has beautiful lighting effects, the marsh long shots, and the house itself is sufficiently grown with moss. The main hero is so likable and good natured, that he is easily sympathized with. To the person who complained that there wasn't enough 'spark' in this film, I'd say that it's because the whole fight against t...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        proc_text  \\\n",
       "0                      I guess when \"Beat Street\" made a national appearance, \"Flashdance\" came at the same time.   \n",
       "1  I was lucky enough to watch this without any pre viewing hype. I was surprised at the resilience of the ghost'   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  I guess when \"Beat Street\" made a national appearance, \"Flashdance\" came at the same time. The problem with \"Flashdance\" is that there was only one break dancing scene and the rest was jazz dance and ballet. That was one of the reasons why \"Beat Street\" was better. The only movie that could rival \"Beat Street\" seems to be \"Footloose\", because both movies focused on how dance had been used by people to express their utmost feelings.<br /><br />The break-dance scenes in \"Beat Street\" come just before the middle and at the end of the flick. And I loved all of them. Almost all of the break tri...   \n",
       "1  I was lucky enough to watch this without any pre viewing hype. I was surprised at the resilience of the ghost's image in my mind the next day, and the day after that. I've watched it 3-4 times, and each time I appreciate it even more. The settings are gorgeous, the town at dusk has beautiful lighting effects, the marsh long shots, and the house itself is sufficiently grown with moss. The main hero is so likable and good natured, that he is easily sympathized with. To the person who complained that there wasn't enough 'spark' in this film, I'd say that it's because the whole fight against t...   \n",
       "\n",
       "   label  is_valid label_name  text_start_char_idx  text_end_char_idx  \n",
       "0      1     False        pos                    0                 89  \n",
       "1      1     False        pos                    0                109  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ClassificationPreprocessor(hf_tokenizer, label_mapping=labels, tok_kwargs={\"max_length\": 24})\n",
    "proc_df = preprocessor.process_df(imdb_df)\n",
    "proc_df.columns, len(proc_df)\n",
    "proc_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Hugging Face `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232318/683270517.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "/tmp/ipykernel_232318/683270517.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['proc_text', 'text', 'label', 'is_valid', 'label_name', 'text_start_char_idx', 'text_end_char_idx'],\n",
       "    num_rows: 1200\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ClassificationPreprocessor(hf_tokenizer, label_mapping=labels)\n",
    "proc_ds = preprocessor.process_hf_dataset(final_ds)\n",
    "proc_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TextInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TextInput` object is returned from the decodes method of `BatchDecodeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchTokenizeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Handles everything you need to assemble a mini-batch of inputs and targets, as well as\n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        \"\"\"\n",
    "        This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        is_dict = isinstance(samples[0][0], dict)\n",
    "        test_inp = samples[0][0][\"text\"] if is_dict else samples[0][0]\n",
    "\n",
    "        if is_listy(test_inp) and not self.is_split_into_words:\n",
    "            if is_dict:\n",
    "                inps = [(item[\"text\"][0], item[\"text\"][1]) for item in samples.itemgot(0).items]\n",
    "            else:\n",
    "                inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = [item[\"text\"] for item in samples.itemgot(0).items] if is_dict else samples.itemgot(0).items\n",
    "\n",
    "        inputs = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs\n",
    "        )\n",
    "\n",
    "        d_keys = inputs.keys()\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), as well as extra information\n",
    "        # if the inputs is a dictionary.\n",
    "        # (< 2.0.0): updated_samples = [(*[{k: inputs[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "        updated_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            inps = {k: inputs[k][idx] for k in d_keys}\n",
    "            if is_dict:\n",
    "                inps = {**inps, **{k: v for k, v in sample[0].items() if k not in [\"text\"]}}\n",
    "\n",
    "            trgs = sample[1:]\n",
    "            if self.include_labels and len(trgs) > 0:\n",
    "                inps[\"labels\"] = trgs[0]\n",
    "\n",
    "            updated_samples.append((*[inps], *trgs))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this [article](https://docs.fast.ai/tutorial.transformers.html), `BatchTokenizeTransform` inputs can come in as raw **text**, **a list of words** (e.g., tasks like Named Entity Recognition (NER), where you want to predict the label of each token), or as a **dictionary** that includes extra information you want to use during post-processing.\n",
    "\n",
    "**On-the-fly Batch-Time Tokenization**: \n",
    "\n",
    "Part of the inspiration for this derives from the mechanics of Hugging Face tokenizers, in particular it can return a collated mini-batch of data given a list of sequences. As such, the collating required for our inputs can be done during tokenization ***before*** our batch transforms run in a `before_batch_tfms` transform (where we get a list of examples)! This allows users of BLURR to have everything done dynamically at batch-time without prior preprocessing with at least four potential benefits:\n",
    "1. Less code\n",
    "2. Faster mini-batch creation\n",
    "3. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "4. Flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchDecodeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchDecodeTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs as `input_return_type` for fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: Type = TextInput,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: Optional[str] = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: Optional[PretrainedConfig] = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: Optional[PreTrainedModel] = None,\n",
    "        # Any other keyword arguments\n",
    "        **kwargs\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def decodes(self, items: dict):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        return self.input_return_type(items[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `BatchDecodeTransform`, (one that occurs \"after\" the batch has been created) that will do the decoding for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def blurr_sort_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "    # if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    txt = example[0][\"text\"] if isinstance(example[0], dict) else example[0]\n",
    "    return len(txt) if is_split_into_words else len(hf_tokenizer.tokenize(txt, **tok_kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### blurr_sort_func\n",
       "\n",
       ">      blurr_sort_func (example, hf_tokenizer:transformers.tokenization_utils_ba\n",
       ">                       se.PreTrainedTokenizerBase,\n",
       ">                       is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
       "if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer at 0x7fa2f8205fa0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(blurr_sort_func, title_level=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your inputs for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: Optional[str] = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: Optional[PretrainedConfig] = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: Optional[PreTrainedModel] = None,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "        # The before_batch_tfm you want to use to tokenize your raw data on the fly\n",
    "        # (defaults to an instance of `BatchTokenizeTransform`)\n",
    "        batch_tokenize_tfm: Optional[BatchTokenizeTransform] = None,\n",
    "        # The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods,\n",
    "        # (defaults to BatchDecodeTransform)\n",
    "        batch_decode_tfm: Optional[BatchDecodeTransform] = None,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: Optional[int] = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = TextInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: Optional[DataLoader] = None,\n",
    "        # Any keyword arguments you want applied to your `batch_tokenize_tfm`\n",
    "        batch_tokenize_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to your `batch_decode_tfm` (will be set as a fastai `batch_tfms`)\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `TextBlock`\n",
    "        **kwargs\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and batch_tokenize_tfm is None:\n",
    "            raise ValueError(\"You must supply an hf_arch, hf_config, hf_tokenizer, hf_model -or- a BatchTokenizeTransform\")\n",
    "\n",
    "        if batch_tokenize_tfm is None:\n",
    "            batch_tokenize_tfm = BatchTokenizeTransform(\n",
    "                hf_arch,\n",
    "                hf_config,\n",
    "                hf_tokenizer,\n",
    "                hf_model,\n",
    "                include_labels=include_labels,\n",
    "                ignore_token_id=ignore_token_id,\n",
    "                max_length=max_length,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                is_split_into_words=is_split_into_words,\n",
    "                tok_kwargs=tok_kwargs.copy(),\n",
    "                **batch_tokenize_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if batch_decode_tfm is None:\n",
    "            batch_decode_tfm = BatchDecodeTransform(input_return_type=input_return_type, **batch_decode_kwargs.copy())\n",
    "\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                blurr_sort_func,\n",
    "                hf_tokenizer=batch_tokenize_tfm.hf_tokenizer,\n",
    "                is_split_into_words=batch_tokenize_tfm.is_split_into_words,\n",
    "                tok_kwargs=batch_tokenize_tfm.tok_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "            dl_type = partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs={\"before_batch\": batch_tokenize_tfm}, batch_tfms=batch_decode_tfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic `DataBlock` for our inputs, `TextBlock` is designed with sensible defaults to minimize user effort in defining their transforms pipeline. It handles setting up your `BatchTokenizeTransform` and `BatchDecodeTransform` transforms regardless of data source (e.g., this will work with files, DataFrames, whatever). \n",
    "\n",
    "**Note**: You must either pass in your own instance of a `BatchTokenizeTransform` class or the Hugging Face objects returned from `BLURR.get_hf_objects` (e.g.,architecture, config, tokenizer, and model). The other args are optional.\n",
    "\n",
    "We also include a `blurr_sort_func` that works with `SortedDL` to properly sort based on the number of tokens in each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility classes and methods \n",
    "\n",
    "These methods are use internally for getting blurr transforms associated to your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = BatchTokenizeTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline, tfm_class:fastcore.\n",
       ">                     transform.Transform=<class'__main__.BatchTokenizeTransform\n",
       ">                     '>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchTokenizeTransform | The transform to find |"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer at 0x7fa2f81b4070>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_blurr_tfm, title_level=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def first_blurr_tfm(\n",
    "    # Your fast.ai `DataLoaders\n",
    "    dls: DataLoaders,\n",
    "    # The Blurr transforms to look for in order\n",
    "    tfms: List[Transform] = [BatchTokenizeTransform, BatchDecodeTransform],\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as\n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    for tfm in tfms:\n",
    "        found_tfm = get_blurr_tfm(dls.before_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n",
    "\n",
    "        found_tfm = get_blurr_tfm(dls.after_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders, tfms:List[fastcore.tra\n",
       ">                       nsform.Transform]=[<class'__main__.BatchTokenizeTransfor\n",
       ">                       m'>,<class'__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | typing.List[fastcore.transform.Transform] | [<class '__main__.BatchTokenizeTransform'>, <class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer at 0x7fa2fbd80340>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(first_blurr_tfm, title_level=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our labels list, we'll use it to look up the value of our target(s)\n",
    "    trg_labels = tfm.kwargs[\"labels\"] if (\"labels\" in tfm.kwargs) else None\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    for idx, (input_ids, label, sample) in enumerate(zip(x, y, samples)):\n",
    "        if idx >= max_n:\n",
    "            break\n",
    "\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "        for item in sample[n_inp:]:\n",
    "            if not torch.is_tensor(item):\n",
    "                trg = trg_labels[int(item)] if trg_labels else item\n",
    "            elif is_listy(item.tolist()):\n",
    "                trg = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.numpy()\n",
    "            else:\n",
    "                trg = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level Examples\n",
    "\n",
    "The following eamples demonstrate several approaches to construct your `DataBlock` for sequence classication tasks using the mid-level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Get your Hugging Face objects.\n",
    "\n",
    "There are a bunch of ways we can get at the four Hugging Face elements we need (e.g., architecture name, tokenizer, config, and model).  We can just create them directly, or we can use one of the helper methods available via `NLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"  # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 512]), 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  8487,     6,  ...,     6,    38,     2],\n",
       "         [    0, 25143,  3082,  ...,    30,  1599,     2],\n",
       "         [    0,    38,  1548,  ...,  1206,   324,     2],\n",
       "         [    0,    38,  1395,  ...,    54, 13119,     2]], device='cuda:1'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:1'),\n",
       " 'labels': TensorCategory([0, 0, 1, 0], device='cuda:1')}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay, so I'm not a big video game buff, but was the game House of the Dead really famous enough to make a movie from? Sure, they went as far as to actually put in quick video game clips throughout the movie, as though justifying any particular scene of violence, but there are dozens and dozens of games that look exactly the same, with the hand in the bottom on the screen, supposedly your own, holding whatever weapon and goo-ing all kinds of aliens or walking dead or snipers or whatever the case</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MYRA BRECKINRIDGE is one of those rare films that established its place in film history immediately. Praise for the film was absolutely nonexistent, even from the people involved in making it. This film was loathed from day one. While every now and then one will come across some maverick who will praise the film on philosophical grounds (aggressive feminism or the courage to tackle the issue of transgenderism), the film has not developed a cult following like some notorious flops do. It's not h</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a preprocessed dataset\n",
    "\n",
    "Preprocessing your raw data is the more traditional approach to using Transformers. It is required, for example, when you want to work with documents longer than your model will allow. A preprocessed dataset is used in the same way a non-preprocessed dataset is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1a: Get your Hugging Face objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1b. Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232318/683270517.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n",
      "/tmp/ipykernel_232318/683270517.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  final_df = final_df.append(self._process_df_batch(batch_df))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['proc_text', 'text', 'label', 'is_valid', 'label_name', 'text_start_char_idx', 'text_end_char_idx'],\n",
       "    num_rows: 1200\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ClassificationPreprocessor(hf_tokenizer, label_mapping=labels)\n",
    "proc_ds = preprocessor.process_hf_dataset(final_ds)\n",
    "proc_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ItemGetter(\"proc_text\"), get_y=ItemGetter(\"label\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(proc_ds, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 1930s saw a vogue for documentary films about remote corners of the world, with an emphasis on wild animals, exotic terrain and primitive people with unusual cultures. Despite the logistics of transporting a film crew to a distant and dangerous place, and then bringing 'em back alive (with the film footage), such films were often much cheaper to make than were conventional Hollywood features... because there were no expensive sets, costumes, or high-priced movie stars.&lt;br /&gt;&lt;br /&gt;The most s</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I revisited Grand Canyon earlier this year when I set out to devise a ten best list of the 1990's. I first saw the film when I was 17 years old. How did I hear about it? It was reviewed, and recommended highly, by Siskel &amp; Ebert in 1991, and I eventually caught it on video a year later.&lt;br /&gt;&lt;br /&gt;It's a great film, a powerful film, a healing film, about the power of listening, truly listening to one another. I've seen it six times now, and it entertains and inspires me with every subsequent vi</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing extra information\n",
    "\n",
    "As of v.2, `BLURR` now also allows you to pass extra information alongside your inputs in the form of a dictionary.  If you use this approach, you must assign your text(s) to the `text` attribute of the dictionary.  This is a useful approach when splitting long documents into chunks, but wanting to score/predict by example rather than chunk (for example in extractive question answering tasks).\n",
    "\n",
    "**Note**: A good place to access to this extra information during training/validation is in the `before_batch` method of a `Callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "\n",
    "\n",
    "def get_x(item):\n",
    "    return {\"text\": item.text, \"another_val\": \"testing123\"}\n",
    "\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=ColReader(\"label\"), splitter=ColSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 512]), 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay, so I'm not a big video game buff, but was the game House of the Dead really famous enough to make a movie from? Sure, they went as far as to actually put in quick video game clips throughout the movie, as though justifying any particular scene of violence, but there are dozens and dozens of games that look exactly the same, with the hand in the bottom on the screen, supposedly your own, holding whatever weapon and goo-ing all kinds of aliens or walking dead or snipers or whatever the case</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I approach films about talking animals with care. For every wonderful one like Babe, you get an equally poor one like the dreadful remake of Homeward Bound: The Incredible Journey. Or in the case of Cats &amp; Dogs, you have a great idea for a film not living up to its potential. When I heard about Paulie, the premise of a wisecracking parrot didn't exactly fill me with confidence. But I found the film a pleasant surprise. And it manages to sneak its way into your heart without you realising.&lt;br /&gt;</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level API\n",
    "\n",
    "For working with PyTorch and/or fast.ai Datasets & DataLoaders, the low-level API allows you to get back fast.ai specific features such as `show_batch`, `show_results`, etc... when using plain ol' PyTorch Datasets, Hugging Face Datasets, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextBatchCreator` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@dataclass\n",
    "class TextBatchCreator:\n",
    "    \"\"\"\n",
    "    A class that can be assigned to a `TfmdDL.create_batch` method; used to in Blurr's low-level API\n",
    "    to create batches that can be used in the Blurr library\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator: Type = None,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.data_collator = data_collator if (data_collator) else DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"This method will collate your data using `self.data_collator` and add a target element to the\n",
    "        returned tuples if `labels` are defined as is the case when most Hugging Face datasets\n",
    "        \"\"\"\n",
    "        batch = self.data_collator(features)\n",
    "        if isinstance(features[0], dict):\n",
    "            return dict(batch), batch[\"labels\"] if (\"labels\" in features[0]) else dict(batch)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextDataLoader` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates()\n",
    "class TextDataLoader(TfmdDL):\n",
    "    \"\"\"\n",
    "    A transformed `DataLoader` that works with Blurr.\n",
    "    From the fastai docs: A `TfmDL` is described as \"a DataLoader that creates Pipeline from a list of Transforms\n",
    "    for the callbacks `after_item`, `before_batch` and `after_batch`. As a result, it can decode or show a processed batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A standard PyTorch Dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `BatchTokenizeTransform`\n",
    "        # to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel,\n",
    "        # An instance of `BlurrBatchCreator` or equivalent (defaults to `BlurrBatchCreator`)\n",
    "        batch_creator: Optional[TextBatchCreator] = None,\n",
    "        # The batch_tfm used to decode Blurr batches (defaults to `BatchDecodeTransform`)\n",
    "        batch_decode_tfm: Optional[BatchDecodeTransform] = None,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: Type = TextInput,\n",
    "        # (optional) A preprocessing function that will be applied to your dataset\n",
    "        preproccesing_func: Callable[\n",
    "            [Union[torch.utils.data.dataset.Dataset, Datasets], PreTrainedTokenizerBase, PreTrainedModel],\n",
    "            Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        ] = None,\n",
    "        # Keyword arguments to be applied to your `batch_decode_tfm`\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Keyword arguments to be applied to `BlurrDataLoader`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # if the underlying dataset needs to be preprocessed first, apply the preproccesing_func to it\n",
    "        if preproccesing_func:\n",
    "            dataset = preproccesing_func(dataset, hf_tokenizer, hf_model)\n",
    "\n",
    "        # define what happens when a batch is created (e.g., this is where collation happens)\n",
    "        if \"create_batch\" in kwargs:\n",
    "            kwargs.pop(\"create_batch\")\n",
    "        if not batch_creator:\n",
    "            batch_creator = TextBatchCreator(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        # define the transform applied after the batch is created (used of show methods)\n",
    "        if \"after_batch\" in kwargs:\n",
    "            kwargs.pop(\"after_batch\")\n",
    "        if not batch_decode_tfm:\n",
    "            batch_decode_tfm = BatchDecodeTransform(\n",
    "                input_return_type, hf_arch, hf_config, hf_tokenizer, hf_model, **batch_decode_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        super().__init__(dataset=dataset, create_batch=batch_creator, after_batch=batch_decode_tfm, **kwargs)\n",
    "        store_attr(names=\"hf_arch, hf_config, hf_tokenizer, hf_model\")\n",
    "\n",
    "    def new(\n",
    "        self,\n",
    "        # A standard PyTorch and fastai dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets] = None,\n",
    "        # The class you want to create an instance of (will be \"self\" if None)\n",
    "        cls: Type = None,\n",
    "        #  Any additional keyword arguments you want to pass to the __init__ method of `cls`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We have to override the new method in order to add back the Hugging Face objects in this factory\n",
    "        method (called for example in places like `show_results`). With the exception of the additions to the kwargs\n",
    "        dictionary, the code below is pulled from the `DataLoaders.new` method as is.\n",
    "        \"\"\"\n",
    "        # we need to add these arguments back in (these, after_batch, and create_batch will go in as kwargs)\n",
    "        kwargs[\"hf_arch\"] = self.hf_arch\n",
    "        kwargs[\"hf_config\"] = self.hf_config\n",
    "        kwargs[\"hf_tokenizer\"] = self.hf_tokenizer\n",
    "        kwargs[\"hf_model\"] = self.hf_model\n",
    "\n",
    "        return super().new(dataset, cls, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level Examples\n",
    "\n",
    "The following example demonstrates how to use the low-level API with standard PyTorch/Hugging Face/fast.ai Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/dev/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34bbb74ec58435eb9b178d40e95e35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dev/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6bcdb1a644e8c684.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc17e2336c145a8a047b1008c45b9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dev/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-86ae103db8d3e040.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return hf_tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Dataset pre-processing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def preproc_hf_dataset(\n",
    "    # A standard PyTorch Dataset or fast.ai Datasets\n",
    "    dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A Hugging Face model\n",
    "    hf_model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training\n",
    "    libraries\n",
    "    \"\"\"\n",
    "    if (\"label\") in dataset.column_names:\n",
    "        dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    hf_model_fwd_args = list(inspect.signature(hf_model.forward).parameters.keys())\n",
    "    bad_cols = set(dataset.column_names).difference(hf_model_fwd_args)\n",
    "    dataset = dataset.remove_columns(bad_cols)\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### preproc_hf_dataset\n",
       "\n",
       ">      preproc_hf_dataset (dataset:Union[torch.utils.data.dataset.Dataset,fastai\n",
       ">                          .data.core.Datasets], hf_tokenizer:transformers.token\n",
       ">                          ization_utils_base.PreTrainedTokenizerBase,\n",
       ">                          hf_model:transformers.modeling_utils.PreTrainedModel)\n",
       "\n",
       "This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dataset | typing.Union[torch.utils.data.dataset.Dataset, fastai.data.core.Datasets] | A standard PyTorch Dataset or fast.ai Datasets |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase | A Hugging Face tokenizer |\n",
       "| hf_model | PreTrainedModel | A Hugging Face model |"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer at 0x7fa2f8e10a60>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(preproc_hf_dataset, title_level=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build your `DataLoaders`.\n",
    "\n",
    "Use `BlurrDataLoader` to build Blurr friendly dataloaders from your datasets. Passing `{'labels': label_names}` to your `batch_tfm_kwargs` will ensure that your lable/target names will be displayed in methods like `show_batch` and `show_results` (just as it works with the mid-level API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"label\"].names\n",
    "\n",
    "trn_dl = TextDataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dl = TextDataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 68])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeVries did make one stop in town Wednesday - registering as a sex offender at the Soledad Police Department. Convicted child molester Brian DeVries spoke out in response to community outrage Wednesday afternoon after registering as a sex offender at the Soledad Police Department.</td>\n",
       "      <td>not_equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Already suffering with the nation's worst credit rating, the state is operating for the first time completely on borrowed money. The Davis administration says the state is operating for the first time completely on borrowed money.</td>\n",
       "      <td>not_equivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForSequenceClassification',\n",
       " 'BartForSequenceClassification',\n",
       " 'BertForSequenceClassification',\n",
       " 'BigBirdForSequenceClassification',\n",
       " 'BigBirdPegasusForSequenceClassification',\n",
       " 'BloomForSequenceClassification',\n",
       " 'CTRLForSequenceClassification',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CanineForSequenceClassification',\n",
       " 'ConvBertForSequenceClassification',\n",
       " 'Data2VecAudioForSequenceClassification',\n",
       " 'Data2VecTextForSequenceClassification',\n",
       " 'DebertaForSequenceClassification',\n",
       " 'DebertaV2ForSequenceClassification',\n",
       " 'DistilBertForSequenceClassification',\n",
       " 'ElectraForSequenceClassification',\n",
       " 'FNetForSequenceClassification',\n",
       " 'FlaubertForSequenceClassification',\n",
       " 'FunnelForSequenceClassification',\n",
       " 'GPT2ForSequenceClassification',\n",
       " 'GPTJForSequenceClassification',\n",
       " 'GPTNeoForSequenceClassification',\n",
       " 'HubertForSequenceClassification',\n",
       " 'IBertForSequenceClassification',\n",
       " 'LEDForSequenceClassification',\n",
       " 'LayoutLMForSequenceClassification',\n",
       " 'LayoutLMv2ForSequenceClassification',\n",
       " 'LayoutLMv3ForSequenceClassification',\n",
       " 'LongformerForSequenceClassification',\n",
       " 'MBartForSequenceClassification',\n",
       " 'MPNetForSequenceClassification',\n",
       " 'MegatronBertForSequenceClassification',\n",
       " 'MobileBertForSequenceClassification',\n",
       " 'NystromformerForSequenceClassification',\n",
       " 'OpenAIGPTForSequenceClassification',\n",
       " 'PLBartForSequenceClassification',\n",
       " 'PerceiverForSequenceClassification',\n",
       " 'QDQBertForSequenceClassification',\n",
       " 'ReformerForSequenceClassification',\n",
       " 'RemBertForSequenceClassification',\n",
       " 'RoFormerForSequenceClassification',\n",
       " 'RobertaForSequenceClassification',\n",
       " 'SEWDForSequenceClassification',\n",
       " 'SEWForSequenceClassification',\n",
       " 'SqueezeBertForSequenceClassification',\n",
       " 'TransfoXLForSequenceClassification',\n",
       " 'UniSpeechForSequenceClassification',\n",
       " 'UniSpeechSatForSequenceClassification',\n",
       " 'Wav2Vec2ConformerForSequenceClassification',\n",
       " 'Wav2Vec2ForSequenceClassification',\n",
       " 'WavLMForSequenceClassification',\n",
       " 'XLMForSequenceClassification',\n",
       " 'XLMRobertaForSequenceClassification',\n",
       " 'XLMRobertaXLForSequenceClassification',\n",
       " 'XLNetForSequenceClassification',\n",
       " 'YosoForSequenceClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "[model_type for model_type in NLP.get_models(task=\"SequenceClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-random-bart\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"google/bigbird-pegasus-large-arxiv\",\n",
    "    \"hf-internal-testing/tiny-random-ctrl\",\n",
    "    \"camembert-base\",\n",
    "    \"hf-internal-testing/tiny-random-canine\",\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    \"hf-internal-testing/tiny-random-deberta-v2\",\n",
    "    \"hf-internal-testing/tiny-random-distilbert\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    \"google/fnet-base\",\n",
    "    \"hf-internal-testing/tiny-random-flaubert\",\n",
    "    \"hf-internal-testing/tiny-random-funnel\",\n",
    "    \"hf-internal-testing/tiny-random-gpt2\",\n",
    "    \"anton-l/gpt-j-tiny-random\",\n",
    "    \"hf-internal-testing/tiny-random-gpt_neo\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    \"hf-internal-testing/tiny-random-led\",\n",
    "    \"hf-internal-testing/tiny-random-longformer\",\n",
    "    \"hf-internal-testing/tiny-random-mbart\",\n",
    "    \"hf-internal-testing/tiny-random-mpnet\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                 could not test\n",
    "    \"hf-internal-testing/tiny-random-mobilebert\",\n",
    "    \"openai-gpt\",\n",
    "    \"google/reformer-crime-and-punishment\",\n",
    "    \"google/rembert\",\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    \"hf-internal-testing/tiny-random-transfo-xl\",\n",
    "    \"xlm-mlm-en-2048\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# for model_name in pretrained_model_names:\n",
    "#     tok = AutoTokenizer.from_pretrained(model_name)\n",
    "#     print(f'=== {model_name} ===')\n",
    "#     print(f'=== {tok.padding_side} ===')\n",
    "#     print(f'=== {tok.pad_token_id} ===')\n",
    "#     print(tok(['hi', 'hello everyone. its good to be here'], ['yo', 'yo'], padding='max_length', max_length=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/dev/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64b102e263d4056bf410eaeab442153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "raw_datasets = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "raw_datasets[0] = raw_datasets[0].add_column(\"is_valid\", [False] * len(raw_datasets[0]))\n",
    "raw_datasets[1] = raw_datasets[1].add_column(\"is_valid\", [True] * len(raw_datasets[1]))\n",
    "\n",
    "final_ds = concatenate_datasets([raw_datasets[0].shuffle().select(range(1000)), raw_datasets[1].shuffle().select(range(200))])\n",
    "imdb_df = pd.DataFrame(final_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/58e74e0b857853ff5f9ee7a575511635d23da669cfe61bcf3b672a36ea4ab454.9af053cb27b8f4424402f231c8e71b10e864ceed92a8b453a8177062fc021f84\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-albert\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 64,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 5000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/spiece.model from cache at /home/dev/.cache/huggingface/transformers/6e205c4ee29c198d999f17a26c33a64233c1c581bcf431b1127d01deb7510780.59ce0212608d6704aca57576f13aa361e91017641acf3e8c617a6e27eb8d2927\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/90d9ebedf8e582300430763b923aaee1c4777349092e3182d6e7ba05165e1db9.9df60c00984c1dc05cbb833ba48423b71a3639e3b10e2170225f8700b0f5a7c7\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/e0f085d5ba6003dd56d84de7485216c332f557b7c0eb99b00ebeddbf6e91c106.15ed5b79b197b4fcc5f3f80b2ee89a5a3ad708dbd076575cd22cffd9e1a56284\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/d941b8eff33876a3e293a9e2cc2994ba7e213dc57619a979cf5231afe5691f36.3fd7ec5af4cc9ea1b3f11cbf79b274b85f9794f5287a4c38cabdc2e3510bf7b1\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-albert/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/92624cdddd009bb05315b0abfb915b30cfb63bc7aedfea0d19c6e017db5f5d9c.8cfb9ed9bcc91b299ad021927f6edb2d5b464139309c0508eb05de84f87d0dba\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-albert were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-albert and are newly initialized: ['albert.pooler.weight', 'classifier.weight', 'classifier.bias', 'albert.pooler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (508 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx(1995) is the developing world's answer to silence of the lambs. where silence' terrorized our peace of mind, citizen' exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals.br /br /citizenx may also argue against(!) the death penalty far better than</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chris rock deserves better than he gives himself in \"down to earth.\" as directed by brothers chris &amp; paul weitz of \"american pie\" fame, this uninspired remake of warren beatty's 1978 fantasy \"heaven can wait,\" itself a rehash of 1941's \"here comes mr. jordan,\" lacks the abrasively profane humor that won chris rock an emmy for his first hbo special. predictably, he spouts swear words from</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d05dbc265bc4edd13898dbd34979f1c8f8bdb813879d888b29613395918f0bd7.94f5f7396aae03ef1dfe169c751fe4cbc3e633b7c0da1447d12708954cc9260e\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-bart\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 16,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 4,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 4,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 100,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-bart ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/2ae54909e15516ce24b748c6c697b88ef42930fb1e85714bf11ff092283d0863.c45ad9d7931b838f87a2793ae47dd5fd5edc1a6b0055b898d6e65c3c693ade29\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/e9728b60cbb5003f94bf08d3d0db74db5a7d288a621cdd2257e595711a3ca2cc.0f509d79aaf2540546fff94fa84c1f23aa8e983149cc0dfb587c82d00a3b2497\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/9388e9110cda2fcf2bbbe6a5e39976e14594c47d150b5bb5704c54b02de715f5.31a3adf1b4fbab82d0dce2f9fa890fad77a90174dc28bf62778ed5bdf884c5f4\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/1a7aaf44c897f1b516e5c72ed0654439713a950096e2b69503f05dba3503f565.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/4e2c13870adb948384165befd27ab680c89cfcad7058a7df64ec31c1557925eb.5bd86515ea403d8ec5536d85d95578736f80d4d44e3190b74ca46fa65e93f48f\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/2e23f9a734dde2df4af786e6eb49ef3cc630fb78406b4dc279d4aca5cd6030f1.1999dd93d35198dcdfb408fbeba4850c17a676c662b948dc949cb734f6c78b83\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-bart were not used when initializing BartForSequenceClassification: ['decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.bias', 'encoder.layers.1.fc1.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.embed_positions.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'final_logits_bias', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'encoder.embed_tokens.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.1.fc2.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.fc2.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.fc1.bias', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.0.final_layer_norm.bias', 'lm_head.weight', 'shared.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.fc1.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.0.fc1.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'encoder.layernorm_embedding.bias', 'decoder.layers.0.fc1.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'qa_outputs.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.final_layer_norm.weight', 'encoder.embed_positions.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'encoder.layers.0.fc1.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.final_layer_norm.weight', 'qa_outputs.weight', 'decoder.embed_tokens.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layernorm_embedding.bias', 'decoder.layernorm_embedding.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.fc1.weight', 'decoder.layers.0.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-bart.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1214 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 Simple Rules for Dating My Teenage Daughter had an auspicious start. The supremely-talented Tom Shadyac was involved in the project. This meant that the comedy would be nothing less of spectacular, and that's exactly what happened: the show remains one of the freshest, funniest, wittiest shows made in a very long time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/4ef0a64342c25365033ef9ffe134f930a8f9719be920a63634d932cfa28f7d25.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/4ef0a64342c25365033ef9ffe134f930a8f9719be920a63634d932cfa28f7d25.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/17cdefb0a9cae0c7b031ec7c5844a6a6ea29d97a1a627938644ef0e1e2414277.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/4ef0a64342c25365033ef9ffe134f930a8f9719be920a63634d932cfa28f7d25.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/4ef0a64342c25365033ef9ffe134f930a8f9719be920a63634d932cfa28f7d25.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-bert/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/e52fefa60ba462f2a83a8ff65490a2319cc5b0d97771d1a7aaeec7967faff70a.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also argue against (! ) the death penalty far better than kevin spacey's the life of david gayle ( 2002 ). &lt; br / &gt; &lt; br / &gt; humans are machiavellian mammals, under</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 simple rules for dating my teenage daughter had an auspicious start. the supremely - talented tom shadyac was involved in the project. this meant that the comedy would be nothing less of spectacular, and that's exactly what happened : the show remains one of the freshest, funniest, wittiest shows made in a very long time. every line, facial expression, casting choice, scene, all wreaked of perfection. there was not one episode after which i thought, \" man that wasn't as good as the rest \". each one was a standout. again, this is the</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/spiece.model from cache at /home/dev/.cache/huggingface/transformers/d318d7bb69cafb1d8964fc87515592ac3092a2c8fdb305068f9ba4020df3ee3b.271d467a9adc15fb44348481bc75c48b63cba0fd4934bc5377d63a63de052c45\n",
      "loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/400be7e354ea6eb77319bcc7fa34899ec9fa2e3aff0fa677f6eb7e45a01b1548.75b358ecb30fa6b001d9d87bfde336c02d9123e7a8f5b90cc890d0f6efc3d4a3\n",
      "loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/d20a688e918d227ce5dbcd5f2b570a093cee6b095952d74b9c245b245e6510de.c8f14f85d9ff88cdd1fe7094cde11f85b74fcb7eb03616822964895bc6626c3b\n",
      "loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/bigbird-roberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/c523b12608662dbff39b2c24a608a6ff30857bc7967a5c9b00cb76d1147e223b.06e7996caf35449212f17d31a2129bb55c59c19054fcf8552a847e4bcb475688\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 Simple Rules for Dating My Teenage Daughter had an auspicious start. The supremely-talented Tom Shadyac was involved in the project. This meant that the comedy would be nothing less of spectacular, and that's exactly what happened: the show remains one of the freshest, funniest, wittiest shows made in a very long time. Every line, facial expression, casting choice, scene, all wreaked of perfection. There was not one episode after which I thought, \"Man that wasn't as good as the rest\". Each one was a standout. Again, this is the kind of perfectionism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/0c5c2a21485ba0e75fd41928cbb901586887479c8fad3f3965b9bcae7632825b.c65855e5554b00a37b55e85d3a9f9dd66ca2c3f276ee79e8daea2165fe581bbf\n",
      "Model config BigBirdPegasusConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-pegasus-large-arxiv\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdPegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 256,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"bigbird_pegasus\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tokenizer_class\": \"PegasusTokenizer\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_bias\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-pegasus-large-arxiv ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/0c5c2a21485ba0e75fd41928cbb901586887479c8fad3f3965b9bcae7632825b.c65855e5554b00a37b55e85d3a9f9dd66ca2c3f276ee79e8daea2165fe581bbf\n",
      "Model config BigBirdPegasusConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-pegasus-large-arxiv\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdPegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 256,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"bigbird_pegasus\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tokenizer_class\": \"PegasusTokenizer\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_bias\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/spiece.model from cache at /home/dev/.cache/huggingface/transformers/4d3b0ca7cb9d40c5e755ffd85282880938e73655c7b65ca44f9eeb070da48be3.21177a4e15d83c08f576a09d92bb0bee2a2b53658cc3ff47604571b79f4db789\n",
      "loading file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/114304786763b6699cb6410cf9157c5467c015572ef969732a328a71f49b2e87.5d90d0c694dc11b42f74927efea80a49f2123fc4d91d2f05048d03acbd34673b\n",
      "loading file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/ef8d91bae0b92a3bc897e6a08313308075de1bfdab34845cec4ed7606ada8f15.4eda581f816a0a941629106f0338c957910ce4839ecf7e3e743bb79523bf7249\n",
      "loading file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/940fc969603f83e41e0bcb33868be25d6e90099e55291fb3fa26f1ce443c0bde.6639f7557cd6e8a5f981cb08349d73b64cf6197341226dfa4e90024a29b14afb\n",
      "loading configuration file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/0c5c2a21485ba0e75fd41928cbb901586887479c8fad3f3965b9bcae7632825b.c65855e5554b00a37b55e85d3a9f9dd66ca2c3f276ee79e8daea2165fe581bbf\n",
      "Model config BigBirdPegasusConfig {\n",
      "  \"_name_or_path\": \"google/bigbird-pegasus-large-arxiv\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdPegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 256,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"bigbird_pegasus\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tokenizer_class\": \"PegasusTokenizer\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_bias\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/bigbird-pegasus-large-arxiv/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/71429d2e11530190897b591fdb8a73d926bddff265d07ef9f310073a543026c3.567d2eb891746c6387d5f8f2d270c24dc633f9c55f4eedbf1eb69378249afacd\n",
      "Some weights of the model checkpoint at google/bigbird-pegasus-large-arxiv were not used when initializing BigBirdPegasusForSequenceClassification: ['lm_head.weight', 'final_logits_bias']\n",
      "- This IS expected if you are initializing BigBirdPegasusForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdPegasusForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdPegasusForSequenceClassification were not initialized from the model checkpoint at google/bigbird-pegasus-large-arxiv and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbigbird_pegasus\n",
      "tokenizer:\tPegasusTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where Silence' terrorized our peace of mind, Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.br /&gt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).br /&gt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains (lizard-logic). Why did</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Within the realm of Science Fiction, two particular themes consistently elicit interest, were initially explored in the literature of a pre-cinematic era, and have since been periodically revisited by filmmakers and writers alike, with varying degrees of success. The first theme, that of time travel, has held an unwavering fascination for fans of film, as well as the written word, most recently on the screen with yet another version of the H.G. Wells classic, The Time Machine.' The second theme, which also manages to hold audiences in thrall, is that of invisibility, which sparks the imagination with it's seemingly endless</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/5108b80520fd143e3b01355b64f8dbaa9f74545878c29b7dc535487b560be93e.5bc39104fccb1ee2915d4ff47bf59231f41d7da372b6e0f2e0a6d617d0115a9f\n",
      "Model config CTRLConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-ctrl\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"dff\": 8192,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"ctrl\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 32,\n",
      "  \"n_head\": 4,\n",
      "  \"n_layer\": 5,\n",
      "  \"n_positions\": 512,\n",
      "  \"pad_token_id\": 98,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 246534\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-ctrl ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/6396dd9d415009ddd4888d67b871335174453bdb7fcefb6a2e9b1801a9e53d40.9446203d3fdccf9052d8f3b374e389456bca56e99546a077c7793df9efd43b7c\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/2c3686335ac7f0cd2edee08ebdfe2f35e9c42fe7ca65ed6f895ef65fd3e92795.e4a0758785dd792eafcbabf01da7f1e7e83f088336f093f96c9fb57c0507a588\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/a589cbcf0a32229eb0d4b767924a4c3bb2fc2a4e7c97f2bdd88bbb9230e606a4.3766b2880f7f2c1cf6aa2e79dec8b09b08c625bfeb6ae8cfaeaf2f1a1f3ee79d\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/6c6a4acf45931d5ea7a048191eb41954b46318a5077dc5114453bfa75a153a99.d5e237d48d65f0df414165f0c7723f784ad889ccec4c2698d7fa63da6f573e32\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-ctrl/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/f507367b52c1a82b5b422e64f4504af8099f841c3770c95f8b28e32daa73b63a.40460b2aee84138e30303b60ad6202eedda3274521f27d3c4a40f76618939d64\n",
      "/home/dev/mambaforge/envs/dev/lib/python3.9/site-packages/transformers/models/ctrl/modeling_ctrl.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model_size)\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-ctrl were not used when initializing CTRLForSequenceClassification: ['h.3.layernorm1.bias', 'h.0.multi_head_attention.Wv.weight', 'h.3.multi_head_attention.Wv.bias', 'h.4.multi_head_attention.Wk.bias', 'h.0.ffn.2.weight', 'h.1.multi_head_attention.Wk.weight', 'h.4.ffn.0.weight', 'h.0.multi_head_attention.Wk.weight', 'h.2.multi_head_attention.Wv.bias', 'h.4.ffn.0.bias', 'h.1.layernorm2.bias', 'h.1.multi_head_attention.Wv.bias', 'h.3.layernorm2.bias', 'h.0.ffn.0.bias', 'h.0.multi_head_attention.Wk.bias', 'h.3.layernorm1.weight', 'h.3.layernorm2.weight', 'h.0.layernorm2.weight', 'h.2.multi_head_attention.Wk.bias', 'h.4.layernorm1.weight', 'h.2.ffn.0.bias', 'h.2.multi_head_attention.Wq.weight', 'h.4.layernorm2.bias', 'h.0.ffn.2.bias', 'h.4.multi_head_attention.dense.bias', 'h.3.multi_head_attention.Wv.weight', 'h.4.multi_head_attention.Wq.bias', 'h.1.multi_head_attention.Wv.weight', 'h.3.multi_head_attention.dense.weight', 'h.1.multi_head_attention.dense.bias', 'h.3.multi_head_attention.dense.bias', 'w.weight', 'h.1.multi_head_attention.Wq.bias', 'h.2.multi_head_attention.Wk.weight', 'h.3.ffn.0.bias', 'h.3.ffn.2.weight', 'lm_head.bias', 'layernorm.weight', 'h.2.layernorm1.weight', 'lm_head.weight', 'h.2.layernorm2.bias', 'h.1.layernorm1.weight', 'h.1.ffn.2.weight', 'h.0.layernorm1.weight', 'h.3.multi_head_attention.Wk.weight', 'h.4.multi_head_attention.Wq.weight', 'h.0.multi_head_attention.dense.bias', 'h.4.ffn.2.weight', 'h.3.multi_head_attention.Wq.bias', 'h.2.ffn.2.weight', 'h.4.multi_head_attention.Wv.bias', 'h.4.multi_head_attention.dense.weight', 'h.0.multi_head_attention.Wq.weight', 'h.4.layernorm2.weight', 'h.1.multi_head_attention.Wk.bias', 'h.1.ffn.0.weight', 'h.4.multi_head_attention.Wv.weight', 'h.0.multi_head_attention.dense.weight', 'h.1.multi_head_attention.Wq.weight', 'h.1.ffn.2.bias', 'h.0.layernorm1.bias', 'layernorm.bias', 'h.4.multi_head_attention.Wk.weight', 'h.2.ffn.0.weight', 'h.3.ffn.2.bias', 'h.3.ffn.0.weight', 'h.2.layernorm2.weight', 'h.0.layernorm2.bias', 'h.4.ffn.2.bias', 'h.2.multi_head_attention.Wv.weight', 'h.0.ffn.0.weight', 'h.2.multi_head_attention.dense.bias', 'h.2.multi_head_attention.dense.weight', 'h.1.layernorm1.bias', 'h.0.multi_head_attention.Wv.bias', 'h.1.layernorm2.weight', 'h.2.layernorm1.bias', 'h.0.multi_head_attention.Wq.bias', 'h.3.multi_head_attention.Wk.bias', 'h.1.ffn.0.bias', 'h.1.multi_head_attention.dense.weight', 'h.3.multi_head_attention.Wq.weight', 'h.2.ffn.2.bias', 'h.4.layernorm1.bias', 'h.2.multi_head_attention.Wq.bias']\n",
      "- This IS expected if you are initializing CTRLForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CTRLForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of CTRLForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-ctrl.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CTRLForSequenceClassification for predictions without further training.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "Adding <pad> to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tctrl\n",
      "tokenizer:\tCTRLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains (lizard-logic). Why did two kids, who knew better, stone to death a toddler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf 'em down, just don't be a pig and go for the extra-super-sized portion and fill up on too much grease (\"too much grease\" is a metaphor for the prose in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/sentencepiece.bpe.model from cache at /home/dev/.cache/huggingface/transformers/dbcb433aefd8b1a136d029fe2205a5c58a6336f8d3ba20e6c010f4d962174f5f.160b145acd37d2b3fd7c3694afcf4c805c2da5fd4ed4c9e4a23985e3c52ee452\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/84c442cc6020fc04ce266072af54b040f770850f629dd86c5951dbc23ac4c0dd.8fd2f10f70e05e6bf043e8a6947f6cdf9bb5dc937df6f9210a5c0ba8ee48e959\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"camembert-base\",\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/camembert-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/7e23f45751ad1fed420ca9f03bb37a279dc98a56c75bf25e671129237e2c893c.ee4d4253e08a7cf9697c0671fd8f022483dbf586691a7b32ead55493a34d72b2\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where Silence' terrorized our peace of mind, Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You have to respect this movie. It may be \"just a dumb kid's movie\" but it's the #1 most frequently requested film title in online movie forums, requested by people who remember the story but can't remember the title. Therefore what follows is a much-needed, detailed plot description, since I haven't been able to find such a description anywhere else on the Internet.&lt;br /&gt;&lt;br /&gt;A typical 2-story</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-canine/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/e2d3a979053c6d984551e0adf9608d704f05998c5714cbde9f9cf77235212dd9.80427d5b9704102be5692d2db6d762f592a571601f7ec30d24087aa0564927f8\n",
      "Model config CanineConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-canine\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 57344,\n",
      "  \"downsampling_rate\": 4,\n",
      "  \"eos_token_id\": 57345,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 37,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_transformer_stride\": 128,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"canine\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hash_buckets\": 16384,\n",
      "  \"num_hash_functions\": 8,\n",
      "  \"num_hidden_layers\": 5,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"upsampling_kernel_size\": 4,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1114112\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-canine ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-canine/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-canine/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/7874bde67e82e1457ee0b9622c5c03ab41dee8dda2d7192cfbef8511ed8d4155.ab71f530366fe02e2834427e7b90198bfd0d573bc4279bfafdb2b95fe2b46dde\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-canine/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/5e81923ebcc227b7d1081886dfde93c2b50118e9eb203e53eaecd3e90cb928b8.7ea4d1bb4eecbb67a9cb64fce7c6d3262759915c4f3dd93eef8d89df46d6656d\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-canine/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/fb20848389b4d48e0a36ad6c31696fdfbc845bab2eec24f2247012b39f09d25b.8b7e251b00c73cf485532772c17a28b99f8cce9aa29b4eea5d651d57dead7947\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-canine were not used when initializing CanineForSequenceClassification: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing CanineForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CanineForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of CanineForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-canine.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CanineForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcanine\n",
      "tokenizer:\tCanineTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"It's like hard to like describe just how like exciting it is like to make a relationship like drama like with all the like po</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/YituTech/conv-bert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/7651fc6ae3906f28c62923bc7c76b0436327540c1ebb62a60b454ec79e102dd1.2a398d65585c12446cf5e632a1839e1754dc16cbbf6b87ccf28ba24c8536394e\n",
      "Model config ConvBertConfig {\n",
      "  \"_name_or_path\": \"YituTech/conv-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"ConvBertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"convbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/YituTech/conv-bert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/7651fc6ae3906f28c62923bc7c76b0436327540c1ebb62a60b454ec79e102dd1.2a398d65585c12446cf5e632a1839e1754dc16cbbf6b87ccf28ba24c8536394e\n",
      "Model config ConvBertConfig {\n",
      "  \"_name_or_path\": \"YituTech/conv-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"ConvBertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"convbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/YituTech/conv-bert-base/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/75608c7373c277fa55de32e1bd71af40f547910ef3a49ed431d3a9fb9b4f5c8c.16ff552dabca3af1d1d07bc63a184047eb39f686be4a6738ba0167c6b1bb0b84\n",
      "loading file https://huggingface.co/YituTech/conv-bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/YituTech/conv-bert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/YituTech/conv-bert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/YituTech/conv-bert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/7651fc6ae3906f28c62923bc7c76b0436327540c1ebb62a60b454ec79e102dd1.2a398d65585c12446cf5e632a1839e1754dc16cbbf6b87ccf28ba24c8536394e\n",
      "Model config ConvBertConfig {\n",
      "  \"_name_or_path\": \"YituTech/conv-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"ConvBertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"convbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/YituTech/conv-bert-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/7651fc6ae3906f28c62923bc7c76b0436327540c1ebb62a60b454ec79e102dd1.2a398d65585c12446cf5e632a1839e1754dc16cbbf6b87ccf28ba24c8536394e\n",
      "Model config ConvBertConfig {\n",
      "  \"_name_or_path\": \"YituTech/conv-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"ConvBertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"convbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/YituTech/conv-bert-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/f71042767b7bb431c7632b9f245661cd34a5edaac1eaf25f3a9e78a73bb711b2.3ee89f2fd82df871ab2d6f643874ee269c534627432695a69f22271e9d077426\n",
      "All model checkpoint weights were used when initializing ConvBertForSequenceClassification.\n",
      "\n",
      "Some weights of ConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also argue against (! ) the death penalty far better than kevin spacey's the life of david gayle ( 2002 ). &lt; br / &gt; &lt; br / &gt; humans are machiavellian mammals, under</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you have to respect this movie. it may be \" just a dumb kid's movie \" but it's the # 1 most frequently requested film title in online movie forums, requested by people who remember the story but can't remember the title. therefore what follows is a much - needed, detailed plot description, since i haven't been able to find such a description anywhere else on the internet. &lt; br / &gt; &lt; br / &gt; a typical 2 - story house is shown in suburbia. 7 - year - old bridget narrates about suspecting something is going on since she and her 11 - year -</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d90b8881b292ab1d856f82690bfb74616d8feee6aef667f6749ac4095fdbd87d.26341b6108474ff148f1434f7c1a0533463b09f1cc722a5f4b0423afcc0d1af6\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-deberta\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 32,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pooler_size\": 32,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 5001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/9e9891bc9eff8fce1ef75d86e233fa84f917fee11401a568eb0b813ad3c2ed2e.ceb913d3853aed91e0e70fdddcd6b626583c4691e06f784d358bfb34cbc052f0\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/19b5b27ce2721786a27d85d07a84444ade35c288d81668671a8ffb0babc93822.fc6bdd4f1130eb74d9b44c5e17acac631c2cd787f5b17b8644cfdb6f47c6f9b4\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/42fa1a1118c2212599265fe2e58cca1727068a57280dc5a220ce8a3eb621bc90.9beb5f062a1f5851b0cf95a3a31ee4842c0b4942ebc12d19cb55b09ed6b97eb4\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/47d639acd1e018dd6064b0a033447947919fdd5d1f2cecf21e40b3ccaffd95e8.8775b0b36c5f216ff7141ca01c145da1366c7b354621dbe2241b6278d603ee0e\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/5189c37bf0f236f9ae75bcded9c685fecc171a03fa354cbb08f985e2bab19fd5.411b06e0569dd95f34d3b8c548681f9c643d3ea7db2bbec7580a008e8149a026\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-deberta/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/3ed17069f028c2048df93e0cd1daf359a64a2aa9b5419bea4d4c68825a9d45de.706b208acd938297b87921e7ed6e0ea12caade0f7c585498368a5ef60b7131c7\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-deberta were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-deberta and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where Silence' terrorized our peace of mind, Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I felt duty bound to watch the 1983 Timothy Dalton / Zelah Clarke adaptation of \"Jane Eyre,\" because I'd just written an article about the 2006 BBC \"Jane Eyre\" for TheScreamOnline.br /br /So, I approached watching this the way I'd approach doing homework.br /br /I was irritated at first. The l</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/541d892494a26636c1408e4a20cb0b256cece4f0e3963071c797ffff5dead2fc.2903e5d8304dfff195ba79d158504b0e2fdad0e7b95f162303517c2bc8b6193a\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-deberta-v2\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 37,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 5,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 32,\n",
      "  \"pos_att_type\": [\n",
      "    \"none\"\n",
      "  ],\n",
      "  \"position_biased_input\": true,\n",
      "  \"relative_attention\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"vocab_size\": 128001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-deberta-v2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/spm.model from cache at /home/dev/.cache/huggingface/transformers/d3a1698ff4c94cf60924ad8a43f22ae17f2e892d1a9c3efc217c10f8f937cfe9.f97515c3cb091c0baf4e9ee1964253062c621d2883289e472def9c11d475bd8e\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/added_tokens.json from cache at /home/dev/.cache/huggingface/transformers/5b39f326e25439bab07e207c58cbd684e7f7184db1446e509113a90b73d9585f.31e96e41adc634e4e1fc60fd4c62af3893566f8238e340201b943701fcc41497\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/8ef692e7d3e97f959567960103c0f683844947f3344492a71ab12171a7f8bf6f.f886166424e457f0fc75f92e81205faabe843b2dbbbef6b25f9d8ec69f64bc7d\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/2b77486a8dea095a56d0256f39cc0012cbb6ba171ad6ea700902bd2de49455e1.4166f4ad96d1bf1885f71f397ef231269211af37bfbcd26418a19cb13f95deaa\n",
      "Adding [MASK] to the vocabulary\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-deberta-v2/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/864d7c11934d2e3b5b7e068784e3551cca2afbaa27bb378356cd54983d357677.0501513890b27770ffbdfa6515cb2b9c42211a213e0b845b98756515952fdc01\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-deberta-v2 were not used when initializing DebertaV2ForSequenceClassification: ['encoder.layer.4.attention.self.query_proj.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.key_proj.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.query_proj.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.self.query_proj.weight', 'encoder.layer.0.attention.self.value_proj.weight', 'embeddings.position_ids', 'encoder.layer.2.attention.self.value_proj.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query_proj.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'cls.predictions.decoder.weight', 'encoder.layer.1.attention.self.key_proj.bias', 'encoder.layer.4.attention.self.key_proj.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.value_proj.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.key_proj.weight', 'encoder.layer.4.attention.self.value_proj.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query_proj.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query_proj.weight', 'encoder.layer.2.attention.self.value_proj.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.self.key_proj.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.self.query_proj.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.key_proj.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query_proj.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key_proj.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.value_proj.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.value_proj.bias', 'encoder.layer.2.output.LayerNorm.bias', 'qa_outputs.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.self.key_proj.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query_proj.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.value_proj.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.self.key_proj.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.value_proj.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.query_proj.weight', 'encoder.layer.1.output.dense.bias', 'qa_outputs.weight', 'cls.predictions.decoder.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value_proj.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'embeddings.word_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-deberta-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdeberta_v2\n",
      "tokenizer:\tDebertaV2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains (lizard-logic). Why did two kids, who knew better,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He's stocky, sweaty, slightly cross-eyed and restless. He stands in front of us and calls himself a pervert. He claims that we  the film viewers  perceive the screen as a toilet bowl, and are all secretly wishing for all the s**t to explode from the inside. He's unpredictable and scary. Well? Come on, you could have guessed by now: he's one of the leading philosophers of our age.&lt;br /&gt;&lt;br /&gt;Slavoj iek is both a narrator and a subject of Sophie Fiennes' extraordinary new film, A Pervert</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/825fc05fc9603996d64ce61190f3b2eadb28e7a2f4db70d2cb44c8b5457deab6.5cd95c0833b01050f80cc06f242975c6b324790c205343941ec863daed8f33c8\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-distilbert\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 32,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 37,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 4,\n",
      "  \"n_layers\": 5,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 1124\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-distilbert ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/cb7356c60bdb1cab21d94b9477e70ec540df332cc5e6e6bb48031b57eef4f0b5.c606291473543140252dbd13b15c0a043aff71a9e326df21137eb5be66f05d35\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/4dfb06510b8aabc27c695ec83c49c93d00d88ef1ea1058336193e98f9c51a83c.1676f8feaaeeb0fa9bf638daa980e785b6e7d5eae2962691b3ca89571e69434e\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/7efbae476db8a6c6fd323850039c973bfb28f8e2949ea6e5eb5b27118fe49a65.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/f9504268580ebd416c0acfae517ca7069fb0f8c29575e46da2fc669c5490fcea.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-distilbert/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/299e0fe6b464b753ce289d6c1fb9824a02df0f1470e41e22f869eb931ae37e50.5e3b0095ecec242fbedad51a189b0675b0a5b3bebc1bb3cb0f784debb35526e7\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-distilbert were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'qa_outputs.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'qa_outputs.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-distilbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and sadde</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i was at first disgusted with director sun - woo jang because i had felt that he cheated me. jang had the potential to create a strong, deeply emotional fi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/2565ece384f57db5fa4a119b223bd9125202ad317452aafc7d65808808aca2cc.90f5fb85fe0424979cbdaf9cfbbdd6961b3beb7fec9dd8beeb61e0850f25178b\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-electra\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 64,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 64,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 5120\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/2dfcd50bd9ce17da99dac8d72b04e96e7d21c47c79e6f101439a2d966f21157d.68a48e717528e12ec2e7adfa6bf6d28826aa788a839d25a03361966e97f4aebc\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/9a7484c23278ab95d7f82ed7a0a90fe2f309a74e0ac4579d6bfcfe51aa6d532c.4f97d4959694d14f04b59e78ba08725b7a9fb43321115cc242c4395a96f89af6\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/1a1dd252117227d357ba22bef0724029f1c6818c7d7a3235fdc9a882204171a2.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/e86a2f9097fc372b9e366b9766242dca181999e704d6042daa17510ac881ac2a.8a1082b0d212c86207294883fe8ee799dd3ed3060f6a4fbc47abcffc92887a8b\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-electra/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/0fdb7c0dda62d076c13fbe08dc2c062a991113a554be9293476697a3c14c912c.724194cc558a629764f640a488693126ad1be2cd7e567f631a35e6d56e15b193\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-electra were not used when initializing ElectraForSequenceClassification: ['generator_lm_head.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_lm_head.bias', 'generator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-electra and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also argue against (! ) the</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you have to respect this movie. it may be \" just a dumb kid's movie \" but it's the # 1 most frequently requested film title in online movie forums, requested by people who remember the story but can't remember the title. therefore what follows is a much - needed, detailed plot description, since i haven't been able to find such a description anywhere else on the internet. &lt; br / &gt; &lt; br / &gt; a typical 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/fnet-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/c5d03090bf0732b6b4184a02a7c5b75b2d422fd6adbbde4637c8167a592589e0.ffa423946401d97c1a1e1b769e1ecb93e4c7b16573df3eff9c641a7ed95133a8\n",
      "Model config FNetConfig {\n",
      "  \"_name_or_path\": \"google/fnet-base\",\n",
      "  \"actual_seq_length\": 512,\n",
      "  \"architectures\": [\n",
      "    \"FNetForPreTraining\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"fnet\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"tpu_short_seq_length\": 512,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_fft\": true,\n",
      "  \"use_latest\": false,\n",
      "  \"use_tpu_fourier_optimizations\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/fnet-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/google/fnet-base/resolve/main/spiece.model from cache at /home/dev/.cache/huggingface/transformers/a7dcc5f9d312fb571fbcdc57a0deb1b01c469fc268d77b52f59328da3c361345.8369e3dce319313b58980019b27a823776dc36c2870dff154e0e207e4e2da50e\n",
      "loading file https://huggingface.co/google/fnet-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/000083fddb46c8c0f78a7d30de170c4602ffdf9b135302f5a10a7942935c026a.ea39e895a4484b873e2d48be9b5f74b1b57b35a0b82baacc189ebc4ccb313f72\n",
      "loading file https://huggingface.co/google/fnet-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/fnet-base/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/f87f635dbe5f16cc29521ebe2f76f7f143db03ea08f0ab7b6cd5d648dea10c82.0b14ebfed591df99cabf37e503a2d455ad6e67f46730feb8ba9e5683772872b5\n",
      "loading file https://huggingface.co/google/fnet-base/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/aac73434b1cdf2424612a660ef46959156d836ebd2c7bf02d825ed3612685ec7.52d612a11d26eb5bff8c69c4c6185c5209719e64fb9c22eb67d1c1636af3c876\n",
      "loading weights file https://huggingface.co/google/fnet-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/3d7ebfb0934ef387288a1526629247de26c8fe7cd70bb19c9146e892585d9e05.ba4c75350e7035c1ea911abf3988a7d8f9633cfcb768122fc6dcd343695dea58\n",
      "Some weights of the model checkpoint at google/fnet-base were not used when initializing FNetForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing FNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FNetForSequenceClassification were not initialized from the model checkpoint at google/fnet-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tfnet\n",
      "tokenizer:\tFNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris Rock deserves better than he gives himself in \"Down To Earth.\" As directed by brothers Chris &amp; Paul Weitz of \"American Pie\" fame, this uninspired remake of Warren Beatty's 1978 fantasy \"Heaven Can Wait,\" itself a rehash of 1941's \"Here Comes Mr. Jordan,\" lacks the abrasively profane humor that won Chris Rock an Emmy for his first HBO special. Predictably, he spouts swear words from A to Z, but he consciously avoids the F-word. Anybody</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/6b234456e0e642cf79fd4c5c6866c64a0932e5c094abe1407d1392bf2dfd4ae2.2a89755cf12744339c4a53d2d98902798870556733fd2dbd3256c22137c2be23\n",
      "Model config FlaubertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-flaubert\",\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_index\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"causal\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"emb_dim\": 32,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"gelu_activation\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_encoder\": true,\n",
      "  \"lang_id\": 0,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_index\": 5,\n",
      "  \"mask_token_id\": 0,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"flaubert\",\n",
      "  \"n_heads\": 4,\n",
      "  \"n_langs\": 2,\n",
      "  \"n_layers\": 5,\n",
      "  \"n_special\": 0,\n",
      "  \"pad_index\": 2,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pre_norm\": false,\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"unk_index\": 3,\n",
      "  \"use_lang_emb\": true,\n",
      "  \"use_proj\": null,\n",
      "  \"vocab_size\": 68729\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-flaubert ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/00fef64f0042385e6d4b33e425c81af4a7e9074ace835cf7fb6c0b6a43a13e7d.96f350d77dfb312f78c88393edcf97dfe20d317cd2b697f959ab0547af3b91c5\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/fb50bf1c835d440510c62e606156397af0aa74b3796504499d2040f2072abfe7.9a950d10c5797f9bd7ddace781fb7c0139b42c9df4c86b319b0bdcd7f701488e\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/aa041369a61e4ecb08169cf15dd3c63a06f3b53125ee14d9b2b4d13a6d4c2bfb.ca56ffb79f370a3d33e0dd7ed83b1de4a5e5187c8dcb3d8b8b4a945738fb7819\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/12dc641e11f9f0bd9e6ed4a0303175b9d1585a55fa1b78bdc8c00d0e6f6c5dea.236efd7e96072a5d82d41d12e1413c30666b58d17bd4ee535fca8a3486219a06\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/76811ef2b34edd662d7daa9e033121b58cac38df69bf940ae3f36a36e0ccf57b.09c231adbb17ab8bc85b3553e60c6dbfea95bd0fb6f3b70ca2466394aad8a550\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-flaubert were not used when initializing FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'attentions.2.out_lin.weight', 'ffns.2.lin2.weight', 'layer_norm2.1.bias', 'attentions.1.k_lin.bias', 'attentions.0.q_lin.bias', 'layer_norm2.3.weight', 'ffns.3.lin1.weight', 'attentions.2.q_lin.weight', 'layer_norm1.0.bias', 'layer_norm2.3.bias', 'attentions.2.out_lin.bias', 'attentions.1.out_lin.bias', 'attentions.4.k_lin.weight', 'attentions.1.q_lin.bias', 'ffns.1.lin2.weight', 'layer_norm1.2.bias', 'attentions.2.v_lin.weight', 'layer_norm1.2.weight', 'attentions.1.out_lin.weight', 'ffns.3.lin1.bias', 'attentions.3.q_lin.bias', 'layer_norm2.2.weight', 'attentions.4.v_lin.bias', 'layer_norm2.2.bias', 'attentions.2.q_lin.bias', 'ffns.1.lin2.bias', 'ffns.4.lin2.bias', 'logits_proj.bias', 'attentions.3.k_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.k_lin.bias', 'attentions.0.q_lin.weight', 'attentions.0.out_lin.bias', 'layer_norm1.1.bias', 'layer_norm1.1.weight', 'ffns.4.lin1.bias', 'layer_norm2.1.weight', 'embeddings.weight', 'attentions.1.k_lin.weight', 'attentions.3.q_lin.weight', 'classifier.weight', 'attentions.0.k_lin.bias', 'attentions.4.q_lin.weight', 'layer_norm_emb.bias', 'layer_norm1.4.weight', 'position_embeddings.weight', 'attentions.0.out_lin.weight', 'layer_norm_emb.weight', 'ffns.2.lin2.bias', 'layer_norm2.0.bias', 'ffns.0.lin2.weight', 'attentions.0.v_lin.bias', 'layer_norm1.3.bias', 'ffns.4.lin1.weight', 'ffns.0.lin2.bias', 'ffns.2.lin1.bias', 'qa_outputs.bias', 'ffns.3.lin2.weight', 'attentions.2.k_lin.weight', 'attentions.4.out_lin.weight', 'ffns.0.lin1.bias', 'attentions.3.v_lin.weight', 'ffns.1.lin1.weight', 'attentions.3.k_lin.bias', 'attentions.4.k_lin.bias', 'logits_proj.weight', 'layer_norm1.3.weight', 'ffns.2.lin1.weight', 'layer_norm1.4.bias', 'ffns.1.lin1.bias', 'layer_norm2.0.weight', 'attentions.3.out_lin.weight', 'attentions.0.k_lin.weight', 'qa_outputs.weight', 'layer_norm2.4.bias', 'classifier.bias', 'attentions.1.q_lin.weight', 'attentions.3.v_lin.bias', 'attentions.4.q_lin.bias', 'layer_norm1.0.weight', 'pred_layer.proj.weight', 'attentions.4.out_lin.bias', 'ffns.4.lin2.weight', 'attentions.1.v_lin.weight', 'attentions.4.v_lin.weight', 'attentions.0.v_lin.weight', 'attentions.1.v_lin.bias', 'lang_embeddings.weight', 'ffns.0.lin1.weight', 'layer_norm2.4.weight', 'attentions.3.out_lin.bias', 'ffns.3.lin2.bias']\n",
      "- This IS expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of FlaubertForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-flaubert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use FlaubertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX ( 1995 ) is the developing world' s answer to Silence of the Lambs. Where'Silence'terrorized our peace of mind,'Citizen'exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; CitizenX may also argue against (! ) the death penalty far better than Kevin Spacey' s</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was at first disgusted with director Sun-Woo Jang because I had felt that he cheated me. Jang had the potential to create a strong, deeply emotional film about sex and its effects on people, but instead chose to focus his strength on the pornography element more than the actual human element. I couldn' t see the characters at first and his sloppy introduction which blended both realism and cinema together was amateurish at best yet this film remained</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/fdf233c855c919615eea4564486f2b5003177a93cb9b11ac383c3f48b4065965.893bc8a50891863b59f64a57083a2e8cb301323a754564a40f3861bab0145211\n",
      "Model config FunnelConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-funnel\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_type\": \"relative_shift\",\n",
      "  \"block_repeats\": [\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"block_sizes\": [\n",
      "    1,\n",
      "    1,\n",
      "    2\n",
      "  ],\n",
      "  \"d_head\": 8,\n",
      "  \"d_inner\": 37,\n",
      "  \"d_model\": 32,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"initializer_range\": 0.1,\n",
      "  \"initializer_std\": null,\n",
      "  \"layer_norm_eps\": 1e-09,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"funnel\",\n",
      "  \"n_head\": 4,\n",
      "  \"num_decoder_layers\": 1,\n",
      "  \"pool_q_only\": true,\n",
      "  \"pooling_type\": \"mean\",\n",
      "  \"separate_cls\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"truncate_seq\": true,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 1126\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-funnel ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/247d9cc22d44243b642d59d7d75db2416e5ef2a2d6588e6b7c599ce85e66aed8.1fdd55620a8938abd0aae89c089853d3d9339a94f0c82b8c135b3e8aaf47d8ef\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/19a27fbdf5c15ac380d0bd5695ff6f84887d1451d0e6b17fc77bbaa60686de1f.405a128004d59f90f85ac4fb1cb1390c419016a8d20fa210c302d86478bfff60\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/3fcafef5b0323d0ede2b3876d5e61ab4bccaaa2fe2d3d4dd62bd15562cb8dd3f.34a22f495fc6b4fddbf5d6b2c62637ae42a7204b6355bbd999c44fee4001336d\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/b564a808b499e0ce675267293041ae788e95b94aaa9e1388f5bf51a3077915d3.68b33460661784da881678dde917a21599db8eb9337a385fb8164cda105b5695\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-funnel/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/b1b200fbab5abc8397ca1af245bab6a3434c90101ae4a0d37583e554c28afe0f.713ae079b03ed959bb8ad1bb8aad3a73eb6867e27ab038c20b444c501280db70\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-funnel were not used when initializing FunnelForSequenceClassification: ['funnel.decoder.layers.0.ffn.layer_norm.weight', 'funnel.decoder.layers.0.attention.k_head.bias', 'funnel.decoder.layers.0.attention.v_head.weight', 'funnel.decoder.layers.0.attention.seg_embed', 'discriminator_predictions.dense.weight', 'funnel.decoder.layers.0.attention.q_head.weight', 'funnel.decoder.layers.0.attention.r_w_bias', 'funnel.decoder.layers.0.ffn.linear_1.bias', 'funnel.decoder.layers.0.attention.layer_norm.bias', 'funnel.decoder.layers.0.attention.v_head.bias', 'funnel.decoder.layers.0.ffn.linear_1.weight', 'qa_outputs.weight', 'funnel.decoder.layers.0.ffn.linear_2.weight', 'lm_head.bias', 'funnel.decoder.layers.0.attention.layer_norm.weight', 'classifier.bias', 'qa_outputs.bias', 'lm_head.weight', 'discriminator_predictions.dense_prediction.bias', 'funnel.decoder.layers.0.ffn.layer_norm.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'funnel.decoder.layers.0.attention.r_r_bias', 'funnel.decoder.layers.0.attention.r_s_bias', 'funnel.decoder.layers.0.attention.k_head.weight', 'funnel.decoder.layers.0.attention.r_kernel', 'funnel.decoder.layers.0.attention.post_proj.weight', 'funnel.decoder.layers.0.ffn.linear_2.bias', 'classifier.weight', 'funnel.decoder.layers.0.attention.post_proj.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of FunnelForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-funnel.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use FunnelForSequenceClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and sadde</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don't take a long time to develop ; basically lean and mean - - he s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/e54a39799ae101f61741e1df6b7eeba26bcd03bea449be2d459c4e8a2966829b.7561c647f82885e386e46900764dd6b21825061b5c56c3614ecd874113243004\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 98,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 98,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 37,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 32,\n",
      "  \"n_head\": 4,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 5,\n",
      "  \"n_positions\": 512,\n",
      "  \"pad_token_id\": 98,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/b8b6c0058f45cbbd8912517eec3805150dc3c372b3bcb65e71e453e433d51b2b.80bde3fb870e0b1632f2c438dd0e0577f17c033d37c700f7395952539ca37c60\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/8eb9d0d2d37f1583d9d9a21417f73ca4f6bd25be0482d68ab9a3ebcf1c33ed45.94b5692cbb69a8f1a0c0f4b8fa214fbd2ae58380e61dbcb8717243807531390b\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/3281c60ddf902d7c6513f4fb220a1a0f41279287801fbb60f364545cb130832c.a326a1f04fc818beeef2fc3d92c43c42db411fe2d07c53a3cb65430eff1a7c4f\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/e03af8b412f2c7449f3f2d4c8c7cd50ac668d1deaec26de6db3258e5617faa98.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/91930ccd3f8a36288a8ff2c7254b3083b8eec2f7a5e3871ec0d0e781c0e72716.1ed40b8bbfcaa6c8f0a9eb550bd5dbaea6cd508f07d9966eb2d08cb87997d132\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/455bef84136c5b8309ad80f32f5bf9932cb5339f591915a5355780f9e87d5f08.3aa634b53cb6430d861be5483e3e2dc5977659bfa5cb8f254352fc996f23efce\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-gpt2 were not used when initializing GPT2ForSequenceClassification: ['h.0.ln_2.bias', 'h.0.attn.masked_bias', 'h.1.mlp.c_fc.weight', 'h.2.mlp.c_fc.weight', 'h.4.ln_1.bias', 'h.3.ln_2.bias', 'h.0.ln_1.weight', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.weight', 'wpe.weight', 'h.3.mlp.c_proj.bias', 'h.4.mlp.c_fc.bias', 'h.4.attn.c_attn.weight', 'h.3.attn.masked_bias', 'h.4.mlp.c_proj.weight', 'h.0.attn.c_attn.bias', 'h.0.mlp.c_fc.weight', 'h.4.attn.c_proj.weight', 'h.3.mlp.c_fc.weight', 'h.1.attn.bias', 'h.2.ln_1.bias', 'h.2.mlp.c_fc.bias', 'h.4.attn.c_proj.bias', 'h.2.attn.bias', 'h.3.attn.bias', 'h.4.mlp.c_fc.weight', 'h.2.attn.masked_bias', 'h.1.mlp.c_fc.bias', 'h.4.attn.masked_bias', 'h.4.ln_2.bias', 'wte.weight', 'h.1.ln_1.weight', 'h.1.attn.masked_bias', 'lm_head.weight', 'h.3.attn.c_attn.bias', 'ln_f.bias', 'h.3.mlp.c_proj.weight', 'h.0.ln_1.bias', 'h.1.attn.c_proj.weight', 'classifier.weight', 'h.4.ln_1.weight', 'h.0.mlp.c_proj.bias', 'h.3.ln_1.bias', 'h.1.ln_2.bias', 'h.2.mlp.c_proj.bias', 'h.4.ln_2.weight', 'h.2.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.1.ln_1.bias', 'h.0.mlp.c_fc.bias', 'h.1.ln_2.weight', 'h.2.ln_1.weight', 'h.1.mlp.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.3.mlp.c_fc.bias', 'h.3.ln_1.weight', 'h.3.attn.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.0.attn.c_attn.weight', 'h.4.attn.c_attn.bias', 'h.0.attn.bias', 'h.1.mlp.c_proj.weight', 'h.3.attn.c_attn.weight', 'ln_f.weight', 'classifier.bias', 'h.0.ln_2.weight', 'h.2.attn.c_attn.weight', 'h.2.ln_2.bias', 'h.3.ln_2.weight', 'h.2.attn.c_attn.bias', 'h.4.mlp.c_proj.bias', 'h.4.attn.bias', 'h.2.ln_2.weight', 'h.3.attn.c_proj.bias', 'h.0.attn.c_proj.bias', 'h.1.attn.c_proj.bias']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of GPT2ForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2ForSequenceClassification for predictions without further training.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1214 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It all starts with a suicide. Or is it a car crash? I guess it all depends on whether you choose to start at the beginning or the end. Director Gabriele Muccino gives you the ability to enter his new film Seven Pounds whichever way you prefer as he starts at the end and works his way back to the beginning, showing us the course of ev</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/1acd57a0409bc29eae1668be6294f24499c3f10a6f87aff688dd924614543989.502e1005452c9e71c3cddf2e4d6156cda93856c20a73cd64ecafa4d78bf5f9a2\n",
      "Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"anton-l/gpt-j-tiny-random\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 512,\n",
      "  \"n_head\": 4,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 8,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== anton-l/gpt-j-tiny-random ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/d6a58e5957db481459fe5f4c20cb6f5cc7b904bb937f9fcf06d7c9e8201d076c.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb\n",
      "loading file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/2f8a468408a0914873ad860a73e7987e44a08044672710e460c5691e44ac230b.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/41bf7e0b2b68da898e4f29af68b1b0e26092c08119f0812a057845336f67f779.3d0a96d28f7abb81fd01697a611f67edbcc1a0ab0522c492c42e9048a927b40f\n",
      "loading file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/added_tokens.json from cache at /home/dev/.cache/huggingface/transformers/3e432440a0043910e1123d2edbcf9311fd47047e9d5db7c71cd7e96015164a77.6d72550d19d27875206f40595cdafde5703113a27789a28ade2f41617361698a\n",
      "loading file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/3b449134f7ceb63d74c70b692727c7a2ff825361d20890fe9f8822def9b73108.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/8b76f0d90efe5c6da01b2595d9b92699776cc0c767a8a1f992be437b1988dcbf.f917b4b62760d7f3b767b8f7343bce0ffb87eddc03b0b6dfb33e8b31dc2118ed\n",
      "loading weights file https://huggingface.co/anton-l/gpt-j-tiny-random/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/eee0f89253c5a0c155ef205bf8e9b3cd591c8ec6453237a39f59a3339af36303.e739f28a0510cbfe540b3f4712e5098a2e4089d56479b77e09e390b00e533fa6\n",
      "Some weights of the model checkpoint at anton-l/gpt-j-tiny-random were not used when initializing GPTJForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTJForSequenceClassification were not initialized from the model checkpoint at anton-l/gpt-j-tiny-random and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgptj\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1353 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains (</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It all starts with a suicide. Or is it a car crash? I guess it all depends on whether you choose to start at the beginning or the end. Director Gabriele Muccino gives you the ability to enter his new film Seven Pounds whichever way you prefer as he starts at the end and works his way back to the beginning, showing us the course of events that led us to that heartbreaking 911 call. This is one powerful movie; maybe that is because I'm a softy when it comes to dramas of this ilk, dripping with weighty moments and chock full of devastating performances, but either way, a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/bf961325156a76e2f32c49873244bc41a042a0ccc0129055d47bca8bb62ecf5f.4fcbfa8ab40a34d125f9f15f7c3610ecddc69cd707c9d26718bfa7efc52378fa\n",
      "Model config GPTNeoConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-gpt_neo\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      2\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 98,\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 98,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 98,\n",
      "  \"resid_dropout\": 0.0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-gpt_neo ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/88e6e05b0878e807f336c523b262f4f57df6502ba03d954f3ee06b965e07802f.80bde3fb870e0b1632f2c438dd0e0577f17c033d37c700f7395952539ca37c60\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/694d878cb33288a2555dbfe9e5f291d43e654d2f8ce86a7511518df04a7e27a8.94b5692cbb69a8f1a0c0f4b8fa214fbd2ae58380e61dbcb8717243807531390b\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/3fed0181f37696a24107fbaf750cb9ad524e5dfdbadaafa6ef3999aab5322a12.a326a1f04fc818beeef2fc3d92c43c42db411fe2d07c53a3cb65430eff1a7c4f\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/29eb843e2bdf6956cd9dea903baaf57888d1eac05c0df97e3ed9a4519898d393.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/36b99de75ec69bb202b29057f51c7923c80f2d7d92641a4bdfff458a1069aaa7.adb1867628e298d10e32c539170e892391a56a8d5187c214bb7523c1d0647623\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/9de9a085718f6acd7e33a412aca8a68f271ec74bdb0782ac01a77bcebe19ba74.90054aa98ce9ae961bca57066078c016b9422d4d8d81f105f4207f8036f183ee\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-gpt_neo were not used when initializing GPTNeoForSequenceClassification: ['h.1.attn.attention.masked_bias', 'h.3.ln_1.bias', 'h.2.attn.attention.q_proj.weight', 'h.3.mlp.c_fc.bias', 'h.3.attn.attention.q_proj.weight', 'h.0.ln_2.bias', 'h.2.attn.attention.masked_bias', 'h.3.ln_1.weight', 'h.0.mlp.c_fc.weight', 'h.1.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.2.attn.attention.out_proj.bias', 'h.2.mlp.c_fc.bias', 'h.2.ln_1.bias', 'h.3.attn.attention.v_proj.weight', 'h.3.attn.attention.out_proj.weight', 'h.1.mlp.c_fc.weight', 'h.2.attn.attention.k_proj.weight', 'h.2.mlp.c_proj.bias', 'h.2.mlp.c_fc.weight', 'h.3.ln_2.bias', 'h.0.attn.attention.v_proj.weight', 'h.1.attn.attention.out_proj.weight', 'h.2.attn.attention.v_proj.weight', 'h.0.ln_1.weight', 'h.0.mlp.c_proj.weight', 'h.1.attn.attention.v_proj.weight', 'h.3.attn.attention.k_proj.weight', 'h.0.attn.attention.bias', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_fc.bias', 'h.2.attn.attention.out_proj.weight', 'wpe.weight', 'h.3.mlp.c_proj.bias', 'ln_f.weight', 'h.2.mlp.c_proj.weight', 'h.0.ln_2.weight', 'h.2.attn.attention.bias', 'h.2.ln_2.bias', 'wte.weight', 'h.1.ln_1.weight', 'lm_head.weight', 'h.3.ln_2.weight', 'h.1.attn.attention.q_proj.weight', 'h.0.attn.attention.masked_bias', 'h.1.ln_1.bias', 'h.0.attn.attention.k_proj.weight', 'h.3.attn.attention.out_proj.bias', 'ln_f.bias', 'h.0.attn.attention.out_proj.bias', 'h.0.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.0.attn.attention.q_proj.weight', 'h.1.ln_2.weight', 'h.0.ln_1.bias', 'h.2.ln_1.weight', 'h.0.mlp.c_proj.bias', 'h.1.mlp.c_proj.bias', 'h.2.ln_2.weight', 'h.0.attn.attention.out_proj.weight', 'h.1.attn.attention.k_proj.weight', 'h.3.attn.attention.masked_bias', 'h.1.attn.attention.out_proj.bias']\n",
      "- This IS expected if you are initializing GPTNeoForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at hf-internal-testing/tiny-random-gpt_neo and are newly initialized: ['transformer.h.1.attn.attention.bias', 'transformer.h.3.attn.attention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2503 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt_neo\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You have to respect this movie. It may be \"just a dumb kid's movie\" but it's the #1 most frequently requested film title in online movie forums, requested by people who remember the story but can't remember the title. Therefore what follows is a much-needed, detailed plot description, since I haven't been able to find such</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/cfb510f67e8b7caa315edb63cf273dcabea566cc7c79256c9279b9aabfabc1e2.6e328a8b48a360bcdc4fa4628970901425656415d87641bc286c517e3f274c05\n",
      "Model config IBertConfig {\n",
      "  \"_name_or_path\": \"kssteven/ibert-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"IBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_dequant\": \"none\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"ibert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"quant_mode\": false,\n",
      "  \"tokenizer_class\": \"RobertaTokenizer\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/673a127a1efd88da2f306da00117064b72a5cc86bcca7be220d81c9c74369858.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "loading file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/5422ece434498216e797c6ef1ecef875e497cb88774ffb57341843b89567fa70.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/c2fbe5b3fb8f721fc3f4d0e8e2b8d2ed77db69bef5aaf6779bbee917930b1c95.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
      "loading file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/45e95c68c39ff43eaa320e2634280dc9be9fbeac498575241b162472b280a60f.e7fcf26aa8cb28b14292f4ba90abfc66937ea92782061f8552cbd61edc0f7c0a\n",
      "loading configuration file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/cfb510f67e8b7caa315edb63cf273dcabea566cc7c79256c9279b9aabfabc1e2.6e328a8b48a360bcdc4fa4628970901425656415d87641bc286c517e3f274c05\n",
      "Model config IBertConfig {\n",
      "  \"_name_or_path\": \"kssteven/ibert-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"IBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_dequant\": \"none\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"ibert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"quant_mode\": false,\n",
      "  \"tokenizer_class\": \"RobertaTokenizer\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kssteven/ibert-roberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/27deb62eabe44acce2863c490a8dcbeb98053eeb8a0275c10326865020aaac43.c555a2e298657aa872e144ac44cbd115f4825d74d48414d00781f2fd00cbb3cd\n",
      "Some weights of the model checkpoint at kssteven/ibert-roberta-base were not used when initializing IBertForSequenceClassification: ['ibert.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'ibert.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing IBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing IBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of IBertForSequenceClassification were not initialized from the model checkpoint at kssteven/ibert-roberta-base and are newly initialized: ['ibert.encoder.layer.1.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.self.query.weight_integer', 'ibert.encoder.layer.7.output.output_activation.x_min', 'ibert.encoder.layer.1.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.7.attention.self.output_activation.x_min', 'ibert.encoder.layer.9.attention.self.key_activation.x_max', 'ibert.encoder.layer.5.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.2.pre_intermediate_act.x_max', 'ibert.encoder.layer.4.attention.self.key_activation.x_max', 'ibert.encoder.layer.4.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.10.attention.self.query.weight_integer', 'ibert.encoder.layer.0.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.self.softmax.act.x_max', 'ibert.encoder.layer.4.attention.self.value.weight_integer', 'ibert.encoder.layer.8.output.dense.weight_integer', 'ibert.embeddings.position_embeddings.weight_scaling_factor', 'ibert.encoder.layer.10.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.8.pre_intermediate_act.x_min', 'ibert.encoder.layer.11.pre_intermediate_act.x_max', 'ibert.encoder.layer.7.intermediate.dense.weight_integer', 'ibert.encoder.layer.7.output.ln_input_act.x_max', 'ibert.encoder.layer.7.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.query.bias_integer', 'ibert.encoder.layer.6.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.key.weight_integer', 'ibert.encoder.layer.4.attention.self.output_activation.x_min', 'ibert.encoder.layer.9.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.2.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.key_activation.x_max', 'ibert.encoder.layer.1.pre_intermediate_act.x_max', 'ibert.encoder.layer.2.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.4.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.LayerNorm.activation.x_max', 'ibert.embeddings.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.output.dense.bias_integer', 'ibert.encoder.layer.6.intermediate.dense.weight_integer', 'ibert.encoder.layer.6.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.5.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.LayerNorm.shift', 'ibert.encoder.layer.7.attention.self.value.bias_integer', 'ibert.encoder.layer.5.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.value_activation.x_min', 'ibert.encoder.layer.7.attention.self.value_activation.x_min', 'ibert.embeddings.embeddings_act2.x_max', 'ibert.encoder.layer.3.attention.self.query_activation.x_max', 'ibert.encoder.layer.5.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.7.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.10.attention.self.softmax.act.act_scaling_factor', 'ibert.embeddings.embeddings_act2.x_min', 'ibert.encoder.layer.6.attention.output.LayerNorm.shift', 'ibert.encoder.layer.0.output.output_activation.x_min', 'ibert.encoder.layer.1.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.output.output_activation.x_min', 'ibert.encoder.layer.4.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.6.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.attention.self.key_activation.x_max', 'ibert.encoder.layer.6.attention.self.key.bias_integer', 'ibert.encoder.layer.9.attention.output.LayerNorm.shift', 'ibert.encoder.layer.4.output.dense.fc_scaling_factor', 'ibert.encoder.layer.3.pre_output_act.x_max', 'ibert.encoder.layer.3.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.query.weight_integer', 'ibert.encoder.layer.11.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.5.output.output_activation.x_max', 'ibert.encoder.layer.9.attention.self.query.weight_integer', 'ibert.encoder.layer.11.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.11.intermediate.output_activation.x_min', 'ibert.encoder.layer.0.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.9.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.value.bias_integer', 'ibert.encoder.layer.9.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.value_activation.x_max', 'ibert.encoder.layer.3.output.ln_input_act.x_min', 'ibert.encoder.layer.3.attention.self.query.weight_integer', 'ibert.encoder.layer.9.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.7.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.3.attention.self.softmax.act.x_min', 'ibert.encoder.layer.1.pre_intermediate_act.x_min', 'ibert.encoder.layer.0.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.1.intermediate.dense.bias_integer', 'ibert.encoder.layer.1.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.output.dense.fc_scaling_factor', 'ibert.encoder.layer.1.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.2.attention.self.value_activation.x_min', 'ibert.encoder.layer.10.attention.self.query_activation.x_max', 'ibert.encoder.layer.5.attention.output.output_activation.x_min', 'ibert.encoder.layer.1.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.output_activation.x_min', 'ibert.encoder.layer.8.intermediate.dense.weight_integer', 'ibert.encoder.layer.6.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.11.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.key.bias_integer', 'ibert.encoder.layer.2.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.2.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.query_activation.x_min', 'ibert.encoder.layer.2.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.self.query.bias_integer', 'ibert.encoder.layer.3.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.output_activation.x_max', 'ibert.encoder.layer.5.attention.self.value.weight_integer', 'ibert.encoder.layer.10.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.10.attention.self.key_activation.x_min', 'ibert.encoder.layer.6.attention.self.value_activation.x_max', 'ibert.encoder.layer.0.pre_intermediate_act.x_max', 'ibert.encoder.layer.2.attention.self.value.bias_integer', 'ibert.encoder.layer.4.pre_output_act.x_max', 'ibert.encoder.layer.1.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.output_activation.x_max', 'ibert.encoder.layer.10.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.8.output.output_activation.x_max', 'ibert.encoder.layer.2.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.dense.weight_integer', 'ibert.encoder.layer.9.attention.self.query_activation.x_min', 'ibert.encoder.layer.11.attention.self.value_activation.x_max', 'ibert.encoder.layer.6.output.ln_input_act.x_max', 'ibert.encoder.layer.2.output.ln_input_act.x_min', 'ibert.encoder.layer.11.output.LayerNorm.shift', 'ibert.embeddings.position_embeddings.weight_integer', 'ibert.encoder.layer.8.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.0.attention.self.key_activation.x_max', 'ibert.encoder.layer.0.intermediate.dense.bias_integer', 'ibert.encoder.layer.11.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.output.output_activation.x_min', 'ibert.encoder.layer.6.pre_intermediate_act.x_min', 'ibert.encoder.layer.10.output.output_activation.x_min', 'ibert.encoder.layer.9.attention.self.output_activation.x_min', 'ibert.encoder.layer.10.attention.self.value.weight_integer', 'ibert.encoder.layer.8.attention.self.value.bias_integer', 'ibert.encoder.layer.11.output.output_activation.x_min', 'ibert.encoder.layer.1.output.ln_input_act.x_min', 'ibert.encoder.layer.8.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.1.intermediate.output_activation.x_min', 'ibert.encoder.layer.3.attention.self.query.bias_integer', 'ibert.encoder.layer.8.output.LayerNorm.shift', 'ibert.encoder.layer.9.pre_output_act.x_max', 'ibert.encoder.layer.10.attention.output.LayerNorm.shift', 'ibert.encoder.layer.5.pre_intermediate_act.x_max', 'ibert.encoder.layer.2.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.10.pre_output_act.x_min', 'ibert.encoder.layer.9.intermediate.dense.bias_integer', 'ibert.encoder.layer.8.attention.self.key.weight_integer', 'ibert.encoder.layer.1.attention.self.query_activation.x_min', 'ibert.encoder.layer.4.attention.output.dense.weight_integer', 'ibert.encoder.layer.5.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.dense.bias_integer', 'ibert.encoder.layer.2.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.output.LayerNorm.activation.x_max', 'classifier.out_proj.weight', 'ibert.encoder.layer.5.attention.output.ln_input_act.x_max', 'ibert.embeddings.output_activation.x_min', 'ibert.encoder.layer.8.attention.self.output_activation.x_max', 'ibert.encoder.layer.4.output.ln_input_act.x_max', 'ibert.encoder.layer.1.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.11.attention.self.key_activation.x_min', 'ibert.encoder.layer.3.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.1.output.output_activation.x_max', 'ibert.encoder.layer.1.attention.self.query.weight_integer', 'ibert.encoder.layer.1.attention.self.output_activation.x_min', 'ibert.encoder.layer.1.pre_output_act.x_max', 'ibert.encoder.layer.3.output.dense.bias_integer', 'ibert.encoder.layer.9.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.self.key.bias_integer', 'ibert.encoder.layer.11.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.output_activation.x_min', 'ibert.embeddings.word_embeddings.weight_integer', 'ibert.encoder.layer.3.attention.output.output_activation.x_min', 'ibert.encoder.layer.1.attention.self.softmax.act.x_min', 'ibert.encoder.layer.2.attention.self.value.weight_integer', 'ibert.encoder.layer.4.attention.self.query_activation.x_max', 'ibert.embeddings.LayerNorm.activation.x_max', 'ibert.encoder.layer.5.attention.self.query_activation.x_max', 'ibert.encoder.layer.10.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.key.bias_integer', 'ibert.encoder.layer.7.attention.self.key.bias_integer', 'ibert.encoder.layer.5.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.4.attention.self.softmax.act.x_min', 'ibert.encoder.layer.3.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.7.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.0.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.5.output.dense.weight_integer', 'ibert.encoder.layer.10.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.4.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.4.pre_output_act.x_min', 'ibert.encoder.layer.11.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.key.weight_integer', 'ibert.encoder.layer.1.attention.output.output_activation.x_max', 'ibert.encoder.layer.3.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.8.attention.self.query_activation.x_min', 'ibert.encoder.layer.4.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.10.output.ln_input_act.x_min', 'ibert.encoder.layer.11.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.9.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.4.attention.output.LayerNorm.shift', 'ibert.encoder.layer.7.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.6.output.LayerNorm.shift', 'ibert.encoder.layer.1.attention.output.LayerNorm.shift', 'ibert.encoder.layer.9.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.1.intermediate.dense.weight_integer', 'ibert.encoder.layer.1.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.6.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.4.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.0.pre_intermediate_act.x_min', 'ibert.encoder.layer.9.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.query_activation.x_max', 'ibert.encoder.layer.9.intermediate.dense.weight_integer', 'ibert.encoder.layer.7.intermediate.output_activation.x_max', 'ibert.encoder.layer.5.attention.self.key.bias_integer', 'ibert.encoder.layer.11.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.9.intermediate.output_activation.x_min', 'ibert.encoder.layer.1.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.7.attention.output.output_activation.x_min', 'ibert.embeddings.word_embeddings.weight_scaling_factor', 'ibert.encoder.layer.11.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.value.weight_integer', 'ibert.encoder.layer.2.output.output_activation.x_max', 'ibert.encoder.layer.9.output.ln_input_act.x_max', 'ibert.encoder.layer.5.pre_output_act.x_max', 'ibert.encoder.layer.7.output.dense.bias_integer', 'ibert.encoder.layer.0.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.9.output.output_activation.x_max', 'ibert.encoder.layer.2.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.7.attention.self.key_activation.x_max', 'ibert.encoder.layer.6.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.10.attention.self.softmax.act.x_min', 'ibert.encoder.layer.8.pre_intermediate_act.x_max', 'ibert.encoder.layer.11.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.2.attention.output.dense.weight_integer', 'ibert.encoder.layer.2.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.dense.bias_integer', 'ibert.encoder.layer.8.pre_output_act.x_max', 'ibert.encoder.layer.5.intermediate.output_activation.x_max', 'ibert.encoder.layer.4.intermediate.output_activation.x_min', 'ibert.encoder.layer.10.output.dense.bias_integer', 'ibert.encoder.layer.3.intermediate.output_activation.x_max', 'ibert.encoder.layer.2.pre_intermediate_act.x_min', 'ibert.encoder.layer.5.attention.self.value_activation.x_min', 'ibert.encoder.layer.10.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.output.dense.weight_integer', 'ibert.encoder.layer.11.output.dense.bias_integer', 'ibert.encoder.layer.3.output.LayerNorm.shift', 'ibert.encoder.layer.9.output.dense.fc_scaling_factor', 'ibert.encoder.layer.9.output.dense.bias_integer', 'ibert.encoder.layer.4.intermediate.dense.weight_integer', 'ibert.encoder.layer.5.attention.self.output_activation.x_min', 'ibert.encoder.layer.6.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.value.bias_integer', 'ibert.encoder.layer.3.intermediate.output_activation.x_min', 'ibert.encoder.layer.11.attention.self.softmax.act.x_min', 'ibert.encoder.layer.9.attention.self.query.bias_integer', 'ibert.encoder.layer.4.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.1.output.dense.weight_integer', 'ibert.encoder.layer.1.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.value_activation.x_max', 'ibert.encoder.layer.11.intermediate.dense.bias_integer', 'ibert.encoder.layer.5.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.7.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.2.pre_output_act.x_max', 'ibert.encoder.layer.7.attention.self.value.weight_integer', 'ibert.encoder.layer.6.attention.output.dense.weight_integer', 'ibert.encoder.layer.5.output.dense.fc_scaling_factor', 'ibert.encoder.layer.0.intermediate.output_activation.x_min', 'ibert.encoder.layer.1.attention.self.key_activation.x_min', 'classifier.out_proj.bias', 'ibert.encoder.layer.0.intermediate.output_activation.x_max', 'ibert.embeddings.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.output_activation.x_min', 'ibert.encoder.layer.0.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.9.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.pre_output_act.x_max', 'ibert.encoder.layer.6.attention.self.softmax.act.x_min', 'ibert.encoder.layer.11.intermediate.output_activation.x_max', 'ibert.encoder.layer.0.output.LayerNorm.shift', 'ibert.encoder.layer.0.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.4.intermediate.dense.bias_integer', 'ibert.encoder.layer.6.attention.self.value_activation.x_min', 'ibert.encoder.layer.9.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.11.attention.self.value_activation.x_min', 'ibert.encoder.layer.5.output.ln_input_act.x_min', 'ibert.encoder.layer.5.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.11.output.output_activation.x_max', 'ibert.encoder.layer.2.attention.self.output_activation.x_max', 'ibert.encoder.layer.7.attention.self.query_activation.x_min', 'ibert.encoder.layer.9.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.0.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.7.output.output_activation.x_max', 'ibert.encoder.layer.9.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.3.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.query_activation.x_max', 'ibert.encoder.layer.1.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.2.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.4.output.dense.bias_integer', 'ibert.encoder.layer.6.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.LayerNorm.shift', 'ibert.encoder.layer.3.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.1.attention.self.key.weight_integer', 'ibert.encoder.layer.6.output.dense.weight_integer', 'ibert.encoder.layer.8.attention.self.output_activation.x_min', 'ibert.encoder.layer.3.attention.output.ln_input_act.act_scaling_factor', 'ibert.embeddings.embeddings_act1.x_min', 'ibert.encoder.layer.3.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.key_activation.x_min', 'ibert.encoder.layer.6.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.8.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.7.output.dense.weight_integer', 'ibert.encoder.layer.6.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.6.attention.self.key_activation.x_max', 'ibert.encoder.layer.6.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.1.attention.self.softmax.act.x_max', 'ibert.encoder.layer.6.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.1.attention.self.value_activation.x_min', 'ibert.encoder.layer.10.attention.self.key.bias_integer', 'ibert.encoder.layer.7.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.query.weight_integer', 'ibert.encoder.layer.7.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.value_activation.x_max', 'ibert.encoder.layer.0.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.softmax.act.x_max', 'ibert.encoder.layer.8.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.10.attention.output.output_activation.x_max', 'ibert.encoder.layer.8.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.output_activation.x_max', 'ibert.encoder.layer.2.attention.self.key.weight_integer', 'ibert.encoder.layer.3.output.dense.weight_integer', 'ibert.encoder.layer.0.attention.self.query_activation.x_min', 'ibert.encoder.layer.0.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.10.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.key.weight_integer', 'ibert.encoder.layer.8.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.output.output_activation.act_scaling_factor', 'ibert.embeddings.output_activation.x_max', 'ibert.encoder.layer.5.attention.output.dense.weight_integer', 'ibert.encoder.layer.10.attention.self.value.bias_integer', 'ibert.encoder.layer.8.attention.self.value.weight_integer', 'ibert.encoder.layer.11.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.value.bias_integer', 'ibert.encoder.layer.6.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.value_activation.x_min', 'ibert.encoder.layer.5.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.8.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.output_activation.x_min', 'ibert.encoder.layer.8.intermediate.output_activation.x_min', 'ibert.encoder.layer.9.attention.output.output_activation.x_max', 'ibert.encoder.layer.11.output.dense.fc_scaling_factor', 'ibert.encoder.layer.2.output.LayerNorm.shift', 'ibert.encoder.layer.3.attention.self.key_activation.x_min', 'ibert.encoder.layer.4.attention.self.output_activation.x_max', 'ibert.encoder.layer.6.attention.self.value.bias_integer', 'ibert.embeddings.embeddings_act1.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.dense.bias_integer', 'ibert.encoder.layer.5.attention.self.key_activation.x_min', 'ibert.encoder.layer.2.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.10.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.5.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.softmax.act.x_max', 'ibert.encoder.layer.9.pre_output_act.x_min', 'ibert.encoder.layer.10.attention.self.output_activation.x_max', 'ibert.encoder.layer.2.intermediate.output_activation.x_min', 'ibert.encoder.layer.5.attention.self.softmax.act.x_min', 'ibert.encoder.layer.5.intermediate.dense.bias_integer', 'ibert.encoder.layer.0.attention.self.value.bias_integer', 'ibert.encoder.layer.8.intermediate.output_activation.x_max', 'ibert.encoder.layer.7.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.8.intermediate.dense.bias_integer', 'ibert.encoder.layer.0.intermediate.dense.weight_integer', 'ibert.encoder.layer.0.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.5.intermediate.dense.weight_integer', 'ibert.encoder.layer.6.attention.self.key.weight_integer', 'ibert.encoder.layer.8.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.LayerNorm.shift', 'ibert.encoder.layer.7.output.LayerNorm.shift', 'ibert.encoder.layer.3.attention.self.output_activation.x_max', 'ibert.encoder.layer.1.attention.self.key.bias_integer', 'ibert.encoder.layer.8.output.output_activation.x_min', 'classifier.dense.weight', 'ibert.encoder.layer.11.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.1.output.dense.fc_scaling_factor', 'ibert.encoder.layer.4.output.ln_input_act.x_min', 'ibert.encoder.layer.9.attention.self.key.bias_integer', 'ibert.encoder.layer.9.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.2.output.dense.fc_scaling_factor', 'ibert.encoder.layer.7.output.ln_input_act.x_min', 'ibert.encoder.layer.1.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.8.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.4.output.LayerNorm.shift', 'ibert.encoder.layer.10.intermediate.output_activation.x_max', 'ibert.encoder.layer.8.attention.self.softmax.act.x_min', 'ibert.encoder.layer.9.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.query_activation.x_min', 'ibert.encoder.layer.2.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.11.output.ln_input_act.x_min', 'ibert.encoder.layer.10.attention.output.dense.bias_integer', 'ibert.encoder.layer.11.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.9.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.query.bias_integer', 'ibert.encoder.layer.4.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.7.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.1.output.ln_input_act.x_max', 'ibert.encoder.layer.0.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.4.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.self.key_activation.x_min', 'ibert.encoder.layer.0.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.4.pre_intermediate_act.x_min', 'ibert.encoder.layer.1.attention.output.output_activation.x_min', 'ibert.encoder.layer.7.attention.self.query_activation.x_max', 'ibert.encoder.layer.6.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.output.output_activation.x_max', 'ibert.encoder.layer.5.pre_intermediate_act.x_min', 'ibert.encoder.layer.0.output.dense.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.value.bias_integer', 'ibert.encoder.layer.9.attention.self.key.weight_integer', 'ibert.encoder.layer.1.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.3.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.8.pre_output_act.x_min', 'ibert.encoder.layer.10.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.10.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.output.output_activation.x_max', 'ibert.encoder.layer.5.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.output_activation.x_max', 'classifier.dense.bias', 'ibert.encoder.layer.4.attention.self.key_activation.x_min', 'ibert.encoder.layer.7.pre_output_act.x_min', 'ibert.encoder.layer.4.attention.self.value_activation.x_min', 'ibert.encoder.layer.4.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.query_activation.x_max', 'ibert.encoder.layer.5.attention.self.value_activation.x_max', 'ibert.encoder.layer.8.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.0.pre_output_act.x_max', 'ibert.encoder.layer.4.attention.self.key_activation.act_scaling_factor', 'ibert.embeddings.embeddings_act2.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.7.attention.self.key_activation.x_min', 'ibert.encoder.layer.10.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.11.attention.self.softmax.act.x_max', 'ibert.encoder.layer.4.attention.self.key.bias_integer', 'ibert.encoder.layer.2.attention.output.dense.bias_integer', 'ibert.encoder.layer.10.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.value.weight_integer', 'ibert.encoder.layer.0.attention.self.key_activation.x_min', 'ibert.encoder.layer.0.attention.self.softmax.act.x_min', 'ibert.encoder.layer.10.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.2.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.6.intermediate.output_activation.x_max', 'ibert.encoder.layer.4.attention.self.softmax.act.x_max', 'ibert.encoder.layer.4.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.11.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.7.attention.self.query.weight_integer', 'ibert.encoder.layer.7.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.8.output.ln_input_act.x_min', 'ibert.encoder.layer.2.attention.self.query_activation.x_max', 'ibert.embeddings.token_type_embeddings.weight_scaling_factor', 'ibert.encoder.layer.3.attention.self.output_activation.x_min', 'ibert.embeddings.embeddings_act1.x_max', 'ibert.encoder.layer.3.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.9.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.8.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.10.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.key_activation.x_max', 'ibert.encoder.layer.10.attention.self.key_activation.x_max', 'ibert.encoder.layer.7.pre_intermediate_act.x_max', 'ibert.encoder.layer.0.attention.self.key.weight_integer', 'ibert.embeddings.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.intermediate.dense.weight_integer', 'ibert.encoder.layer.4.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.query.weight_integer', 'ibert.encoder.layer.2.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.4.output.output_activation.x_min', 'ibert.encoder.layer.6.attention.self.value.weight_integer', 'ibert.encoder.layer.8.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.9.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.10.output.output_activation.x_max', 'ibert.encoder.layer.7.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.value_activation.x_max', 'ibert.encoder.layer.5.attention.self.key_activation.x_max', 'ibert.encoder.layer.5.output.LayerNorm.shift', 'ibert.encoder.layer.11.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.0.attention.self.output_activation.x_min', 'ibert.encoder.layer.9.attention.self.output_activation.x_max', 'ibert.encoder.layer.0.attention.self.query.weight_integer', 'ibert.encoder.layer.4.output.dense.weight_integer', 'ibert.encoder.layer.7.attention.self.query.bias_integer', 'ibert.encoder.layer.5.attention.output.LayerNorm.shift', 'ibert.encoder.layer.5.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.10.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.dense.bias_integer', 'ibert.encoder.layer.7.attention.self.value_activation.x_max', 'ibert.encoder.layer.6.output.dense.bias_integer', 'ibert.encoder.layer.6.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.2.intermediate.dense.bias_integer', 'ibert.encoder.layer.0.output.dense.bias_integer', 'ibert.encoder.layer.3.attention.self.query_activation.x_min', 'ibert.encoder.layer.4.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.5.output.ln_input_act.x_max', 'ibert.encoder.layer.3.attention.self.value.weight_integer', 'ibert.encoder.layer.3.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.output_activation.x_max', 'ibert.encoder.layer.1.attention.output.dense.bias_integer', 'ibert.encoder.layer.3.pre_intermediate_act.x_max', 'ibert.encoder.layer.5.attention.self.output_activation.x_max', 'ibert.encoder.layer.9.output.output_activation.x_min', 'ibert.encoder.layer.11.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.5.output.output_activation.x_min', 'ibert.encoder.layer.6.pre_intermediate_act.x_max', 'ibert.encoder.layer.8.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.dense.weight_integer', 'ibert.encoder.layer.2.attention.self.softmax.act.x_min', 'ibert.encoder.layer.7.intermediate.output_activation.x_min', 'ibert.encoder.layer.2.intermediate.dense.weight_integer', 'ibert.encoder.layer.11.attention.self.output_activation.x_max', 'ibert.encoder.layer.8.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.2.output.output_activation.x_min', 'ibert.encoder.layer.3.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.dense.weight_integer', 'ibert.encoder.layer.10.pre_intermediate_act.x_max', 'ibert.encoder.layer.9.output.ln_input_act.x_min', 'ibert.encoder.layer.1.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.output.output_activation.x_max', 'ibert.encoder.layer.6.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.5.intermediate.output_activation.x_min', 'ibert.encoder.layer.4.pre_intermediate_act.x_max', 'ibert.encoder.layer.8.attention.output.LayerNorm.shift', 'ibert.encoder.layer.3.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.output.dense.fc_scaling_factor', 'ibert.encoder.layer.9.pre_intermediate_act.x_min', 'ibert.encoder.layer.10.attention.self.softmax.act.x_max', 'ibert.encoder.layer.8.attention.self.key.bias_integer', 'ibert.encoder.layer.4.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.10.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.6.attention.output.output_activation.x_max', 'ibert.encoder.layer.3.attention.output.dense.weight_integer', 'ibert.encoder.layer.10.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.value.bias_integer', 'ibert.encoder.layer.1.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.softmax.act.x_max', 'ibert.encoder.layer.10.attention.self.key.weight_integer', 'ibert.encoder.layer.5.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.4.attention.self.value_activation.x_max', 'ibert.encoder.layer.9.attention.self.softmax.act.x_min', 'ibert.embeddings.LayerNorm.shift', 'ibert.encoder.layer.9.intermediate.output_activation.x_max', 'ibert.encoder.layer.10.attention.self.output_activation.x_min', 'ibert.encoder.layer.9.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.1.output.output_activation.x_min', 'ibert.encoder.layer.8.output.ln_input_act.x_max', 'ibert.encoder.layer.8.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.0.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.2.output.ln_input_act.x_max', 'ibert.encoder.layer.11.attention.output.dense.weight_integer', 'ibert.encoder.layer.8.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.11.pre_output_act.x_max', 'ibert.encoder.layer.2.attention.self.output_activation.x_min', 'ibert.encoder.layer.0.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.11.output.dense.weight_integer', 'ibert.encoder.layer.5.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.query_activation.x_max', 'ibert.encoder.layer.8.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.output_activation.x_max', 'ibert.encoder.layer.7.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.1.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.key.weight_integer', 'ibert.encoder.layer.3.attention.self.softmax.act.x_max', 'ibert.encoder.layer.6.attention.output.output_activation.x_min', 'ibert.encoder.layer.3.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.2.output.dense.bias_integer', 'ibert.encoder.layer.3.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.4.attention.output.output_activation.x_max', 'ibert.encoder.layer.10.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.9.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.4.attention.self.query.weight_integer', 'ibert.encoder.layer.8.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.1.attention.self.value_activation.x_max', 'ibert.encoder.layer.11.pre_intermediate_act.x_min', 'ibert.encoder.layer.11.attention.self.query_activation.x_max', 'ibert.encoder.layer.1.output.LayerNorm.shift', 'ibert.encoder.layer.2.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.4.attention.output.output_activation.x_min', 'ibert.encoder.layer.6.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.pre_intermediate_act.x_max', 'ibert.encoder.layer.1.pre_output_act.x_min', 'ibert.encoder.layer.10.pre_intermediate_act.x_min', 'ibert.encoder.layer.11.intermediate.dense.weight_integer', 'ibert.encoder.layer.0.attention.output.output_activation.x_min', 'ibert.embeddings.token_type_embeddings.weight_integer', 'ibert.encoder.layer.6.output.ln_input_act.x_min', 'ibert.encoder.layer.11.output.ln_input_act.x_max', 'ibert.encoder.layer.2.attention.output.LayerNorm.shift', 'ibert.encoder.layer.5.attention.output.dense.bias_integer', 'ibert.encoder.layer.4.attention.self.key.weight_integer', 'ibert.encoder.layer.5.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.4.intermediate.output_activation.x_max', 'ibert.encoder.layer.0.output.dense.weight_integer', 'ibert.encoder.layer.2.output.dense.weight_integer', 'ibert.encoder.layer.3.attention.self.value_activation.x_max', 'ibert.encoder.layer.5.pre_output_act.x_min', 'ibert.encoder.layer.5.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.0.output.ln_input_act.x_min', 'ibert.encoder.layer.0.output.output_activation.x_max', 'ibert.encoder.layer.6.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.4.attention.self.query_activation.x_min', 'ibert.encoder.layer.1.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.dense.bias_integer', 'ibert.encoder.layer.3.pre_output_act.x_min', 'ibert.encoder.layer.0.attention.self.query.bias_integer', 'ibert.encoder.layer.3.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.3.pre_intermediate_act.x_min', 'ibert.encoder.layer.9.attention.self.value.weight_integer', 'ibert.encoder.layer.2.attention.self.query_activation.x_min', 'ibert.encoder.layer.11.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.1.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.key.bias_integer', 'ibert.encoder.layer.0.attention.self.output_activation.x_max', 'ibert.encoder.layer.0.attention.self.softmax.act.x_max', 'ibert.encoder.layer.0.attention.output.dense.bias_integer', 'ibert.encoder.layer.3.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.10.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.0.attention.self.query_activation.x_max', 'ibert.encoder.layer.11.attention.output.output_activation.x_max', 'ibert.encoder.layer.2.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.0.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.10.attention.output.output_activation.x_min', 'ibert.encoder.layer.6.intermediate.output_activation.x_min', 'ibert.encoder.layer.4.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.output_activation.x_min', 'ibert.encoder.layer.10.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.10.intermediate.dense.weight_integer', 'ibert.encoder.layer.9.attention.output.dense.bias_integer', 'ibert.encoder.layer.10.output.ln_input_act.x_max', 'ibert.encoder.layer.11.attention.self.query.weight_integer', 'ibert.encoder.layer.6.attention.output.dense.bias_integer', 'ibert.encoder.layer.1.attention.self.output_activation.x_max', 'ibert.encoder.layer.8.output.dense.bias_integer', 'ibert.encoder.layer.11.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.query.bias_integer', 'ibert.encoder.layer.5.output.dense.bias_integer', 'ibert.encoder.layer.0.attention.output.LayerNorm.shift', 'ibert.encoder.layer.10.intermediate.dense.bias_integer', 'ibert.encoder.layer.10.attention.self.query.bias_integer', 'ibert.encoder.layer.8.attention.self.key_activation.x_min', 'ibert.encoder.layer.10.pre_output_act.x_max', 'ibert.encoder.layer.10.output.LayerNorm.shift', 'ibert.encoder.layer.6.intermediate.dense.bias_integer', 'ibert.encoder.layer.0.attention.self.value_activation.x_max', 'ibert.encoder.layer.7.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.value_activation.x_min', 'ibert.encoder.layer.7.pre_output_act.x_max', 'ibert.encoder.layer.3.output.ln_input_act.x_max', 'ibert.encoder.layer.3.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.query_activation.x_min', 'ibert.encoder.layer.1.attention.self.query.bias_integer', 'ibert.encoder.layer.7.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.8.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.query_activation.x_min', 'ibert.encoder.layer.5.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.1.attention.self.value.weight_integer', 'ibert.encoder.layer.4.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.10.output.dense.weight_integer', 'ibert.encoder.layer.3.attention.self.value_activation.x_min', 'ibert.encoder.layer.11.pre_output_act.x_min', 'ibert.encoder.layer.2.attention.output.output_activation.x_min', 'ibert.encoder.layer.9.output.LayerNorm.shift', 'ibert.encoder.layer.6.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.10.intermediate.output_activation.x_min', 'ibert.encoder.layer.7.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.1.intermediate.output_activation.x_max', 'ibert.encoder.layer.2.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.9.attention.self.softmax.act.x_max', 'ibert.encoder.layer.8.attention.output.dense.weight_integer', 'ibert.encoder.layer.11.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.0.output.ln_input_act.x_max', 'ibert.encoder.layer.7.intermediate.dense.bias_integer', 'ibert.encoder.layer.8.attention.self.key_activation.x_max', 'ibert.encoder.layer.0.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.10.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.5.attention.self.query.bias_integer', 'ibert.encoder.layer.6.pre_output_act.x_min', 'ibert.encoder.layer.0.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.2.intermediate.output_activation.x_max', 'ibert.encoder.layer.9.output.dense.weight_integer', 'ibert.encoder.layer.3.intermediate.dense.bias_integer', 'ibert.encoder.layer.2.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.5.attention.self.softmax.act.x_max', 'ibert.encoder.layer.4.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.0.attention.self.value_activation.x_min', 'ibert.encoder.layer.8.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.7.pre_intermediate_act.x_min', 'ibert.encoder.layer.10.attention.output.dense.weight_integer', 'ibert.encoder.layer.3.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.value.bias_integer', 'ibert.encoder.layer.6.output.dense.fc_scaling_factor', 'ibert.encoder.layer.7.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.11.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.2.attention.self.key_activation.x_max', 'ibert.encoder.layer.7.attention.self.softmax.act.x_min', 'ibert.encoder.layer.6.attention.self.query.bias_integer', 'ibert.encoder.layer.2.pre_output_act.x_min', 'ibert.encoder.layer.1.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.8.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.0.pre_output_act.x_min', 'ibert.encoder.layer.8.attention.output.output_activation.x_max', 'ibert.encoder.layer.2.attention.output.output_activation.x_max', 'ibert.encoder.layer.9.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.1.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.9.attention.self.key_activation.x_min', 'ibert.encoder.layer.9.attention.self.query_activation.act_scaling_factor']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 Simple Rules for Dating My Teenage Daughter had an auspicious start. The supremely-talented Tom Shadyac was involved in the project. This meant that the comedy would be nothing less of spectacular, and that's exactly what happened: the show remains one of the freshest, funniest, wittiest shows made in a very long time. Every line, facial expression, casting choice, scene, all wreaked of perfection. There was not one episode after which I thought, \"Man that wasn't as good as the rest\". Each one was a standout. Again, this is the kind of perfectionism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/e171c86ab27ab74c8a41930dc7c8db43032c0e11ffecbd2d39cc30f0463da471.3f37e6d260352c00a840364dc5458bec8e32025edc714e6a579f63753b03f051\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-led\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_window\": 4,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 16,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 4,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 2,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 4,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"max_position_embeddings\": 32,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-led ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/b96100da8f192372e452a0a2a28ba58786a628ac570ffe7ed47fb5be0110893c.c45ad9d7931b838f87a2793ae47dd5fd5edc1a6b0055b898d6e65c3c693ade29\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/e737c0fd0559f2048e8ee281c4b0ce447c4d643253d228ab92e4c0bfda7d8087.0f509d79aaf2540546fff94fa84c1f23aa8e983149cc0dfb587c82d00a3b2497\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/35e261f171e347450705f7724f6dc4c3d2f6020fdd89918be457de08cc7b4f18.6df0972d7138f4fc432df7e74a3d8cb7db67aa4401bd58b6a7598b7707692475\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/beff82e200f1bf4ed54677264ce146cc300da477813c601d7e438040e1a9bac5.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/a222e3828f5ffdf6a557d8511e89cb39f60dbea1c71826f20a6beb02707b7b97.8786c6d364b41f3da70545e852766061845bdb07141930ef8597fa56f89a893a\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-led/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/804683a09c2de67306150ddc03afa69c1e4deb78481a88a8cd71b0dd71ffe096.5c5d147b49ab386ece1b0a19ad91fb564e48bdb4c8e5c6d38970751d7d5a1f81\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-led were not used when initializing LEDForSequenceClassification: ['qa_outputs.bias', 'lm_head.weight', 'qa_outputs.weight', 'final_logits_bias']\n",
      "- This IS expected if you are initializing LEDForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEDForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of LEDForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-led.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tled\n",
      "tokenizer:\tLEDTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I felt duty bound to watch the 1983 Timothy Dalton / Zelah Clarke adaptation of \"Jane Eyre,\" because I'd just written an article about the 2006 BBC \"Jane Eyre\" for TheScreamOnline.&lt;br /&gt;&lt;br /&gt;So, I approached watching this the way I'd approach doing homework.&lt;br /&gt;&lt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/81708081a8082a3dee0e4ff2a0dcc8a8a6ad3bf4f9aff009a805b9d7a478fa31.da7d874b35acfd69e29620b3f87f366a726067f620b17c894607146de775fef1\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": 4,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 32,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 37,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 5,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-longformer ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/eed36a714785cf42451a72bf4f9b77fd87742d9dc1311da0530a6cd557e79001.c45ad9d7931b838f87a2793ae47dd5fd5edc1a6b0055b898d6e65c3c693ade29\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/f451b0dc9ba9bda1973da374aea24045e0a188e082a669014b22c4b8c80ac4f7.0f509d79aaf2540546fff94fa84c1f23aa8e983149cc0dfb587c82d00a3b2497\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/024232e00fe3fdbdd0e663a21bea15c890159cd38ef7197ba16bd5cd0fd9aa2e.31a3adf1b4fbab82d0dce2f9fa890fad77a90174dc28bf62778ed5bdf884c5f4\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/191db244318d0d2e5cbf38b6a93dc2b5cbd51b9c62c7a7e20ec30928d2e53c15.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/0a0379b58c742370b60cc79e53d7d8e05268b3392db8dfaef5cd97e0bf900fb0.e84d04fd7e812b6054e9ad0f8ac73fe32029e3faaf357e082f9e36361b9016cf\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-longformer/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/b651e0ec4234ac9c8a9a344bcac826aa55c7c0d6e2febfb98a3be17e707dcab3.1f9c9c836ccf80cad9b7362f1580547b794d504f6a92f270fa51c93e473fa36f\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-longformer were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'classifier.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'qa_outputs.bias', 'lm_head.dense.weight', 'classifier.weight', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-longformer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 Simple Rules for Dating My Teenage Daughter had an auspicious start. The supremely-talented Tom Shadyac was involved in the project. This meant that the comedy would be nothing less of spectacular, and that's exactly what happened: the show remains one of the freshest, funniest, wittiest shows made in a very long time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/922349f7fa5af9c9cf0c8e4ee912664a0b36c79752350f7e56aeb29909ded451.c3b3a430fd159282ee6be3a2b33ba09820c3dd59903a75b75fe40083b6958bfe\n",
      "Model config MBartConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-mbart\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 16,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 4,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 4,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 100,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 1030\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-mbart ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/sentencepiece.bpe.model from cache at /home/dev/.cache/huggingface/transformers/ee84fa04b4ac155fc71a0b3636feffa9a4995e7348e7e18bac6e6f776887f261.23157302a65b38857b36a1190b1e0d6130eab98c0885f45aed5df1e5d6d906f9\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/21bc0b1cf71ea2d003cc0a558ac33daf1e7dbd75ea875ee67dad5980f257bb43.05b321ed794244ec380dcc72e05f12fd3383edbf9f07ef7b4de70e7707fea72a\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/2384606ae1ffb46dba9a6d83357724a2179b0d710e09110919955e708209712d.e30c9fb7d9f6e7b1c3279df9995750097a8135dfe5aa55f9596ce80e7d3ba47b\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/bbbb0abaa97aab02b9a4356adaf23010fa07ba86351444b8141b50a8911d2fc0.8d9c86256496e6807bf3cf66f8a346a1e0bc357332a91da0dc6f755420ca8a01\n",
      "Assigning ['ar_AR', 'cs_CZ', 'de_DE', 'en_XX', 'es_XX', 'et_EE', 'fi_FI', 'fr_XX', 'gu_IN', 'hi_IN', 'it_IT', 'ja_XX', 'kk_KZ', 'ko_KR', 'lt_LT', 'lv_LV', 'my_MM', 'ne_NP', 'nl_XX', 'ro_RO', 'ru_RU', 'si_LK', 'tr_TR', 'vi_VN', 'zh_CN'] to the additional_special_tokens key of the tokenizer\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/5df740e058182cc29ddb37457ab6db8cb8e2b914503fb20f317544fae62d18b5.504ae4a4c3759439df9e9897c2416d79b70c88bf7940c375acf73d085ce1cca8\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-mbart were not used when initializing MBartForSequenceClassification: ['decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layer_norm.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.bias', 'encoder.layers.1.fc1.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.embed_positions.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'final_logits_bias', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'encoder.embed_tokens.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.1.fc2.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.fc2.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.fc1.bias', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.0.final_layer_norm.bias', 'decoder.layer_norm.weight', 'lm_head.weight', 'shared.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.fc1.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.0.fc1.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'encoder.layernorm_embedding.bias', 'decoder.layers.0.fc1.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'qa_outputs.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.final_layer_norm.weight', 'encoder.embed_positions.weight', 'encoder.layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.final_layer_norm.weight', 'qa_outputs.weight', 'decoder.embed_tokens.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layernorm_embedding.bias', 'decoder.layernorm_embedding.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.fc1.weight', 'decoder.layers.0.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing MBartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MBartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of MBartForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-mbart.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmbart\n",
      "tokenizer:\tMBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen(15) is the developing world's answer to Silence of the Lambs. Where Silence' terrorized our peace of mind, Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The release of TARAN THE APE MAN, in 12, caused a sensation. It may be hard to believe, 0 years later, but the film had much of the same kind of impact as THE MATRI, or THE LORD OF THE RINGS has achieved, at a time when movies and radio were the major so</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d54e5bf3e86b76e9c28657d3c43bb028d1e9a97570cb75463e6a9be832581ea8.cb5cbdb7fbab594ea91faa392666d1f7d58705fa38a497c66e37af7dd3d9ac50\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-mpnet\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 64,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 64,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 5,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 1125\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-mpnet ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/38f9d83ccba3884762e6934a5e9c3b76405143ab42f120d36cf25af42c9b587f.02b166fb3942e04a8c97454890d839105fe103e72d8fc23d1f57ac37480589b4\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/653e07e84a4df690308acec39f9fe2b24e2bc5fcfe92c16f4f007286ee743e1a.b772630b3633429246cd4773a61a0f8b3bca7b15fa9b5375d28bcb243fab889a\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/ce3f963ed6320a016db84f22bedd2ff06a63b5ae816bd6c3b3900cd0a5a47b54.18ebceb237d999d8f1cb15935e35b314f3e73dd6c4f65e119f4790fa226c9236\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/a4b2b6afeca9f9b8f148d7c0ceadceb9362fb7528b89faea5575496f3e69f7c9.df90f5e01f10913bc31be845f020486cbff70854585c949a634f004b170d659d\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-mpnet/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/177992b222eb3bc08fa27fcb69cd8196d2c9887f9e20917dbaa636d03a2859f8.13549495e94be4b1151d29fccecdb853c9fef8377a271033e784713044bb078c\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-mpnet were not used when initializing MPNetForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'pooler.dense.weight', 'classifier.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'qa_outputs.bias', 'lm_head.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of MPNetForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-mpnet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPNetForSequenceClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and sadde</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by 1987 hong kong had given the world such films as sammo hung's ` encounters of the spooky kind'chow yun fat in john woo's iconic ` a better tomorrow ', ` z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/7d4862953e5ffac5b9a850accf13bd9f9d14110adabf9a90798f9e8e5aafb9c6.72b4f91db66e830123ce06c4caa311cf151539a2860ac0a9f3f53ede5404cf0d\n",
      "Model config MobileBertConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-mobilebert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_activation\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 32,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 64,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 37,\n",
      "  \"intra_bottleneck_size\": 128,\n",
      "  \"key_query_shared_bottleneck\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"mobilebert\",\n",
      "  \"normalization_type\": \"no_norm\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_feedforward_networks\": 4,\n",
      "  \"num_hidden_layers\": 5,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"trigram_input\": true,\n",
      "  \"true_hidden_size\": 128,\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"use_bottleneck\": true,\n",
      "  \"use_bottleneck_attention\": false,\n",
      "  \"vocab_size\": 1124\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-mobilebert ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/9e0862fee2bea252beff5e9b94de8cb1a7dc5d173c7da10ddeb5ea2cfc1051f1.1d55127e14ebde4e25453f50c0f2cc32f52bb8757ce34c4636ac2e4964fffd58\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/2ce2bded53fee5496012defc7b4a28bc9a770ffc9ece071b4e97e642996aaf2d.9948d01bf1c3e913d498d87f5fd019e127e074e8813b7610be5f6c3ca87db501\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/b6b9a0b695e9ec03596e2e04c05d72b6429693e5615b28ae2e7fce98bfb5806b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/8df616c2d8b3dfb3279dff23c8ae048543654ec346e7aed020f190fad28922e1.8b495d08b066aaca97c76f8b9e38baa16bd8cde8772d50ccb0736e5471a67d16\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-mobilebert/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/55d03a94d3371222a047af26e324e30ff211581c26fe2395a41db0818625d63f.366f1424493e68ecab0490dc0bd8632a5dfe92bf9f7b8112c853db000c1d69d8\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-mobilebert were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of MobileBertForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-mobilebert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MobileBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and sadde</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chris rock deserves better than he gives himself in \" down to earth. \" as directed by brothers chris &amp; paul weitz of \" american pie \" fame, this uninspired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/openai-gpt/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/bebb46f5735701bc248ef9faa26f12577944fa7fc8e9be1a774b94d4cb8b79b6.ba6f10a5446f364b92311c09e55e49aa27024a4aeefc1ea50fd733b77bcd997d\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"_name_or_path\": \"openai-gpt\",\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai-gpt ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/openai-gpt/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/bebb46f5735701bc248ef9faa26f12577944fa7fc8e9be1a774b94d4cb8b79b6.ba6f10a5446f364b92311c09e55e49aa27024a4aeefc1ea50fd733b77bcd997d\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"_name_or_path\": \"openai-gpt\",\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/openai-gpt/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/918c57540c636a2a662770d208fcf20aa8c3faea78201fc612e5c84f052f1119.ac55819e76b0f8b0c32cbb407436947d090d98f8952f38376ee249ed382927ab\n",
      "loading file https://huggingface.co/openai-gpt/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/a682e219a788dde0e4f77bc5a470d85a4d7e493420506ce7e3266f7be122cf9e.2150b9689fda7ca7c6224ff32672c004259f974e96934e8eb69d8dd546d682db\n",
      "loading file https://huggingface.co/openai-gpt/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/325373fcbb0daa99905371727842a87ae9ca0f02f71db071720bb4d5a59076cf.b1810f3c6ed9fc0632664008484a9b569103559c04ac90321723cd808a3a96f9\n",
      "loading file https://huggingface.co/openai-gpt/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/openai-gpt/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/openai-gpt/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/openai-gpt/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/bebb46f5735701bc248ef9faa26f12577944fa7fc8e9be1a774b94d4cb8b79b6.ba6f10a5446f364b92311c09e55e49aa27024a4aeefc1ea50fd733b77bcd997d\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"_name_or_path\": \"openai-gpt\",\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/openai-gpt/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/3e867ce638da986403594a5acbb39846ecb9c3b360a3b526348dd54b06938e55.93527980a112896731f93175b7c1cbc6b0fd771fad85fcc777ff5d49d249782e\n",
      "All model checkpoint weights were used when initializing OpenAIGPTForSequenceClassification.\n",
      "\n",
      "Some weights of OpenAIGPTForSequenceClassification were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\topenai\n",
      "tokenizer:\tOpenAIGPTTokenizerFast\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also argue against (! ) the death penalty far better than kevin spacey's the life of david gayle ( 2002 ). &lt; br / &gt; &lt; br / &gt; humans are machiavellian mammals, under which lie limbic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i was at first disgusted with director sun - woo jang because i had felt that he cheated me. jang had the potential to create a strong, deeply emotional film about sex and its effects on people, but instead chose to focus his strength on the pornography element more than the actual human element. i couldn't see the characters at first and his sloppy introduction which blended both realism and cinema together was amateurish at best yet this film remained in my mind for days after i viewed it. what stayed with me wasn't the story, it wasn't the characters, nor was it the apparent pornographic nature of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/fb4dd7d2aa0fd618977b8007a9482d065c0e2a316a29d921b00f8c55973876c3.ef7496bdec4f3ac0f715459076cb51059bdf3c754a951e02c0d43bab967a152f\n",
      "Model config ReformerConfig {\n",
      "  \"_name_or_path\": \"google/reformer-crime-and-punishment\",\n",
      "  \"architectures\": [\n",
      "    \"ReformerModelWithLMHead\"\n",
      "  ],\n",
      "  \"attention_head_size\": 64,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attn_layers\": [\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\"\n",
      "  ],\n",
      "  \"axial_norm_std\": 1.0,\n",
      "  \"axial_pos_embds\": true,\n",
      "  \"axial_pos_embds_dim\": [\n",
      "    64,\n",
      "    192\n",
      "  ],\n",
      "  \"axial_pos_shape\": [\n",
      "    512,\n",
      "    1024\n",
      "  ],\n",
      "  \"chunk_size_lm_head\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_size\": 512,\n",
      "  \"hash_seed\": null,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.05,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_attention_probs_dropout_prob\": 0.05,\n",
      "  \"local_attn_chunk_length\": 64,\n",
      "  \"local_num_chunks_after\": 0,\n",
      "  \"local_num_chunks_before\": 1,\n",
      "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
      "  \"lsh_attn_chunk_length\": 64,\n",
      "  \"lsh_num_chunks_after\": 0,\n",
      "  \"lsh_num_chunks_before\": 1,\n",
      "  \"max_position_embeddings\": 524288,\n",
      "  \"model_type\": \"reformer\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_buckets\": [\n",
      "    64,\n",
      "    128\n",
      "  ],\n",
      "  \"num_chunks_after\": 0,\n",
      "  \"num_chunks_before\": 1,\n",
      "  \"num_hashes\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 320\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/reformer-crime-and-punishment ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/fb4dd7d2aa0fd618977b8007a9482d065c0e2a316a29d921b00f8c55973876c3.ef7496bdec4f3ac0f715459076cb51059bdf3c754a951e02c0d43bab967a152f\n",
      "Model config ReformerConfig {\n",
      "  \"_name_or_path\": \"google/reformer-crime-and-punishment\",\n",
      "  \"architectures\": [\n",
      "    \"ReformerModelWithLMHead\"\n",
      "  ],\n",
      "  \"attention_head_size\": 64,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attn_layers\": [\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\"\n",
      "  ],\n",
      "  \"axial_norm_std\": 1.0,\n",
      "  \"axial_pos_embds\": true,\n",
      "  \"axial_pos_embds_dim\": [\n",
      "    64,\n",
      "    192\n",
      "  ],\n",
      "  \"axial_pos_shape\": [\n",
      "    512,\n",
      "    1024\n",
      "  ],\n",
      "  \"chunk_size_lm_head\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_size\": 512,\n",
      "  \"hash_seed\": null,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.05,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_attention_probs_dropout_prob\": 0.05,\n",
      "  \"local_attn_chunk_length\": 64,\n",
      "  \"local_num_chunks_after\": 0,\n",
      "  \"local_num_chunks_before\": 1,\n",
      "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
      "  \"lsh_attn_chunk_length\": 64,\n",
      "  \"lsh_num_chunks_after\": 0,\n",
      "  \"lsh_num_chunks_before\": 1,\n",
      "  \"max_position_embeddings\": 524288,\n",
      "  \"model_type\": \"reformer\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_buckets\": [\n",
      "    64,\n",
      "    128\n",
      "  ],\n",
      "  \"num_chunks_after\": 0,\n",
      "  \"num_chunks_before\": 1,\n",
      "  \"num_hashes\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 320\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/spiece.model from cache at /home/dev/.cache/huggingface/transformers/70ee1293a84a4569f1e57a9da31b86133a901126a6e56fc09e706a6421243543.6c44dabbd46df652a6e67acd157b1f4691811a3e0cd953b4d65297069047df37\n",
      "loading file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/50592a59a46d22459f847fba80bc607ac53e980c01c8d78350a391a08c346c6b.c983c6e69a59e318e93c7332db66931317ab23d33257c41fb80e85b6c90b2649\n",
      "loading file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/fb4dd7d2aa0fd618977b8007a9482d065c0e2a316a29d921b00f8c55973876c3.ef7496bdec4f3ac0f715459076cb51059bdf3c754a951e02c0d43bab967a152f\n",
      "Model config ReformerConfig {\n",
      "  \"_name_or_path\": \"google/reformer-crime-and-punishment\",\n",
      "  \"architectures\": [\n",
      "    \"ReformerModelWithLMHead\"\n",
      "  ],\n",
      "  \"attention_head_size\": 64,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attn_layers\": [\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\"\n",
      "  ],\n",
      "  \"axial_norm_std\": 1.0,\n",
      "  \"axial_pos_embds\": true,\n",
      "  \"axial_pos_embds_dim\": [\n",
      "    64,\n",
      "    192\n",
      "  ],\n",
      "  \"axial_pos_shape\": [\n",
      "    512,\n",
      "    1024\n",
      "  ],\n",
      "  \"chunk_size_lm_head\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_size\": 512,\n",
      "  \"hash_seed\": null,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.05,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_attention_probs_dropout_prob\": 0.05,\n",
      "  \"local_attn_chunk_length\": 64,\n",
      "  \"local_num_chunks_after\": 0,\n",
      "  \"local_num_chunks_before\": 1,\n",
      "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
      "  \"lsh_attn_chunk_length\": 64,\n",
      "  \"lsh_num_chunks_after\": 0,\n",
      "  \"lsh_num_chunks_before\": 1,\n",
      "  \"max_position_embeddings\": 524288,\n",
      "  \"model_type\": \"reformer\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_buckets\": [\n",
      "    64,\n",
      "    128\n",
      "  ],\n",
      "  \"num_chunks_after\": 0,\n",
      "  \"num_chunks_before\": 1,\n",
      "  \"num_hashes\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 100\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 320\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/reformer-crime-and-punishment/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/234cc182595e496db2c02bf610236cb1136c422cb4a944b6867f6fc3abdf49e2.e2a68382c3ad65e530f38ec080b658e8b1056c71aa5d48c7056e426b2dcfc30d\n",
      "You might want to disable causal masking for sequence classification\n",
      "Some weights of the model checkpoint at google/reformer-crime-and-punishment were not used when initializing ReformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing ReformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ReformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ReformerForSequenceClassification were not initialized from the model checkpoint at google/reformer-crime-and-punishment and are newly initialized: ['reformer.encoder.layers.5.attention.self_attention.self_mask_value_float32', 'reformer.encoder.layers.3.attention.self_attention.self_mask_value_float16', 'reformer.encoder.layers.1.attention.self_attention.mask_value_float16', 'reformer.encoder.layers.2.attention.self_attention.mask_value_float16', 'reformer.encoder.layers.4.attention.self_attention.mask_value_float16', 'reformer.encoder.layers.0.attention.self_attention.mask_value_float32', 'reformer.encoder.layers.1.attention.self_attention.self_mask_value_float32', 'reformer.encoder.layers.1.attention.self_attention.self_mask_value_float16', 'reformer.encoder.layers.3.attention.self_attention.self_mask_value_float32', 'classifier.dense.bias', 'classifier.dense.weight', 'reformer.encoder.layers.5.attention.self_attention.mask_value_float32', 'reformer.encoder.layers.2.attention.self_attention.mask_value_float32', 'reformer.encoder.layers.3.attention.self_attention.mask_value_float16', 'reformer.encoder.layers.1.attention.self_attention.mask_value_float32', 'reformer.encoder.layers.5.attention.self_attention.mask_value_float16', 'reformer.encoder.layers.0.attention.self_attention.mask_value_float16', 'classifier.out_proj.bias', 'reformer.encoder.layers.4.attention.self_attention.mask_value_float32', 'reformer.encoder.layers.3.attention.self_attention.mask_value_float32', 'reformer.encoder.layers.5.attention.self_attention.self_mask_value_float16', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\treformer\n",
      "tokenizer:\tReformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citizen( is the developing worlds answer to Silence of the Lambs. Where Silence terrorized our peace of mind, Citizen exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, tha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By  Hong Kong had given the world such films as Sammo Hungs Encounters of the Spooky Kind Chow Yun Fat in ohn Woos iconic A Better Tomorrow, Zu Warriors and the classic Mr ampire. ackie C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/rembert/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/1d4f8b58603e2714a98447b0de319bf42eaada64567b08be0202aa36989ec31f.61fd5e9ebb7b22b0d19c50c73bcf5471e19b1e77d27f4e7d1e62981e9c2f8a06\n",
      "Model config RemBertConfig {\n",
      "  \"_name_or_path\": \"google/rembert\",\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 312,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"embedding_dropout_prob\": 0,\n",
      "  \"embedding_size\": 256,\n",
      "  \"eos_token_id\": 313,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_embedding_size\": 256,\n",
      "  \"intermediate_size\": 4608,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"rembert\",\n",
      "  \"num_attention_heads\": 18,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"output_embedding_size\": 1664,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250300\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/google/rembert/resolve/main/sentencepiece.model from cache at /home/dev/.cache/huggingface/transformers/7ca0279022753d7b063992b91da7fc84ccc88203cd5595663f84d903f43fe301.bd5b05922d485604855403b3bc218ca924b9543483bd08debfc9bc7d2ffb5d11\n",
      "loading file https://huggingface.co/google/rembert/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/aecbfd30ac098cd64a7ef62afbbb949a7bf80cb8804189d74e1c9cc1f252627f.d25565a9dc594a409d4526c522844b49afdb8c27349e04d079968d8a95f61391\n",
      "loading file https://huggingface.co/google/rembert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/rembert/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/d0d5cf448e7367ce69a8cbb48980c788a66b736ec136a0d3061fd26b5c1b25f0.f886166424e457f0fc75f92e81205faabe843b2dbbbef6b25f9d8ec69f64bc7d\n",
      "loading file https://huggingface.co/google/rembert/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/0192cd69242ad121f14a85e5a75456dcd1fa2ecf237311799988d3918acd7ffe.ee799d1c9dc9e598b5f38efc31c9126e75addfc18390d9adf0865fbd2a3aa53d\n",
      "loading weights file https://huggingface.co/google/rembert/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/b1a0dbf3bd2a625dcc4bbe19018436d251cf3f0e654165f88ce935aa8dc280ac.4fd6d0349211af4be18647e7f918e54f772fae707c2d5dfdbb1698c554ca4196\n",
      "All model checkpoint weights were used when initializing RemBertForSequenceClassification.\n",
      "\n",
      "Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (400 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antonioni, by making this film, had assumed the role of Papa Smurf to all the little long-haired, American, radical student-Smurfs. He had taken them under the guiding protection of his European communist wings, showing appreciation and support for their confused American ways. (These Smurfs are red and wear blue, not the other way around.) The radical Smurfs were happy to get the guidance of a wise old man with gray hair who regularly preys to the God of all long-haired Smurfs, Lenin the Communist - another wise old man whose beard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d2e42d2d9c083050e4ce532ca0cfe4a130976aa9946518bbdcb3b28ef7e150fe.be5a0dc6d5ebeb31f02c23667d0d315be5533770f9211e18d23fc8c74eac5e91\n",
      "Model config RoFormerConfig {\n",
      "  \"_name_or_path\": \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
      "  \"architectures\": [\n",
      "    \"RoFormerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"embedding_size\": 384,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roformer\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_activation\": \"linear\",\n",
      "  \"rotary_value\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 12000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d2e42d2d9c083050e4ce532ca0cfe4a130976aa9946518bbdcb3b28ef7e150fe.be5a0dc6d5ebeb31f02c23667d0d315be5533770f9211e18d23fc8c74eac5e91\n",
      "Model config RoFormerConfig {\n",
      "  \"_name_or_path\": \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
      "  \"architectures\": [\n",
      "    \"RoFormerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"embedding_size\": 384,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roformer\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_activation\": \"linear\",\n",
      "  \"rotary_value\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 12000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/be949afaec34af05a5aae840aa22a63e0075cc9d539a130f9c9931eceb9dcff2.5de0938b357af512a56e4a61702082dd6cde542af35272cc9ba7e5dfb52073c8\n",
      "loading file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d2e42d2d9c083050e4ce532ca0cfe4a130976aa9946518bbdcb3b28ef7e150fe.be5a0dc6d5ebeb31f02c23667d0d315be5533770f9211e18d23fc8c74eac5e91\n",
      "Model config RoFormerConfig {\n",
      "  \"_name_or_path\": \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
      "  \"architectures\": [\n",
      "    \"RoFormerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"embedding_size\": 384,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roformer\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_activation\": \"linear\",\n",
      "  \"rotary_value\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 12000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/d2e42d2d9c083050e4ce532ca0cfe4a130976aa9946518bbdcb3b28ef7e150fe.be5a0dc6d5ebeb31f02c23667d0d315be5533770f9211e18d23fc8c74eac5e91\n",
      "Model config RoFormerConfig {\n",
      "  \"_name_or_path\": \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
      "  \"architectures\": [\n",
      "    \"RoFormerForCausalLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"embedding_size\": 384,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roformer\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_activation\": \"linear\",\n",
      "  \"rotary_value\": false,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 12000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/junnyu/roformer_chinese_sim_char_ft_small/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/d3731d95ce2cfed3e4e3ff8cce4e9909eaea6210cdcc9f0794e7e3768fc976bc.e7d5ce15d8e034122d1cdae155617b2d2061c70f903e4be5fb805e296cdd308b\n",
      "Some weights of the model checkpoint at junnyu/roformer_chinese_sim_char_ft_small were not used when initializing RoFormerForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'roformer.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'roformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RoFormerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoFormerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at junnyu/roformer_chinese_sim_char_ft_small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'roformer.encoder.embed_positions.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where silence'terrorized our peace of mind, citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>within the realm of science fiction, two particular themes consistently elicit interest, were initially explored in the literature of a pre - cinematic era, and have since been periodically revisited by filmmakers and writers alike, with varying degrees of success. the first theme, that of time travel, has held an unwavering fascination for fans of film, as well as the wri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic brains</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but you're still gonna scarf 'em down, just don't be a pig and go for the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/3953e1a6f4509cd931511911edbb30b9651fcc668edd98df1afc2acc42f1c3aa.0f40271b2d963efad02f5495564fb1a105ec6ea0a484cc97c7d6ffb3385ce4b5\n",
      "Model config SqueezeBertConfig {\n",
      "  \"_name_or_path\": \"squeezebert/squeezebert-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_groups\": 4,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"k_groups\": 4,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"squeezebert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_groups\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"post_attention_groups\": 1,\n",
      "  \"q_groups\": 4,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"v_groups\": 4,\n",
      "  \"vocab_size\": 30528\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/3953e1a6f4509cd931511911edbb30b9651fcc668edd98df1afc2acc42f1c3aa.0f40271b2d963efad02f5495564fb1a105ec6ea0a484cc97c7d6ffb3385ce4b5\n",
      "Model config SqueezeBertConfig {\n",
      "  \"_name_or_path\": \"squeezebert/squeezebert-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_groups\": 4,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"k_groups\": 4,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"squeezebert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_groups\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"post_attention_groups\": 1,\n",
      "  \"q_groups\": 4,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"v_groups\": 4,\n",
      "  \"vocab_size\": 30528\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/vocab.txt from cache at /home/dev/.cache/huggingface/transformers/d595c2091b46360bc76b24b9bc1d12508d92390435ac54222d55da5ceaf6d8b2.3cb12d4c71324774a019cfab0a9c451fc3f8d26e837970c566dbb7b159d3307b\n",
      "loading file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/2107fae829069c2e2bc46b013d0665581b15dbeab589d32a37ef114cf576b7e4.728be533bb18c7d114c42783366cd8a666cf36bfba470c0b452fe72df99b0720\n",
      "loading file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/3953e1a6f4509cd931511911edbb30b9651fcc668edd98df1afc2acc42f1c3aa.0f40271b2d963efad02f5495564fb1a105ec6ea0a484cc97c7d6ffb3385ce4b5\n",
      "Model config SqueezeBertConfig {\n",
      "  \"_name_or_path\": \"squeezebert/squeezebert-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_groups\": 4,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"k_groups\": 4,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"squeezebert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_groups\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"post_attention_groups\": 1,\n",
      "  \"q_groups\": 4,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"v_groups\": 4,\n",
      "  \"vocab_size\": 30528\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/squeezebert/squeezebert-uncased/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/9c2e418a80dc0b0e9f2e8db3698f8be6f470c1d66a24ea71ef9d5cb6d9a773d8.39863e299959ab4c360e400b21f061b4913c925ded19330b76a85207f2d7d1c3\n",
      "Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing SqueezeBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SqueezeBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SqueezeBertForSequenceClassification were not initialized from the model checkpoint at squeezebert/squeezebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where ` silence'terrorized our peace of mind, ` citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also argue against (! ) the death penalty far better than kevin spacey's the life of david gayle ( 2002 ). &lt; br / &gt; &lt; br / &gt; humans are machiavellian mammals, under</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by 1987 hong kong had given the world such films as sammo hung's ` encounters of the spooky kind'chow yun fat in john woo's iconic ` a better tomorrow ', ` zu warriors'and the classic ` mr vampire '. jackie chan was having international success on video, but it was with ` a chinese ghost story'that hk cinema had its first real crossover theatrical hit in the west for many years. &lt; br / &gt; &lt; br / &gt; western filmgoers had never seen anything like it. it was a film that took various ingredients that hk cinema had used for years ( flying swordsman</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/56c7c46d5e64ca137303d4f67c605e9c918c624fba287ed03583b812793e7206.3dcbe74a6aa772d2b92743c75d5ec6e321150a8760477beb4c906a983fc0554d\n",
      "Model config TransfoXLConfig {\n",
      "  \"_name_or_path\": \"hf-internal-testing/tiny-random-transfo-xl\",\n",
      "  \"adaptive\": true,\n",
      "  \"attn_type\": 0,\n",
      "  \"clamp_len\": 15,\n",
      "  \"cutoffs\": [\n",
      "    10,\n",
      "    50,\n",
      "    80\n",
      "  ],\n",
      "  \"d_embed\": 32,\n",
      "  \"d_head\": 8,\n",
      "  \"d_inner\": 128,\n",
      "  \"d_model\": 32,\n",
      "  \"div_val\": 2,\n",
      "  \"dropatt\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"init\": \"normal\",\n",
      "  \"init_range\": 0.01,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"mem_len\": 30,\n",
      "  \"model_type\": \"transfo-xl\",\n",
      "  \"n_head\": 4,\n",
      "  \"n_layer\": 5,\n",
      "  \"pad_token_id\": 98,\n",
      "  \"pre_lnorm\": false,\n",
      "  \"proj_init_std\": 0.01,\n",
      "  \"same_length\": true,\n",
      "  \"sample_softmax\": -1,\n",
      "  \"tie_projs\": [\n",
      "    false,\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 267735\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-transfo-xl ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/vocab.pkl from cache at /home/dev/.cache/huggingface/transformers/292b5a7b077fd9ef8a928d85c8012b568f01b9bbf0ae7926e99630a881c38f91.fdda7f8f560b0223cea8e83236dab189ca8f7822e8f44342d3ca22e0d894dee1\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/vocab.bin from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/vocab.txt from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/special_tokens_map.json from cache at /home/dev/.cache/huggingface/transformers/f1e14f51f41c9ef29a252f2305ab96489a9a5ee07f0649984449b321aa198753.bc4a156c14335d3e6946d8ab8c583772687dca53c2d0a1595b8325215927a136\n",
      "loading file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/0857e03da96985b75420f764513c90b8c8b1fe12eaa9dbda2c3c81e042aaef16.04b5aa0b9f00320861444be57de894527f22f864faf19513d8b99145a7039020\n",
      "loading weights file https://huggingface.co/hf-internal-testing/tiny-random-transfo-xl/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/99eb8c1fe646b8dae84c7aa2c251ca12ec750d4f1de6e9141ad1642ce726c0fa.f0790ea6b2cda3490499c9032609f3d6dbac8064dba7a83d888e02c2c181054a\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-transfo-xl were not used when initializing TransfoXLForSequenceClassification: ['layers.3.pos_ff.layer_norm.weight', 'layers.2.pos_ff.layer_norm.bias', 'layers.1.pos_ff.CoreNet.3.bias', 'layers.2.dec_attn.r_r_bias', 'crit.out_layers.1.bias', 'layers.3.dec_attn.r_net.weight', 'layers.1.pos_ff.layer_norm.weight', 'layers.0.pos_ff.CoreNet.3.bias', 'crit.out_layers.1.weight', 'layers.1.pos_ff.CoreNet.0.bias', 'word_emb.emb_projs.3', 'crit.out_layers.3.weight', 'crit.cluster_weight', 'crit.out_layers.3.bias', 'layers.1.dec_attn.r_net.weight', 'layers.4.dec_attn.r_w_bias', 'word_emb.emb_layers.1.weight', 'layers.1.pos_ff.CoreNet.3.weight', 'word_emb.emb_projs.2', 'layers.0.dec_attn.layer_norm.weight', 'layers.3.dec_attn.o_net.weight', 'crit.out_layers.0.weight', 'pos_emb.inv_freq', 'layers.0.pos_ff.CoreNet.0.weight', 'layers.2.dec_attn.qkv_net.weight', 'layers.0.pos_ff.layer_norm.bias', 'layers.3.pos_ff.layer_norm.bias', 'layers.3.pos_ff.CoreNet.0.weight', 'layers.2.dec_attn.r_w_bias', 'layers.0.dec_attn.r_r_bias', 'layers.3.dec_attn.layer_norm.weight', 'layers.4.pos_ff.layer_norm.weight', 'layers.2.pos_ff.layer_norm.weight', 'layers.1.dec_attn.r_w_bias', 'layers.3.dec_attn.r_w_bias', 'layers.3.dec_attn.r_r_bias', 'word_emb.emb_layers.0.weight', 'crit.out_layers.0.bias', 'layers.1.pos_ff.layer_norm.bias', 'layers.4.dec_attn.r_r_bias', 'layers.2.pos_ff.CoreNet.3.bias', 'crit.out_layers.2.bias', 'layers.0.pos_ff.CoreNet.3.weight', 'layers.3.pos_ff.CoreNet.0.bias', 'layers.3.pos_ff.CoreNet.3.bias', 'layers.4.pos_ff.CoreNet.3.bias', 'layers.1.dec_attn.o_net.weight', 'crit.out_projs.3', 'layers.3.dec_attn.qkv_net.weight', 'crit.out_layers.2.weight', 'layers.4.dec_attn.qkv_net.weight', 'layers.4.pos_ff.CoreNet.3.weight', 'word_emb.emb_projs.0', 'layers.0.dec_attn.layer_norm.bias', 'layers.1.dec_attn.layer_norm.weight', 'layers.3.pos_ff.CoreNet.3.weight', 'layers.2.dec_attn.o_net.weight', 'layers.2.pos_ff.CoreNet.0.bias', 'layers.4.dec_attn.o_net.weight', 'layers.2.pos_ff.CoreNet.3.weight', 'layers.4.dec_attn.layer_norm.weight', 'layers.2.pos_ff.CoreNet.0.weight', 'layers.2.dec_attn.r_net.weight', 'layers.0.pos_ff.layer_norm.weight', 'layers.1.dec_attn.layer_norm.bias', 'word_emb.emb_projs.1', 'crit.cluster_bias', 'layers.1.pos_ff.CoreNet.0.weight', 'crit.out_projs.2', 'layers.0.dec_attn.o_net.weight', 'layers.0.dec_attn.r_w_bias', 'word_emb.emb_layers.2.weight', 'layers.4.pos_ff.CoreNet.0.weight', 'layers.2.dec_attn.layer_norm.weight', 'layers.1.dec_attn.qkv_net.weight', 'layers.4.dec_attn.r_net.weight', 'crit.out_projs.0', 'layers.0.dec_attn.r_net.weight', 'layers.4.dec_attn.layer_norm.bias', 'layers.0.pos_ff.CoreNet.0.bias', 'crit.out_projs.1', 'layers.1.dec_attn.r_r_bias', 'layers.0.dec_attn.qkv_net.weight', 'layers.3.dec_attn.layer_norm.bias', 'word_emb.emb_layers.3.weight', 'layers.4.pos_ff.CoreNet.0.bias', 'layers.4.pos_ff.layer_norm.bias', 'layers.2.dec_attn.layer_norm.bias']\n",
      "- This IS expected if you are initializing TransfoXLForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TransfoXLForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TransfoXLForSequenceClassification were initialized from the model checkpoint at hf-internal-testing/tiny-random-transfo-xl.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TransfoXLForSequenceClassification for predictions without further training.\n",
      "Using pad_token, but it is not set yet.\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "Adding <pad> to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\ttransfo_xl\n",
      "tokenizer:\tTransfoXLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1995) is the developing world's answer to Silence of the Lambs. Where 'Silence' terrorized our peace of mind, 'Citizen' exhausts and saddens us instead. This dramatization of the case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; may also argue against (!) the death penalty far better than Kevin Spacey's The Life of David Gayle (2002). &lt; br / &gt; &lt; br / &gt; Humans are Machiavellian mammals, under which lie limbic brains (lizard-logic). Why did two kids, who knew better, stone to death</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 Simple Rules for Dating My Teenage Daughter had an auspicious start. The supremely-talented Tom Shadyac was involved in the project. This meant that the comedy would be nothing less of spectacular, and that's exactly what happened: the show remains one of the freshest, funniest, wittiest shows made in a very long time. Every line, facial expression, casting choice, scene, all wreaked of perfection. There was not one episode after which I thought, \"Man that wasn't as good as the rest.\" Each one was a standout. Again, this is the kind of perfectionism that we've come to expect from Tom. For those who don</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-mlm-en-2048/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/871962fa7b55c55e16a9f67271c0e834c21d65aea30e4891f49854833f9b7ca2.f0e67c8211991e2f6e18881d243fc96e2df93077c1c00806d44bf3375b7947ab\n",
      "Model config XLMConfig {\n",
      "  \"_name_or_path\": \"xlm-mlm-en-2048\",\n",
      "  \"architectures\": [\n",
      "    \"XLMWithLMHeadModel\"\n",
      "  ],\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_index\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"causal\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"emb_dim\": 2048,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"gelu_activation\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder\": true,\n",
      "  \"lang_id\": 0,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_index\": 5,\n",
      "  \"mask_token_id\": 0,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"xlm\",\n",
      "  \"n_heads\": 16,\n",
      "  \"n_langs\": 1,\n",
      "  \"n_layers\": 12,\n",
      "  \"pad_index\": 2,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"unk_index\": 3,\n",
      "  \"use_lang_emb\": true,\n",
      "  \"vocab_size\": 30145\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-mlm-en-2048/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/871962fa7b55c55e16a9f67271c0e834c21d65aea30e4891f49854833f9b7ca2.f0e67c8211991e2f6e18881d243fc96e2df93077c1c00806d44bf3375b7947ab\n",
      "Model config XLMConfig {\n",
      "  \"_name_or_path\": \"xlm-mlm-en-2048\",\n",
      "  \"architectures\": [\n",
      "    \"XLMWithLMHeadModel\"\n",
      "  ],\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_index\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"causal\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"emb_dim\": 2048,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"gelu_activation\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder\": true,\n",
      "  \"lang_id\": 0,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_index\": 5,\n",
      "  \"mask_token_id\": 0,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"xlm\",\n",
      "  \"n_heads\": 16,\n",
      "  \"n_langs\": 1,\n",
      "  \"n_layers\": 12,\n",
      "  \"pad_index\": 2,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"unk_index\": 3,\n",
      "  \"use_lang_emb\": true,\n",
      "  \"vocab_size\": 30145\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-mlm-en-2048/resolve/main/vocab.json from cache at /home/dev/.cache/huggingface/transformers/eb341040b6efe38b05232dd91486c18c161a44d3ff6a35e3f97cbd53d86e49fe.2442c7095f582948952b2d7ff1585f94565d8bc9c6f70a39a0b12c977ffba3f8\n",
      "loading file https://huggingface.co/xlm-mlm-en-2048/resolve/main/merges.txt from cache at /home/dev/.cache/huggingface/transformers/4901ef02f591921fbb4347442e434f67e2464b4823bb4e7233b1252875fa0d88.ff3b76de8b3c603255f3886eb3fc29951d87093da108c99c4b6117d6179cd973\n",
      "loading file https://huggingface.co/xlm-mlm-en-2048/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-mlm-en-2048/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-mlm-en-2048/resolve/main/tokenizer_config.json from cache at /home/dev/.cache/huggingface/transformers/a8ec564f4e389b92b2d19e0275ec3bf60c7f2d8f0f262f93c6b59660e8687102.13707721b7b253dc80eca3f1cde6ea102814cc740aecfc5e4a8754be107cfc86\n",
      "loading configuration file https://huggingface.co/xlm-mlm-en-2048/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/871962fa7b55c55e16a9f67271c0e834c21d65aea30e4891f49854833f9b7ca2.f0e67c8211991e2f6e18881d243fc96e2df93077c1c00806d44bf3375b7947ab\n",
      "Model config XLMConfig {\n",
      "  \"_name_or_path\": \"xlm-mlm-en-2048\",\n",
      "  \"architectures\": [\n",
      "    \"XLMWithLMHeadModel\"\n",
      "  ],\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_index\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"causal\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"emb_dim\": 2048,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"gelu_activation\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder\": true,\n",
      "  \"lang_id\": 0,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_index\": 5,\n",
      "  \"mask_token_id\": 0,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"xlm\",\n",
      "  \"n_heads\": 16,\n",
      "  \"n_langs\": 1,\n",
      "  \"n_layers\": 12,\n",
      "  \"pad_index\": 2,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"unk_index\": 3,\n",
      "  \"use_lang_emb\": true,\n",
      "  \"vocab_size\": 30145\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlm-mlm-en-2048/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/be6d34b1853ed7ca8e163ff2982380795ec6efb2953d4ceaf87d09a7e364c443.e58e37beaf42e3db1e230cd4e94150bc943f565f1945b8a8a487313ae8c2dfc1\n",
      "Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizenx ( 1995 ) is the developing world's answer to silence of the lambs. where'silence'terrorized our peace of mind,'citizen'exhausts and saddens us instead. this dramatization of the chikatilo case translates rather well, thanks to a westernized friendship between two rostov cops who become equals. &lt; br / &gt; &lt; br / &gt; citizenx may also argue against (! ) the death penalty far better than kevin spacey's the life of david gayle ( 2002 ). &lt; br / &gt; &lt; br / &gt; humans are machiavellian mammals, under which lie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the majority of stephen king's short stories are little gems, with original ideas that don 't take a long time to develop ; basically lean and mean--he sets them up quickly in a scarce number of pages, you read'em, and you're finished before you know you've begun. they're like the equivalent of a carton of mcdonald's fries--they taste really good and you know there's not much nutritional value in them ( re : from a literary standpoint, they don 't say much about the universal human condition ), but you're still gonna scarf'em down, just don 't be a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/dev/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mamma</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8 Simple Rules for Dating My Teenage Daughter had an auspicious start. The supremely-talented Tom Shadyac was involved in the project. This meant that the comedy would be nothing less of spectacular, and that's exactly what happened: the show remains one of the freshest, funniest, wittiest shows made in a very long time. Every line, facial expression, casting choice, scene, all wreaked of perfection. There was not one episode after which I thought, \"Man that wasn't as good as the rest\". Each one was a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
      "Model config XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
      "Model config XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /home/dev/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /home/dev/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /home/dev/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
      "Model config XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /home/dev/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You have to respect this movie. It may be \"just a dumb kid's movie\" but it's the #1 most frequently requested film title in online movie forums, requested by people who remember the story but can't remember the title. Therefore what follows is a much-needed, detailed plot description, since I haven't been able to find such a description anywhere else on the Internet.&lt;br /&gt;&lt;br /&gt;A typical 2-story house is shown in suburbia. 7-year-old Bridget narrates about suspecting something is going on since she and her 11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_class = RobertaTokenizer if (\"/ibert\" in model_name) else None\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(model_name, model_cls=model_cls, tokenizer_cls=tok_class)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz), CategoryBlock)\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n",
    "        dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\\n\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"PASSED\", \"\"))\n",
    "        dls.show_batch(dataloaders=dls, max_n=2, trunc_at=1000)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"FAILED\", err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-albert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-bart</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-bert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>google/bigbird-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigbird_pegasus</td>\n",
       "      <td>PegasusTokenizerFast</td>\n",
       "      <td>google/bigbird-pegasus-large-arxiv</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>CTRLTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-ctrl</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>camembert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>canine</td>\n",
       "      <td>CanineTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-canine</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>YituTech/conv-bert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-deberta</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deberta_v2</td>\n",
       "      <td>DebertaV2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-deberta-v2</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-distilbert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-electra</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fnet</td>\n",
       "      <td>FNetTokenizerFast</td>\n",
       "      <td>google/fnet-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-flaubert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-funnel</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-gpt2</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gptj</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>anton-l/gpt-j-tiny-random</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt_neo</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-gpt_neo</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>kssteven/ibert-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>led</td>\n",
       "      <td>LEDTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-led</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-longformer</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mbart</td>\n",
       "      <td>MBartTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-mbart</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-mpnet</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-mobilebert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>openai</td>\n",
       "      <td>OpenAIGPTTokenizerFast</td>\n",
       "      <td>openai-gpt</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>reformer</td>\n",
       "      <td>ReformerTokenizerFast</td>\n",
       "      <td>google/reformer-crime-and-punishment</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>google/rembert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>junnyu/roformer_chinese_sim_char_ft_small</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>squeezebert/squeezebert-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>TransfoXLTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-transfo-xl</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>xlm-mlm-en-2048</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>xlnet-base-cased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text.data.core` module contains the fundamental bits for all data preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('dev')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
