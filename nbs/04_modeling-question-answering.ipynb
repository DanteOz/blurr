{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.question_answering\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for question answering tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar,master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering, logging,\n",
    "    PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    ")\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import TextBlock, BlurrDataLoader, first_blurr_tfm\n",
    "from blurr.modeling.core import BaseModelCallback, PreCalculatedLoss, Blearner\n",
    "from blurr.data.question_answering import QuestionAnswerTextInput, QABatchTokenizeTransform\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "import pdb\n",
    "\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import BaseModelWrapper, PreCalculatedLoss, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions('torch fastai transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full\n",
    "# squad_df = pd.read_csv('./data/task-question-answering/squad_cleaned.csv'); len(squad_df)\n",
    "\n",
    "# sample\n",
    "squad_df = pd.read_csv('./squad_sample.csv'); len(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ds_type</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five G...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>train</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five G...</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "      <td>train</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five G...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five G...   \n",
       "\n",
       "                                                     question  \\\n",
       "0                    When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was growing up?   \n",
       "\n",
       "                                                    answers ds_type  \\\n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}   train   \n",
       "1  {'text': ['singing and dancing'], 'answer_start': [207]}   train   \n",
       "\n",
       "           answer_text  is_impossible  \n",
       "0    in the late 1990s          False  \n",
       "1  singing and dancing          False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "hf_model_cls = AutoModelForQuestionAnswering\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=hf_model_cls)\n",
    "\n",
    "# # here's a pre-trained roberta model for squad you can try too\n",
    "# pretrained_model_name = \"ahotrod/roberta_large_squad2\"\n",
    "# hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, \n",
    "#                                                                   model_cls=AutoModelForQuestionAnswering)\n",
    "\n",
    "# # here's a pre-trained xlm model for squad you can try too\n",
    "# pretrained_model_name = 'xlm-mlm-ende-1024'\n",
    "# hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name,\n",
    "#                                                                   model_cls=AutoModelForQuestionAnswering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_process_squad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59787/3844588814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msquad_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquad_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_process_squad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_process_squad' is not defined"
     ]
    }
   ],
   "source": [
    "squad_df = squad_df.apply(partial(pre_process_squad, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = squad_df[(squad_df.tokenized_input_len < max_seq_len) & (squad_df.is_impossible == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(range(max_seq_len))\n",
    "# vocab = dict(enumerate(range(max_seq_len)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for tokenizers that pad on right or left side\n",
    "trunc_strat = 'only_second' if (hf_tokenizer.padding_side == 'right') else 'only_first'\n",
    "\n",
    "before_batch_tfm = QABatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                             max_length=max_seq_len, \n",
    "                                             truncation=trunc_strat, \n",
    "                                             tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=QuestionAnswerTextInput), \n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab)\n",
    ")\n",
    "\n",
    "def get_x(x):\n",
    "    return (x.question, x.context) if (hf_tokenizer.padding_side == 'right') else (x.context, x.question)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=get_x,\n",
    "                   get_y=[ColReader('tok_answer_start'), ColReader('tok_answer_end')],\n",
    "                   splitter=RandomSplitter(),\n",
    "                   n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(squad_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls.vocab), dls.vocab[0], dls.vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "Here we create a question/answer specific subclass of `BaseModelCallback` in order to get all the start and end prediction.  We also add here a new loss function that can handle multiple targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HF_QstAndAnsModelCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_QstAndAnsModelCallback(BaseModelCallback):  \n",
    "    \"\"\"The prediction is a combination start/end logits\"\"\"\n",
    "    def after_pred(self):\n",
    "        super().after_pred()\n",
    "        self.learn.pred = (self.pred.start_logits, self.pred.end_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MultiTargetLoss`\n",
    "\n",
    "And here we provide a custom loss function our question answer task, expanding on some techniques learned from here and here.\n",
    "\n",
    "In fact, this new loss function can be used in many other multi-modal architectures, with any mix of loss functions.  For example, this can be ammended to include the `is_impossible` task, as well as the start/end token tasks in the SQUAD v2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiTargetLoss(Module):\n",
    "    \"\"\"Provides the ability to apply different loss functions to multi-modal targets/predictions\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        # The loss function for each target\n",
    "        loss_classes:List[Callable]=[CrossEntropyLossFlat, CrossEntropyLossFlat], \n",
    "        # Any kwargs you want to pass to the loss functions above\n",
    "        loss_classes_kwargs:List[dict]=[{}, {}], \n",
    "        # The weights you want to apply to each loss (default: [1,1])\n",
    "        weights:Union[List[float], List[int]]=[1, 1], \n",
    "        # The `reduction` parameter of the lass function (default: 'mean')\n",
    "        reduction:str='mean'\n",
    "    ):\n",
    "        loss_funcs = [ cls(reduction=reduction, **kwargs) for cls, kwargs in zip(loss_classes, loss_classes_kwargs) ]\n",
    "        store_attr(self=self, names='loss_funcs, weights')\n",
    "        self._reduction = reduction\n",
    "        \n",
    "    # custom loss function must have either a reduction attribute or a reduction argument (like all fastai and\n",
    "    # PyTorch loss functions) so that the framework can change this as needed (e.g., when doing lear.get_preds \n",
    "    # it will set = 'none'). see this forum topic for more info: https://bit.ly/3br2Syz\n",
    "    @property\n",
    "    def reduction(self): return self._reduction\n",
    "    \n",
    "    @reduction.setter\n",
    "    def reduction(self, v): \n",
    "        self._reduction = v\n",
    "        for lf in self.loss_funcs: lf.reduction = v\n",
    "\n",
    "    def forward(self, outputs, *targets):\n",
    "        loss = 0.\n",
    "        for i, loss_func, weights, output, target in zip(range(len(outputs)), \n",
    "                                                         self.loss_funcs, self.weights,\n",
    "                                                         outputs, targets):\n",
    "            loss += weights * loss_func(output, target) \n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    def activation(self, outs): \n",
    "        acts = [ self.loss_funcs[i].activation(o) for i, o in enumerate(outs) ]\n",
    "        return acts\n",
    "\n",
    "    def decodes(self, outs):   \n",
    "        decodes = [ self.loss_funcs[i].decodes(o) for i, o in enumerate(outs) ]\n",
    "        return decodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Notice below how I had to define the loss function *after* creating the `Learner` object.  I'm not sure why, but the `MultiTargetLoss` above prohibits the learner from being exported if I do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam, decouple_wd=True),\n",
    "                cbs=[HF_QstAndAnsModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.loss_func=MultiTargetLoss()\n",
    "learn.create_opt()                # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_start, y_end = dls.one_batch()\n",
    "preds = learn.model(x)\n",
    "len(preds),preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_results`\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `QuestionAnswerTextInput` typed inputs\n",
    "    x:QuestionAnswerTextInput, \n",
    "    # The targets\n",
    "    y, \n",
    "    # Your raw inputs/targets\n",
    "    samples,     \n",
    "    # The model's predictions\n",
    "    outs,           \n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into \n",
    "    # something understandable\n",
    "    learner, \n",
    "    # Whether you want to remove special tokens during decoding/showing the outputs\n",
    "    skip_special_tokens=True, \n",
    "    # Your `show_results` context\n",
    "    ctxs=None, \n",
    "    # The maximum number of items to show\n",
    "    max_n=6, \n",
    "     # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None, \n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs\n",
    "):     \n",
    "    tfm = first_blurr_tfm(learner.dls)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    \n",
    "    res = L()\n",
    "    for sample, input_ids, start, end, pred in zip(samples, x, *y, outs):\n",
    "        txt = hf_tokenizer.decode(sample[0], skip_special_tokens=True)[:trunc_at]\n",
    "        ans_toks = hf_tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=False)[start:end]\n",
    "        pred_ans_toks = hf_tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=False)[int(pred[0]):int(pred[1])]\n",
    "        \n",
    "        res.append((txt,\n",
    "                    (start.item(),end.item()), hf_tokenizer.convert_tokens_to_string(ans_toks),\n",
    "                    (int(pred[0]),int(pred[1])), hf_tokenizer.convert_tokens_to_string(pred_ans_toks)))\n",
    "\n",
    "    df = pd.DataFrame(res, columns=['text', 'start/end', 'answer', 'pred start/end', 'pred answer'])\n",
    "    display_df(df[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(learner=learn, skip_special_tokens=True, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and lets see how `Learner.blurr_predict` works with question/answering tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df = pd.DataFrame.from_dict([{\n",
    "    'question': 'What did George Lucas make?',\n",
    "    'context': 'George Lucas created Star Wars in 1977. He directed and produced it.'   \n",
    "}], \n",
    "    orient='columns')\n",
    "\n",
    "learn.blurr_predict(inf_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df = pd.DataFrame.from_dict([\n",
    "    {\n",
    "        'question': 'What did George Lucas make?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.'   \n",
    "    }, {\n",
    "        'question': 'What year did Star Wars come out?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.' \n",
    "    }, {\n",
    "        'question': 'What did George Lucas do?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.' \n",
    "    }], \n",
    "    orient='columns')\n",
    "\n",
    "learn.blurr_predict(inf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ids = hf_tokenizer.encode('What did George Lucas make?',\n",
    "                              'George Lucas created Star Wars in 1977. He directed and produced it.')\n",
    "\n",
    "hf_tokenizer.convert_ids_to_tokens(inp_ids, skip_special_tokens=False)[11:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a bug currently in fastai v2 (or with how I'm assembling everything) that currently prevents us from seeing the decoded predictions and probabilities for the \"end\" token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df = pd.DataFrame.from_dict([{\n",
    "    'question': 'When was Star Wars made?',\n",
    "    'context': 'George Lucas created Star Wars in 1977. He directed and produced it.'\n",
    "}], \n",
    "    orient='columns')\n",
    "\n",
    "test_dl = dls.test_dl(inf_df)\n",
    "inp = test_dl.one_batch()[0]['input_ids']\n",
    "probs, _, preds = learn.get_preds(dl=test_dl, with_input=False, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.convert_ids_to_tokens(inp.tolist()[0], \n",
    "                                   skip_special_tokens=False)[torch.argmax(probs[0]):torch.argmax(probs[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can unfreeze and continue training like normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lr_max=slice(1e-7, 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.blurr_predict(inf_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, pred_classes, probs = zip(*learn.blurr_predict(inf_df.iloc[0]))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ids = hf_tokenizer.encode('When was Star Wars made?',\n",
    "                              'George Lucas created Star Wars in 1977. He directed and produced it.')\n",
    "\n",
    "hf_tokenizer.convert_ids_to_tokens(inp_ids, skip_special_tokens=False)[int(preds[0][0]):int(preds[0][1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Note that I had to replace the loss function because of the above-mentioned issue to exporting the model with the `MultiTargetLoss` loss function.  After getting our inference learner, we put it back and we're good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name = 'q_and_a_learn_export'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyLossFlat()\n",
    "learn.export(fname=f'{export_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(fname=f'{export_name}.pkl')\n",
    "inf_learn.loss_func = MultiTargetLoss()\n",
    "\n",
    "inf_df = pd.DataFrame.from_dict([\n",
    "    {\n",
    "        'question': 'What did George Lucas make?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.'   \n",
    "    }, {\n",
    "        'question': 'What year did Star Wars come out?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.' \n",
    "    }, {\n",
    "        'question': 'What did George Lucas do?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.' \n",
    "    }], \n",
    "    orient='columns')\n",
    "\n",
    "inf_learn.blurr_predict(inf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ids = hf_tokenizer.encode('What did George Lucas make?',\n",
    "                              'George Lucas created Star Wars in 1977. He directed and produced it.')\n",
    "\n",
    "hf_tokenizer.convert_ids_to_tokens(inp_ids, skip_special_tokens=False)[11:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLearnerForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "try: del learn; del inf_learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForQuestionAnswering(Blearner):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dls:DataLoaders, \n",
    "        hf_model: PreTrainedModel, \n",
    "        **kwargs\n",
    "    ):\n",
    "        kwargs['loss_func'] = kwargs.get('loss_func', MultiTargetLoss())\n",
    "        super().__init__(dls, hf_model, base_model_cb=HF_QstAndAnsModelCallback, **kwargs)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_model_cls(self): \n",
    "        return AutoModelForQuestionAnswering\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_x(\n",
    "        cls, \n",
    "        x, \n",
    "        qst, \n",
    "        ctx, \n",
    "        padding_side='right'\n",
    "    ): \n",
    "         return (x[qst], x[ctx]) if (padding_side == 'right') else (x[ctx], x[qst])\n",
    "        \n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls, \n",
    "        # Your raw dataset\n",
    "        data, \n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset \n",
    "        preprocess_func:Callable=None, \n",
    "        # The maximum sequence length to constrain our data\n",
    "        max_seq_len:int=None,\n",
    "        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')\n",
    "        context_attr:str='context', \n",
    "        # The attribute in your dataset that contains the question being asked (default: 'question')\n",
    "        question_attr:str='question', \n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        answer_text_attr:str='answer_text',\n",
    "        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')\n",
    "        tok_ans_start_attr:str='tok_answer_start', \n",
    "        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')\n",
    "        tok_ans_end_attr:str='tok_answer_end', \n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter:Callable=RandomSplitter(), \n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={}, \n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={}\n",
    "    ):\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path, \n",
    "                                                                          model_cls=cls.get_model_cls())\n",
    "        \n",
    "        # potentially used by our preprocess_func, it is the basis for our CategoryBlock vocab\n",
    "        if (max_seq_len is None):\n",
    "            max_seq_len = hf_config.get('max_position_embeddings', 128)\n",
    "            \n",
    "        # client can pass in a function that takes the raw data, hf objects, and max_seq_len ... and\n",
    "        # returns a DataFrame with the expected format\n",
    "        if (preprocess_func):\n",
    "            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, max_seq_len, \n",
    "                                   context_attr, question_attr, answer_text_attr, \n",
    "                                   tok_ans_start_attr, tok_ans_end_attr)\n",
    "        \n",
    "        # bits required by our \"before_batch_tfm\" and DataBlock\n",
    "        vocab = list(range(max_seq_len))\n",
    "        padding_side = hf_tokenizer.padding_side\n",
    "        trunc_strat = 'only_second' if (padding_side == 'right') else 'only_first'\n",
    "\n",
    "        before_batch_tfm = QABatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                                     max_length=max_seq_len, \n",
    "                                                     truncation=trunc_strat, \n",
    "                                                     tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "        \n",
    "        # define getters\n",
    "        if (isinstance(data, pd.DataFrame)):\n",
    "            get_x = partial(cls._get_x, qst=question_attr, ctx=context_attr, padding_side=padding_side)\n",
    "            get_y = [ColReader(tok_ans_start_attr), ColReader(tok_ans_end_attr)]\n",
    "        else:\n",
    "            get_x = partial(cls._get_x, qst=question_attr, ctx=context_attr, padding_side=padding_side)\n",
    "            get_y = [ItemGetter(tok_ans_start_attr), ItemGetter(tok_ans_end_attr)]\n",
    "            \n",
    "        # define DataBlock and DataLoaders\n",
    "        blocks = (\n",
    "            TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=QuestionAnswerTextInput), \n",
    "            CategoryBlock(vocab=vocab),\n",
    "            CategoryBlock(vocab=vocab)\n",
    "        )\n",
    "        \n",
    "        dblock = DataBlock(blocks=blocks, \n",
    "                           get_x=get_x,\n",
    "                           get_y=get_y,\n",
    "                           splitter=dblock_splitter,\n",
    "                           n_inp=1)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "        \n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls, \n",
    "        # Your pandas DataFrame\n",
    "        df:pd.DataFrame, \n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset \n",
    "        preprocess_func:Callable=None, \n",
    "        # The maximum sequence length to constrain our data\n",
    "        max_seq_len:int=None,\n",
    "        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')\n",
    "        context_attr:str='context', \n",
    "        # The attribute in your dataset that contains the question being asked (default: 'question')\n",
    "        question_attr:str='question', \n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        answer_text_attr:str='answer_text',\n",
    "        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')\n",
    "        tok_ans_start_attr:str='tok_answer_start', \n",
    "        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')\n",
    "        tok_ans_end_attr:str='tok_answer_end', \n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter:Callable=ColSplitter(), \n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={}, \n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={}\n",
    "    ):\n",
    "        return cls._create_learner(df, pretrained_model_name_or_path, preprocess_func, max_seq_len,\n",
    "                                   context_attr, question_attr, answer_text_attr,\n",
    "                                   tok_ans_start_attr, tok_ans_end_attr, dblock_splitter,\n",
    "                                   dl_kwargs, learner_kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls, \n",
    "        # The path to your csv file\n",
    "        csv_file:Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset \n",
    "        preprocess_func:Callable=None, \n",
    "        # The maximum sequence length to constrain our data\n",
    "        max_seq_len:int=None,\n",
    "        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')\n",
    "        context_attr:str='context', \n",
    "        # The attribute in your dataset that contains the question being asked (default: 'question')\n",
    "        question_attr:str='question', \n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        answer_text_attr:str='answer_text',\n",
    "        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')\n",
    "        tok_ans_start_attr:str='tok_answer_start', \n",
    "        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')\n",
    "        tok_ans_end_attr:str='tok_answer_end', \n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter:Callable=ColSplitter(), \n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={}, \n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={}\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        return cls.from_dataframe(df, pretrained_model_name_or_path, preprocess_func, max_seq_len,\n",
    "                                  context_attr, question_attr, answer_text_attr,\n",
    "                                  tok_ans_start_attr, tok_ans_end_attr, dblock_splitter,\n",
    "                                  dl_kwargs, learner_kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls, \n",
    "        # A list of dictionaries\n",
    "        ds:List[Dict], \n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path:Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset \n",
    "        preprocess_func:Callable=None, \n",
    "        # The maximum sequence length to constrain our data\n",
    "        max_seq_len:int=None,\n",
    "        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')\n",
    "        context_attr:str='context', \n",
    "        # The attribute in your dataset that contains the question being asked (default: 'question')\n",
    "        question_attr:str='question', \n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        answer_text_attr:str='answer_text',\n",
    "        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')\n",
    "        tok_ans_start_attr:str='tok_answer_start', \n",
    "        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')\n",
    "        tok_ans_end_attr:str='tok_answer_end', \n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter:Callable=RandomSplitter(), \n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={}, \n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={}\n",
    "    ):\n",
    "        return cls._create_learner(ds, pretrained_model_name_or_path, preprocess_func, max_seq_len,\n",
    "                                   context_attr, question_attr, answer_text_attr, \n",
    "                                   tok_ans_start_attr, tok_ans_end_attr, dblock_splitter,\n",
    "                                   dl_kwargs, learner_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BLearnerForQuestionAnswering` requires a question, context (within which to find the answer to the question), and the start/end indices of where the answer lies in the *tokenized context*. Because those indices vary by tokenizer, we can pass a `preprocess_func` that will take our raw data, perform any preprocessing we want, and return it in a way that will work for extractive QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, hf_arch, hf_config, hf_tokenizer, hf_model, max_seq_len, \n",
    "                  context_attr, question_attr, answer_text_attr, tok_ans_start_attr, tok_ans_end_attr):\n",
    "    \n",
    "    df = df.apply(partial(pre_process_squad, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, ctx_attr=context_attr, \n",
    "                          qst_attr=question_attr, ans_attr=answer_text_attr), axis=1)\n",
    "    \n",
    "    df = df[(df.tokenized_input_len < max_seq_len) & (df.is_impossible == False)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-grab the raw data and use the high-level API to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = pd.read_csv('./squad_sample.csv')\n",
    "\n",
    "pretrained_model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "learn = BlearnerForQuestionAnswering.from_dataframe(squad_df, pretrained_model_name,\n",
    "                                                    preprocess_func=preprocess_df, max_seq_len=128,\n",
    "                                                    dblock_splitter=RandomSplitter(), \n",
    "                                                    dl_kwargs={ 'bs': 4 }).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(learner=learn, skip_special_tokens=True, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CrossEntropyLossFlat()\n",
    "learn.export(fname=f'{export_name}.pkl')\n",
    "inf_learn = load_learner(fname=f'{export_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(fname=f'{export_name}.pkl')\n",
    "inf_learn.loss_func = MultiTargetLoss()\n",
    "\n",
    "inf_df = pd.DataFrame.from_dict([\n",
    "    {\n",
    "        'question': 'What did George Lucas make?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.'   \n",
    "    }, {\n",
    "        'question': 'What year did Star Wars come out?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.' \n",
    "    }, {\n",
    "        'question': 'What did George Lucas do?',\n",
    "        'context': 'George Lucas created Star Wars in 1977. He directed and produced it.' \n",
    "    }], \n",
    "    orient='columns')\n",
    "\n",
    "inf_learn.blurr_predict(inf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for extractive Q&A tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
