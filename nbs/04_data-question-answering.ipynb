{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.question_answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.question_answering\n",
    "\n",
    "> Question/Answering tasks are models that require two text inputs (a context that includes the answer and the question).  The objective is to predict the start/end tokens of the answer in the context). This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for question/answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ast\n",
    "from functools import reduce\n",
    "\n",
    "from datasets import Dataset\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock, ColReader, ColSplitter\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import AutoModelForQuestionAnswering, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import TextInput, BatchDecodeTransform, BatchTokenizeTransform, Preprocessor, first_blurr_tfm\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import *\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.data.core import TextBlock\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `squad_v2` to demonstrate how to configure your blurr code for extractive question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (/home/wgilliam/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset(\"squad_v2\", split='train[:1000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "\n",
       "                                                                                               context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "\n",
       "                                                     question  \\\n",
       "0                    When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was growing up?   \n",
       "\n",
       "                                                    answers  \n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}  \n",
       "1  {'text': ['singing and dancing'], 'answer_start': [207]}  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df = pd.DataFrame(train_ds)\n",
    "\n",
    "print(len(squad_df))\n",
    "squad_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = AutoModelForQuestionAnswering\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"  #'xlm-mlm-ende-1024'\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n",
    "\n",
    "max_seq_len = 128\n",
    "vocab = dict(enumerate(range(max_seq_len)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods\n",
    "\n",
    "Starting with version 2.0.0, blurr provides a `find_answer_token_idxs` method that can be used during preprocessing to find the start/end token indices from the start/end character indices commonly included in the raw data.  It returns tensors indicating the start/end token indicies and one indicating whether the answer can be found in the provided set of `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_answer_token_idxs(start_char_idx, end_char_idx, offset_mapping, qst_mask):\n",
    "    # mask the question tokens so they aren't included in the search\n",
    "    masked_offset_mapping = offset_mapping.clone()\n",
    "    masked_offset_mapping[qst_mask] = tensor([-100, -100])\n",
    "\n",
    "    # based on the character start/end index, see if we can find the span of tokens in the `offset_mapping`\n",
    "    starts = torch.where((masked_offset_mapping[:, 0] == start_char_idx) | (masked_offset_mapping[:, 1] == start_char_idx))[0]\n",
    "    ends = torch.where((masked_offset_mapping[:, 0] <= end_char_idx) & (masked_offset_mapping[:, 1] >= end_char_idx))[0]\n",
    "\n",
    "    if len(starts) > 0 and len(ends) > 0:\n",
    "        for s in starts:\n",
    "            if masked_offset_mapping[s][0] <= start_char_idx:\n",
    "                start = s\n",
    "\n",
    "        for e in ends:\n",
    "            if e >= s and masked_offset_mapping[e][1] >= end_char_idx:\n",
    "                end = e\n",
    "\n",
    "        if end < len(masked_offset_mapping):\n",
    "            return (start, end)\n",
    "\n",
    "    # if neither star or end is found, or the end token is part of this chunk, consider the answer not found\n",
    "    return (tensor(0), tensor(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"find_answer_token_idxs\" class=\"doc_header\"><code>find_answer_token_idxs</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>find_answer_token_idxs</code>(**`start_char_idx`**, **`end_char_idx`**, **`offset_mapping`**, **`qst_mask`**)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`start_char_idx`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`end_char_idx`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`offset_mapping`** : *`<class 'inspect._empty'>`*\n",
       "\n",
       " - **`qst_mask`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(find_answer_token_idxs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "With version 2.0.0 of blurr, we include a `Preprocessor` for question answering that can either truncate texts or else chunk long documents into multiple examples.\n",
    "\n",
    "**Important**: Unlike other NLP tasks in BLURR, extractive question answering ***requires*** preprocessing in order to convert our raw start/end character indices into start/end token indices to be used as our labels. We are therefore precluded from figuring them out at batch time since they must be specified as the targets we want to predict.\n",
    "\n",
    "In addition to returning the appropriate start/end token indices for each answer, the preprocessing method here also returns the `input_ids` which ***must*** be used when chunking long documents, and in general, should be used in all cases for consistency sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QuestionAnsweringPreprocessor(Preprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # The unique identifier in the dataset. If not specified and \"return_overflowing_tokens\": True, an \"_id\" attribute\n",
    "        # will be added to your dataset with its value a unique, sequential integer, assigned to each record\n",
    "        id_attr: Optional[str] = None,\n",
    "        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')\n",
    "        ctx_attr: str = \"context\",\n",
    "        # The attribute in your dataset that contains the question being asked (default: 'question')\n",
    "        qst_attr: str = \"question\",\n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        ans_attr: str = \"answer_text\",\n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        ans_start_char_idx: str = \"ans_start_char_idx\",\n",
    "        # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "        ans_end_char_idx: str = \"ans_end_char_idx\",\n",
    "        # The attribute that should be created if your are processing individual training and validation\n",
    "        # datasets into a single dataset, and will indicate to which each example is associated\n",
    "        is_valid_attr: Optional[str] = \"is_valid\",\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer (default: {\"return_overflowing_tokens\": True})\n",
    "        tok_kwargs: dict = {\"return_overflowing_tokens\": True},\n",
    "    ):\n",
    "        # these values are mandatory\n",
    "        tok_kwargs[\"return_offsets_mapping\"] = True  # allows us to map tokens -> raw characters\n",
    "        tok_kwargs[\"padding\"] = tok_kwargs.get(\"padding\", True)\n",
    "        tok_kwargs[\"return_tensors\"] = \"pt\"\n",
    "\n",
    "        # shift the question and context appropriately based on the tokenizers padding strategy\n",
    "        if hf_tokenizer.padding_side == \"right\":\n",
    "            tok_kwargs[\"truncation\"] = \"only_second\"\n",
    "            text_attrs = [qst_attr, ctx_attr]\n",
    "        else:\n",
    "            tok_kwargs[\"truncation\"] = \"only_first\"\n",
    "            text_attrs = [ctx_attr, qst_attr]\n",
    "\n",
    "        super().__init__(hf_tokenizer, batch_size, text_attrs=text_attrs, tok_kwargs=tok_kwargs)\n",
    "\n",
    "        self.id_attr = id_attr\n",
    "        self.qst_attr, self.ctx_attr = qst_attr, ctx_attr\n",
    "        self.ans_attr, self.ans_start_char_idx, self.ans_end_char_idx = ans_attr, ans_start_char_idx, ans_end_char_idx\n",
    "        self.is_valid_attr = is_valid_attr\n",
    "\n",
    "    def process_df(self, training_df: pd.DataFrame, validation_df: Optional[pd.DataFrame] = None):\n",
    "        df = super().process_df(training_df, validation_df)\n",
    "\n",
    "        # a unique Id for each example is required to properly score question answering results when chunking long\n",
    "        # documents (e.g., return_overflowing_tokens=True)\n",
    "        if self.id_attr is None and self.tok_kwargs.get(\"return_overflowing_tokens\", False):\n",
    "            df.insert(0, \"_id\", range(len(df)))\n",
    "\n",
    "        proc_data = []\n",
    "        for row_idx, row in df.iterrows():\n",
    "            # fetch data elements required to build a modelable dataset\n",
    "            inputs = self._tokenize_function(row)\n",
    "            ans_text, start_char_idx, end_char_idx = row[self.ans_attr], row[self.ans_start_char_idx], row[self.ans_end_char_idx] + 1\n",
    "\n",
    "            # if \"return_overflowing_tokens = True\", our BatchEncoding will include an \"overflow_to_sample_mapping\" list\n",
    "            overflow_mapping = inputs[\"overflow_to_sample_mapping\"] if (\"overflow_to_sample_mapping\" in inputs) else [0]\n",
    "\n",
    "            for idx in range(len(overflow_mapping)):\n",
    "                # update the targets: is_found (s[1]), answer start token index (s[2]), and answer end token index (s[3])\n",
    "                qst_mask = [i != 1 if self.hf_tokenizer.padding_side == \"right\" else i != 0 for i in inputs.sequence_ids(idx)]\n",
    "                start, end  = find_answer_token_idxs(start_char_idx, end_char_idx, inputs[\"offset_mapping\"][idx], qst_mask)\n",
    "\n",
    "                overflow_row = row.copy()\n",
    "                overflow_row[self.ans_end_char_idx] = end_char_idx\n",
    "                overflow_row[\"ans_start_token_idx\"] = start.item()\n",
    "                overflow_row[\"ans_end_token_idx\"] = end.item()\n",
    "\n",
    "                for k in inputs.keys():\n",
    "                    overflow_row[k] = inputs[k][idx].numpy()\n",
    "\n",
    "                proc_data.append(overflow_row)\n",
    "\n",
    "        return pd.DataFrame(proc_data)\n",
    "\n",
    "    def process_hf_dataset(self, training_ds: Dataset, validation_ds: Optional[Dataset] = None):\n",
    "        ds = super().process_hf_dataset(training_ds, validation_ds)\n",
    "\n",
    "        # return the pre-processed DataFrame\n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten your raw dataset (as needed) \n",
    "\n",
    "The `QuestionAnsweringPreprocessor` class requires that start/end character indicies exist in their own respective columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ans_start_char_idx</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>ans_end_char_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "      <td>207</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "\n",
       "                                                                                               context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "\n",
       "                                                     question  \\\n",
       "0                    When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was growing up?   \n",
       "\n",
       "                                                    answers  \\\n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}   \n",
       "1  {'text': ['singing and dancing'], 'answer_start': [207]}   \n",
       "\n",
       "   ans_start_char_idx          answer_text  ans_end_char_idx  \n",
       "0                 269    in the late 1990s               286  \n",
       "1                 207  singing and dancing               226  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df[\"ans_start_char_idx\"] = squad_df.answers.apply(lambda v: v[\"answer_start\"][0])\n",
    "squad_df[\"answer_text\"] = squad_df.answers.apply(lambda v: v[\"text\"][0])\n",
    "squad_df[\"ans_end_char_idx\"] = squad_df[\"ans_start_char_idx\"].astype(int) + squad_df[\"answer_text\"].str.len()\n",
    "\n",
    "print(len(squad_df))\n",
    "squad_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to preprocess your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ans_start_char_idx</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>ans_end_char_idx</th>\n",
       "      <th>ans_start_token_idx</th>\n",
       "      <th>ans_end_token_idx</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>overflow_to_sample_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>84</td>\n",
       "      <td>89</td>\n",
       "      <td>[0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>[0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 11, 2499, 6, 1184, 6, 79, 3744, 11, 1337,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 333, 1059, 65, 9, 5, 232, 18, 275, 12, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "      <td>207</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>227</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>[0, 653, 911, 222, 12674, 1755, 3511, 11, 77, 79, 21, 1197, 62, 116, 2, 2, 12674, 12695, 272, 35...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 10], [11, 14], [15, 20], [20, 22], [23, 30], [31, 33], [34, 38], [39, 42], ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "      <td>207</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>227</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>[0, 653, 911, 222, 12674, 1755, 3511, 11, 77, 79, 21, 1197, 62, 116, 2, 2, 3390, 4, 8912, 8, 117...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 10], [11, 14], [15, 20], [20, 22], [23, 30], [31, 33], [34, 38], [39, 42], ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "\n",
       "                                                                                               context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "\n",
       "                                                     question  \\\n",
       "0                    When did Beyonce start becoming popular?   \n",
       "0                    When did Beyonce start becoming popular?   \n",
       "0                    When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was growing up?   \n",
       "1  What areas did Beyonce compete in when she was growing up?   \n",
       "\n",
       "                                                    answers  \\\n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}   \n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}   \n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}   \n",
       "1  {'text': ['singing and dancing'], 'answer_start': [207]}   \n",
       "1  {'text': ['singing and dancing'], 'answer_start': [207]}   \n",
       "\n",
       "   ans_start_char_idx          answer_text  ans_end_char_idx  \\\n",
       "0                 269    in the late 1990s               287   \n",
       "0                 269    in the late 1990s               287   \n",
       "0                 269    in the late 1990s               287   \n",
       "1                 207  singing and dancing               227   \n",
       "1                 207  singing and dancing               227   \n",
       "\n",
       "   ans_start_token_idx  ans_end_token_idx  \\\n",
       "0                   84                 89   \n",
       "0                   32                 37   \n",
       "0                    0                  0   \n",
       "1                   77                 80   \n",
       "1                   30                 33   \n",
       "\n",
       "                                                                                             input_ids  \\\n",
       "0  [0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634...   \n",
       "0  [0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 11, 2499, 6, 1184, 6, 79, 3744, 11, 1337,...   \n",
       "0  [0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 333, 1059, 65, 9, 5, 232, 18, 275, 12, 11...   \n",
       "1  [0, 653, 911, 222, 12674, 1755, 3511, 11, 77, 79, 21, 1197, 62, 116, 2, 2, 12674, 12695, 272, 35...   \n",
       "1  [0, 653, 911, 222, 12674, 1755, 3511, 11, 77, 79, 21, 1197, 62, 116, 2, 2, 3390, 4, 8912, 8, 117...   \n",
       "\n",
       "                                                                                        attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                                                                        offset_mapping  \\\n",
       "0  [[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...   \n",
       "0  [[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...   \n",
       "0  [[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...   \n",
       "1  [[0, 0], [0, 4], [5, 10], [11, 14], [15, 20], [20, 22], [23, 30], [31, 33], [34, 38], [39, 42], ...   \n",
       "1  [[0, 0], [0, 4], [5, 10], [11, 14], [15, 20], [20, 22], [23, 30], [31, 33], [34, 38], [39, 42], ...   \n",
       "\n",
       "  overflow_to_sample_mapping  \n",
       "0                          0  \n",
       "0                          0  \n",
       "0                          0  \n",
       "1                          0  \n",
       "1                          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_kwargs = {\"return_overflowing_tokens\": True, \"max_length\": max_seq_len, \"stride\": 64}\n",
    "preprocessor = QuestionAnsweringPreprocessor(hf_tokenizer, id_attr=\"id\", tok_kwargs=tok_kwargs)\n",
    "proc_df = preprocessor.process_df(squad_df)\n",
    "\n",
    "print(len(proc_df))\n",
    "proc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = proc_df.sample(n=10)\n",
    "for row_idx, row in sampled_df.iterrows():\n",
    "    test_example = row\n",
    "\n",
    "    if test_example.ans_start_token_idx != 0 and test_example.ans_end_token_idx != 0:\n",
    "        test_eq(\n",
    "            test_example.answer_text,\n",
    "            hf_tokenizer.decode(test_example[\"input_ids\"][test_example.ans_start_token_idx : test_example.ans_end_token_idx]).strip(),\n",
    "        )\n",
    "    else:\n",
    "        test_eq(test_example.ans_start_token_idx, 0)\n",
    "        test_eq(test_example.ans_end_token_idx, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove texts longer than your model will hold (and include only answerable contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ans_start_char_idx</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>ans_end_char_idx</th>\n",
       "      <th>ans_start_token_idx</th>\n",
       "      <th>ans_end_token_idx</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>84</td>\n",
       "      <td>89</td>\n",
       "      <td>[0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "      <td>207</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>227</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>[0, 653, 911, 222, 12674, 1755, 3511, 11, 77, 79, 21, 1197, 62, 116, 2, 2, 12674, 12695, 272, 35...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 10], [11, 14], [15, 20], [20, 22], [23, 30], [31, 33], [34, 38], [39, 42], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>{'text': ['Houston, Texas'], 'answer_start': [166]}</td>\n",
       "      <td>166</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>181</td>\n",
       "      <td>69</td>\n",
       "      <td>72</td>\n",
       "      <td>[0, 96, 99, 343, 8, 194, 222, 12674, 1755, 1437, 1733, 62, 116, 1437, 2, 2, 12674, 12695, 272, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 2], [3, 7], [8, 12], [13, 16], [17, 22], [23, 26], [27, 32], [32, 34], [35, 35], [3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>{'text': ['late 1990s'], 'answer_start': [276]}</td>\n",
       "      <td>276</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>87</td>\n",
       "      <td>90</td>\n",
       "      <td>[0, 96, 61, 2202, 222, 12674, 1755, 555, 3395, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 2], [3, 8], [9, 15], [16, 19], [20, 25], [25, 27], [28, 34], [35, 41], [41, 42], [0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9603</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>In what R&amp;B group was she the lead singer?</td>\n",
       "      <td>{'text': ['Destiny's Child'], 'answer_start': [320]}</td>\n",
       "      <td>320</td>\n",
       "      <td>Destiny's Child</td>\n",
       "      <td>336</td>\n",
       "      <td>103</td>\n",
       "      <td>106</td>\n",
       "      <td>[0, 96, 99, 248, 947, 387, 333, 21, 79, 5, 483, 3250, 116, 2, 2, 12674, 12695, 272, 354, 6591, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 2], [3, 7], [8, 9], [9, 10], [10, 11], [12, 17], [18, 21], [22, 25], [26, 29], [30,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "1  56be85543aeaaa14008c9065  Beyoncé   \n",
       "3  56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4  56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "5  56bf6b0f3aeaaa14008c9603  Beyoncé   \n",
       "\n",
       "                                                                                               context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "5  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "\n",
       "                                                     question  \\\n",
       "0                    When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was growing up?   \n",
       "3               In what city and state did Beyonce  grow up?    \n",
       "4                  In which decade did Beyonce become famous?   \n",
       "5                  In what R&B group was she the lead singer?   \n",
       "\n",
       "                                                    answers  \\\n",
       "0    {'text': ['in the late 1990s'], 'answer_start': [269]}   \n",
       "1  {'text': ['singing and dancing'], 'answer_start': [207]}   \n",
       "3       {'text': ['Houston, Texas'], 'answer_start': [166]}   \n",
       "4           {'text': ['late 1990s'], 'answer_start': [276]}   \n",
       "5      {'text': ['Destiny's Child'], 'answer_start': [320]}   \n",
       "\n",
       "   ans_start_char_idx          answer_text  ans_end_char_idx  \\\n",
       "0                 269    in the late 1990s               287   \n",
       "1                 207  singing and dancing               227   \n",
       "3                 166       Houston, Texas               181   \n",
       "4                 276           late 1990s               287   \n",
       "5                 320      Destiny's Child               336   \n",
       "\n",
       "   ans_start_token_idx  ans_end_token_idx  \\\n",
       "0                   84                 89   \n",
       "1                   77                 80   \n",
       "3                   69                 72   \n",
       "4                   87                 90   \n",
       "5                  103                106   \n",
       "\n",
       "                                                                                             input_ids  \\\n",
       "0  [0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634...   \n",
       "1  [0, 653, 911, 222, 12674, 1755, 3511, 11, 77, 79, 21, 1197, 62, 116, 2, 2, 12674, 12695, 272, 35...   \n",
       "3  [0, 96, 99, 343, 8, 194, 222, 12674, 1755, 1437, 1733, 62, 116, 1437, 2, 2, 12674, 12695, 272, 3...   \n",
       "4  [0, 96, 61, 2202, 222, 12674, 1755, 555, 3395, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1...   \n",
       "5  [0, 96, 99, 248, 947, 387, 333, 21, 79, 5, 483, 3250, 116, 2, 2, 12674, 12695, 272, 354, 6591, 1...   \n",
       "\n",
       "                                                                                        attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                                                                        offset_mapping  \n",
       "0  [[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...  \n",
       "1  [[0, 0], [0, 4], [5, 10], [11, 14], [15, 20], [20, 22], [23, 30], [31, 33], [34, 38], [39, 42], ...  \n",
       "3  [[0, 0], [0, 2], [3, 7], [8, 12], [13, 16], [17, 22], [23, 26], [27, 32], [32, 34], [35, 35], [3...  \n",
       "4  [[0, 0], [0, 2], [3, 8], [9, 15], [16, 19], [20, 25], [25, 27], [28, 34], [35, 41], [41, 42], [0...  \n",
       "5  [[0, 0], [0, 2], [3, 7], [8, 9], [9, 10], [10, 11], [12, 17], [18, 21], [22, 25], [26, 29], [30,...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = QuestionAnsweringPreprocessor(hf_tokenizer, tok_kwargs={\"return_overflowing_tokens\": False, \"max_length\": max_seq_len})\n",
    "proc2_df = preprocessor.process_df(squad_df)\n",
    "proc2_df = proc2_df[(proc2_df.ans_end_token_idx < max_seq_len) & (proc2_df.ans_start_token_idx != 0) & (proc2_df.ans_end_token_idx != 0)]\n",
    "\n",
    "print(len(proc2_df))\n",
    "proc2_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `QuestionAnswerTextInput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QuestionAnswerTextInput(TextInput):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `QABatchTokenizeTransform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QABatchTokenizeTransform(BatchTokenizeTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # Contray to other NLP tasks where batch-time tokenization (`is_pretokenized` = False) is the default, with\n",
    "        # extractive question answering pre-processing is required, and as such we set it to True here\n",
    "        is_pretokenized: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = \"only_second\",\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs. \n",
    "        # Since extractive requires pre-tokenized input_ids, we default this to not include any \"special\" tokens as they are already\n",
    "        # included in the pre-processed input_ids\n",
    "        tok_kwargs: dict = {\"add_special_tokens\": False},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        # \"return_special_tokens_mask\" and \"return_offsets_mapping\" are mandatory for extractive QA in blurr\n",
    "        tok_kwargs = { **tok_kwargs, **{\"return_special_tokens_mask\": True, \"return_offsets_mapping\": True}}\n",
    "\n",
    "        super().__init__(\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            is_pretokenized=is_pretokenized,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            is_split_into_words=is_split_into_words,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        samples, batch_encoding = super().encodes(samples, return_batch_encoding=True)\n",
    "\n",
    "        for idx, s in enumerate(samples):\n",
    "            # cls_index: location of CLS token (used by xlnet and xlm); is a list.index(value) for pytorch tensor's\n",
    "            s[0][\"cls_index\"] = (s[0][\"input_ids\"] == self.hf_tokenizer.cls_token_id).nonzero()[0]\n",
    "            # p_mask: mask with 1 for token than cannot be in the answer, else 0 (used by xlnet and xlm)\n",
    "            s[0][\"p_mask\"] = s[0][\"special_tokens_mask\"]\n",
    "\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "The following eamples demonstrate several approaches to construct your `DataBlock` for question answering tasks using the mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Get your Hugging Face objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilroberta-base\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=AutoModelForQuestionAnswering)\n",
    "\n",
    "max_seq_len = 128\n",
    "vocab = dict(enumerate(range(max_seq_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ans_start_char_idx</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>ans_end_char_idx</th>\n",
       "      <th>ans_start_token_idx</th>\n",
       "      <th>ans_end_token_idx</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>overflow_to_sample_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>84</td>\n",
       "      <td>89</td>\n",
       "      <td>[0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "\n",
       "                                                                                               context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "\n",
       "                                   question  \\\n",
       "0  When did Beyonce start becoming popular?   \n",
       "\n",
       "                                                  answers  ans_start_char_idx  \\\n",
       "0  {'text': ['in the late 1990s'], 'answer_start': [269]}                 269   \n",
       "\n",
       "         answer_text  ans_end_char_idx  ans_start_token_idx  \\\n",
       "0  in the late 1990s               287                   84   \n",
       "\n",
       "   ans_end_token_idx  \\\n",
       "0                 89   \n",
       "\n",
       "                                                                                             input_ids  \\\n",
       "0  [0, 520, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634...   \n",
       "\n",
       "                                                                                        attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                                                                        offset_mapping  \\\n",
       "0  [[0, 0], [0, 4], [5, 8], [9, 14], [14, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, ...   \n",
       "\n",
       "  overflow_to_sample_mapping  \n",
       "0                          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_kwargs = {\"return_overflowing_tokens\": True, \"max_length\": max_seq_len, \"stride\": 64}\n",
    "preprocessor = QuestionAnsweringPreprocessor(hf_tokenizer, id_attr=\"id\", tok_kwargs=tok_kwargs)\n",
    "proc_df = preprocessor.process_df(squad_df)\n",
    "\n",
    "proc_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 3: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = QABatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=max_seq_len)\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(batch_tokenize_tfm=before_batch_tfm, input_return_type=QuestionAnswerTextInput),\n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader('input_ids'),\n",
    "    get_y=[ColReader('ans_start_token_idx'), ColReader('ans_end_token_idx')],\n",
    "    splitter=RandomSplitter(),\n",
    "    n_inp=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 156)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(proc_df, bs=4)\n",
    "len(dls.train), len(dls.valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0]), len(b[1]), len(b[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]), torch.Size([4, 128]), torch.Size([4]), torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][\"input_ids\"].shape, b[0][\"attention_mask\"].shape, b[1].shape, b[2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `QuestionAnswerTextInput` typed inputs\n",
    "    x: QuestionAnswerTextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders, tfms=[QABatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    res = L()\n",
    "    for sample, input_ids, start, end in zip(samples, x, *y):\n",
    "        txt = hf_tokenizer.decode(sample[0], skip_special_tokens=True)[:trunc_at]\n",
    "        found = (start.item() != 0 and end.item() != 0)\n",
    "        ans_text = hf_tokenizer.decode(input_ids[start:end], skip_special_tokens=False)\n",
    "        res.append((txt, found, (start.item(), end.item()), ans_text))\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"text\", \"found\", \"start/end\", \"answer\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_batch` method above allows us to create a more interpretable view of our question/answer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>found</th>\n",
       "      <th>start/end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If Beyonce won three Grammies in 2015, how many was she nominated for? early September and also the Global Citizen Festival later that month. Beyoncé made an uncredited featured appearance on the track \"Hymn for the Weekend\" by British rock band Coldplay, on their seventh studio album A Head Full of Dreams (2015), which saw release in December. On January 7, 2016, Pepsi announced Beyoncé would perform alongside Coldplay at Super Bowl 50 in February. Knowles has previously performed at four Super Bowl shows throughout her career, serving as the main headliner of the 47th Super Bowl halftime</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What song did Beyoncé perform at the first inaugural dance for the Obamas. the Neighborhood Ball two days later. Beyoncé and Jay Z held a fundraiser at the latter's 40/40 Club in Manhattan for Obama's 2012 presidential campaign which raised $4 million. Beyoncé uploaded pictures of her paper ballot on Tumblr, confirming she had voted in support for the Democratic Party and to encourage others to do so. She also performed the American national anthem at his second inauguration, singing along with a pre-recorded track. She publicly endorsed same sex marriage on March 26, 2013, after the Supreme Court debate on California's Proposition</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What establishments did Frédéric frequently visit in Paris that influenced his career? and often technically demanding; his own performances were noted for their nuance and sensitivity. Chopin invented the concept of instrumental ballade. His major piano works also include mazurkas, waltzes, nocturnes, polonaises, études, impromptus, scherzos, preludes and sonatas, some published only after his death. Influences on his compositional style include Polish folk music, the classical tradition of J. S. Bach, Mozart and Schubert, the</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who did Beyoncé team up with in 2010 to get her fashions into Brazil?, a \"high-style\" mobile game with a social networking component, featuring the House of Deréon collection. In July 2009, Beyoncé and her mother launched a new junior apparel label, Sasha Fierce for Deréon, for back-to-school selling. The collection included sportswear, outerwear, handbags, footwear, eyewear, lingerie and jewelry. It was available at department stores including Macy's and Dillard's, and specialty stores Jimmy Jazz and Against All Odds. On</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing extra information\n",
    "\n",
    "As mentioned in the data.core module documentation, BLURR now also allows you to pass extra information alongside your inputs in the form of a dictionary.  If we are splitting long documents into chunks but want to predict/aggregation by example (rather than by chunk), we'll want to at least include a unique identifier for each example. When we look at `modeling.question_answer` module, we'll see how the question answering bits can use such an Id for this purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Get your Hugging Face objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name ='bert-large-uncased-whole-word-masking-finetuned-squad' # \"roberta-base\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=AutoModelForQuestionAnswering)\n",
    "\n",
    "max_seq_len = 128\n",
    "vocab = dict(enumerate(range(max_seq_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ans_start_char_idx</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>ans_end_char_idx</th>\n",
       "      <th>ans_start_token_idx</th>\n",
       "      <th>ans_end_token_idx</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "      <th>overflow_to_sample_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "      <td>269</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>287</td>\n",
       "      <td>75</td>\n",
       "      <td>79</td>\n",
       "      <td>[101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773, 21025, 19358, 22815, 1011, 5708, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[[0, 0], [0, 4], [5, 8], [9, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, 7], [8, 10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56be85543aeaaa14008c9063  Beyoncé   \n",
       "\n",
       "                                                                                               context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an America...   \n",
       "\n",
       "                                   question  \\\n",
       "0  When did Beyonce start becoming popular?   \n",
       "\n",
       "                                                  answers  ans_start_char_idx  \\\n",
       "0  {'text': ['in the late 1990s'], 'answer_start': [269]}                 269   \n",
       "\n",
       "         answer_text  ans_end_char_idx  ans_start_token_idx  \\\n",
       "0  in the late 1990s               287                   75   \n",
       "\n",
       "   ans_end_token_idx  \\\n",
       "0                 79   \n",
       "\n",
       "                                                                                             input_ids  \\\n",
       "0  [101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773, 21025, 19358, 22815, 1011, 5708, 10...   \n",
       "\n",
       "                                                                                        token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                                                                        attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                                                                        offset_mapping  \\\n",
       "0  [[0, 0], [0, 4], [5, 8], [9, 16], [17, 22], [23, 31], [32, 39], [39, 40], [0, 0], [0, 7], [8, 10...   \n",
       "\n",
       "  overflow_to_sample_mapping  \n",
       "0                          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = QuestionAnsweringPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    id_attr=\"id\",\n",
    "    ctx_attr=\"context\",\n",
    "    qst_attr=\"question\",\n",
    "    ans_attr=\"answer_text\",\n",
    "    ans_start_char_idx=\"ans_start_char_idx\",\n",
    "    ans_end_char_idx=\"ans_end_char_idx\",\n",
    "    tok_kwargs={\"return_overflowing_tokens\": True, \"max_length\": max_seq_len, \"stride\": 2},\n",
    ")\n",
    "proc_df = preprocessor.process_df(squad_df)\n",
    "proc_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = QABatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=max_seq_len)\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(batch_tokenize_tfm=before_batch_tfm, input_return_type=QuestionAnswerTextInput),\n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab),\n",
    ")\n",
    "\n",
    "# since its pre-tokenized, we include an \"input_ids\" key with the value being the preprocessed input_ids\n",
    "def get_x(item):\n",
    "    return {\"input_ids\": item.input_ids, \"id\": item.id}\n",
    "\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=get_x,\n",
    "    get_y=[ItemGetter(\"ans_start_token_idx\"), ItemGetter(\"ans_end_token_idx\")],\n",
    "    splitter=RandomSplitter(),\n",
    "    n_inp=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 107)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(proc_df, bs=4)\n",
    "len(dls.train), len(dls.valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 4, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0]), len(b[1]), len(b[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0], device='cuda:1')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].keys()\n",
    "b[0]['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]), torch.Size([4, 128]), torch.Size([4]), torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][\"input_ids\"].shape, b[0][\"attention_mask\"].shape, b[1].shape, b[2].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that any additional data is now located in the inputs dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56bfc281a10cfb14005512b8',\n",
       " '56ce0f42aab44d1400b88421',\n",
       " '56bed32f3aeaaa14008c94cf',\n",
       " '56d4d9a92ccc5a1400d832a6']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>found</th>\n",
       "      <th>start/end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How has L.A. Reid described her? Jim Farber of the Daily News and Stephanie Classen of Star Phoenix both praised her strong voice and her stage presence.</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On what date did Frédéric give his first performance of Piano Concerto No. 2 in F minor, Op. 21? Back in Warsaw that year, Chopin heard Niccolò Paganini play the violin, and composed a set of variations, Souvenir de Paganini. It may have been this experience which encouraged him to commence writing his first Études, (1829–32), exploring the capacities of his own instrument. On 11 August, three weeks after completing his studies at the Warsaw Conservatory, he made his debut in Vienna. He gave two piano concerts and received many</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Together how records have they sold? in recent years. Beyoncé suffered a miscarriage in 2010 or 2011, describing it as \"the saddest thing\" she had ever endured. She returned to the studio and wrote music in order to cope with the loss. In April 2011, Beyoncé and Jay Z traveled to Paris in order to shoot the album cover for her 4, and unexpectedly became pregnant in Paris.</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the lead single on Beyoncé's first album?Sung Collaboration for \"Crazy in Love\", and Best R&amp;B Performance by a Duo or Group with Vocals for \"The Closer I Get to You\" with Luther Vandross.</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for extractive Q&A tasks data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
