{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.question_answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.question_answering\n",
    "\n",
    "> This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for question/answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast\n",
    "from functools import reduce\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import DataBlock, CategoryBlock, ColReader, ColSplitter\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering, logging,\n",
    "    PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    ")\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import HF_BaseInput, HF_BeforeBatchTransform, first_blurr_tfm\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "import pdb\n",
    "\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import *\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.data.core import HF_TextBlock\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions('torch fastai transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question/Answering tokenization, batch transform, and DataBlock methods\n",
    "\n",
    "Question/Answering tasks are models that require two text inputs (a context that includes the answer and the question).  The objective is to predict the start/end tokens of the answer in the context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./')\n",
    "squad_df = pd.read_csv(path/'squad_sample.csv'); len(squad_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a simple subset of a pre-processed SQUADv2 dataset below just for demonstration purposes. There is a lot that can be done to make this much better and more fully functional.  The idea here is just to show you how things can work for tasks beyond sequence classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = AutoModelForQuestionAnswering\n",
    "\n",
    "pretrained_model_name = 'roberta-base' #'xlm-mlm-ende-1024'\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pre_process_squad(\n",
    "    # A row in your pd.DataFrame\n",
    "    row, \n",
    "    # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "    hf_arch:str,   \n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer:PreTrainedTokenizerBase,  \n",
    "    # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')\n",
    "    ctx_attr:str='context', \n",
    "    # The attribute in your dataset that contains the question being asked (default: 'question')\n",
    "    qst_attr:str='question', \n",
    "    # The attribute in your dataset that contains the actual answer (default: 'answer_text')\n",
    "    ans_attr:str='answer_text'\n",
    "):\n",
    "    context, qst, ans = row[ctx_attr], row[qst_attr], row[ans_attr]\n",
    "    \n",
    "    tok_kwargs = {}\n",
    "\n",
    "    if(hf_tokenizer.padding_side == 'right'):\n",
    "        tok_input = hf_tokenizer.convert_ids_to_tokens(hf_tokenizer.encode(qst, context, **tok_kwargs))\n",
    "    else:\n",
    "        tok_input = hf_tokenizer.convert_ids_to_tokens(hf_tokenizer.encode(context, qst, **tok_kwargs))\n",
    "                                                                       \n",
    "    tok_ans = hf_tokenizer.tokenize(str(row[ans_attr]), **tok_kwargs)\n",
    "    \n",
    "    start_idx, end_idx = 0,0\n",
    "    for idx, tok in enumerate(tok_input):\n",
    "        try:\n",
    "            if (tok == tok_ans[0] and tok_input[idx:idx + len(tok_ans)] == tok_ans): \n",
    "                start_idx, end_idx = idx, idx + len(tok_ans)\n",
    "                break\n",
    "        except: pass\n",
    "            \n",
    "    row['tokenized_input'] = tok_input\n",
    "    row['tokenized_input_len'] = len(tok_input)\n",
    "    row['tok_answer_start'] = start_idx\n",
    "    row['tok_answer_end'] = end_idx\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(pre_process_squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pre_process_squad` method is structured around how we've setup the squad DataFrame above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = squad_df.apply(partial(pre_process_squad, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = squad_df[(squad_df.tok_answer_end < max_seq_len) & (squad_df.is_impossible == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(enumerate(range(max_seq_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_QuestionAnswerInput(HF_BaseInput): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return a `HF_QuestionAnswerInput` from our custom `HF_BeforeBatchTransform` so that we can customize the show_batch/results methods for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_QABeforeBatchTransform(HF_BeforeBatchTransform):\n",
    "    \"\"\"Handles everything you need to assemble a mini-batch of inputs and targets, as well as \n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch:str,   \n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config:PretrainedConfig,   \n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer:PreTrainedTokenizerBase,  \n",
    "        # A Hugging Face model\n",
    "        hf_model:PreTrainedModel,      \n",
    "        # To control the length of the padding/truncation. It can be an integer or None, \n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no \n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length:int=None,             \n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to \n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding:Union[bool, str]=True, \n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation:Union[bool, str]=True, \n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words:bool=False, \n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs={}, \n",
    "        # Keyword arguments to apply to `HF_BeforeBatchTransform`\n",
    "        **kwargs\n",
    "    ):         \n",
    "        super().__init__(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                         max_length=max_length, padding=padding, truncation=truncation, \n",
    "                         is_split_into_words=is_split_into_words, tok_kwargs=tok_kwargs, **kwargs)\n",
    "        \n",
    "    def encodes(self, samples):  \n",
    "        samples = super().encodes(samples)\n",
    "        for s in samples:\n",
    "            # cls_index: location of CLS token (used by xlnet and xlm); is a list.index(value) for pytorch tensor's\n",
    "            s[0]['cls_index'] = (s[0]['input_ids'] == self.hf_tokenizer.cls_token_id).nonzero()[0]\n",
    "            # p_mask: mask with 1 for token than cannot be in the answer, else 0 (used by xlnet and xlm)\n",
    "            s[0]['p_mask'] = s[0]['special_tokens_mask']\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By overriding `HF_BeforeBatchTransform` we can add other inputs to each example for this particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_QABeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n",
    "                                             max_length=max_seq_len, truncation='only_second', \n",
    "                                             tok_kwargs={ 'return_special_tokens_mask': True })\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_QuestionAnswerInput), \n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: (x.question, x.context),\n",
    "                   get_y=[ColReader('tok_answer_start'), ColReader('tok_answer_end')],\n",
    "                   splitter=RandomSplitter(),\n",
    "                   n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(squad_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]), len(b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]['input_ids'].shape, b[0]['attention_mask'].shape, b[1].shape, b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `HF_QuestionAnswerInput` typed inputs\n",
    "    x:HF_QuestionAnswerInput, \n",
    "    # Your targets\n",
    "    y,              \n",
    "    # Your raw inputs/targets\n",
    "    samples,        \n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for \n",
    "    # decoding them into something understandable\n",
    "    dataloaders,    \n",
    "    # Your `show_batch` context\n",
    "    ctxs=None, \n",
    "    # The maximum number of items to show\n",
    "    max_n=6, \n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None, \n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs\n",
    "):  \n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    \n",
    "    res = L()\n",
    "    for sample, input_ids, start, end in zip(samples, x, *y):\n",
    "        txt = hf_tokenizer.decode(sample[0], skip_special_tokens=True)[:trunc_at]\n",
    "\n",
    "        ans_toks = hf_tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=False)[start:end]\n",
    "        res.append((txt, (start.item(),end.item()), hf_tokenizer.convert_tokens_to_string(ans_toks)))\n",
    "                       \n",
    "    display_df(pd.DataFrame(res, columns=['text', 'start/end', 'answer'])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_batch` method above allows us to create a more interpretable view of our question/answer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module includes all the low, mid, and high-level API bits for extractive Q&A tasks data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
