{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Various utility functions used by the blurr package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import importlib, inspect, sys, torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from enum import Enum\n",
    "from fastcore.foundation import L\n",
    "from transformers import logging, AutoConfig, AutoTokenizer\n",
    "from typing import List, Union, Type\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "import os, pdb\n",
    "\n",
    "from nbverbose.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility classes and methods\n",
    "\n",
    "The `Singleton` class and `str_to_type` methods are used in the construction of the `BLURR` instance, as well as, in other parts of the library. `print_versions` is a nicety added for developers wishing to know what versions of specific libraries are being used for documentation and troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Singleton:\n",
    "    def __init__(self, cls):\n",
    "        self._cls, self._instance = cls, None\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._instance == None: self._instance = self._cls(*args, **kwargs)\n",
    "        return self._instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Singleton` functions as python decorator.  Use this above any class to turn that class into a singleton (see [here](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html) for more info on the singleton pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Singleton\n",
    "class TestSingleton: pass\n",
    "\n",
    "a = TestSingleton()\n",
    "b = TestSingleton()\n",
    "test_eq(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def str_to_type(\n",
    "    typename:str  # The name of a type as a string\n",
    ") -> Type:        # Returns the actual type\n",
    "    \"Converts a type represented as a string to the actual class\"\n",
    "    return getattr(sys.modules[__name__], typename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"str_to_type\" class=\"doc_header\"><code>str_to_type</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>str_to_type</code>(**`typename`**:`str`)\n",
       "\n",
       "Converts a type represented as a string to the actual class\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`typename`** : *`<class 'str'>`*\t<p>The name of a type as a string</p>\n",
       "\n",
       "\n",
       "\n",
       "**Returns**:\n",
       "\t\n",
       " * *`typing.Type`*\t<p>Returns the actual type</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(str_to_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing.List\n",
      "<function test_eq at 0x7f2fce278af0>\n",
      "<__main__.Singleton object at 0x7f2fcdca5280>\n"
     ]
    }
   ],
   "source": [
    "print(str_to_type('List'))\n",
    "print(str_to_type('test_eq'))\n",
    "print(str_to_type('TestSingleton'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_versions(\n",
    "    # A string of space delimited package names or a list of package names\n",
    "    packages:Union[str, List[str]] \n",
    "):   \n",
    "    \"\"\" Prints the name and version of one or more packages in your environment\"\"\"\n",
    "    packages = packages.split(' ') if isinstance(packages, str) else packages\n",
    "\n",
    "    for item in packages:\n",
    "        item = item.strip()\n",
    "        print(f'{item}: {importlib.import_module(item).__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"print_versions\" class=\"doc_header\"><code>print_versions</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>print_versions</code>(**`packages`**:`Union`\\[`str`, `List`\\[`str`\\]\\])\n",
       "\n",
       "Prints the name and version of one or more packages in your environment\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`packages`** : *`typing.Union[str, typing.List[str]]`*\t<p>A string of space delimited package names or a list of package names</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(print_versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.7.1\n",
      "transformers: 4.9.2\n",
      "fastai: 2.5.0\n",
      "---\n",
      "torch: 1.7.1\n",
      "transformers: 4.9.2\n",
      "fastai: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "print_versions('torch transformers fastai')\n",
    "print('---')\n",
    "print_versions(['torch', 'transformers', 'fastai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlurrUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Singleton\n",
    "class BlurrUtil():\n",
    "    \"\"\"A general utility class for getting your Hugging Face objects\"\"\"\n",
    "    def __init__(self):\n",
    "        # get hf classes (tokenizers, configs, models, etc...)\n",
    "        transformer_classes = inspect.getmembers(importlib.import_module('transformers'))\n",
    "        \n",
    "        # build a df that we can query against to get various transformers objects/info\n",
    "        self._df = pd.DataFrame(transformer_classes, columns=['class_name', 'class_location'])\n",
    "        self._df = self._df[self._df.class_location.apply(lambda v: isinstance(v, type))]\n",
    "        \n",
    "        # add the module each class is included in\n",
    "        self._df['module'] = self._df.class_location.apply(lambda v: v.__module__)\n",
    "        \n",
    "        # remove class_location (don't need it anymore)\n",
    "        self._df.drop(labels=['class_location'], axis=1, inplace=True)\n",
    "        \n",
    "        # break up the module into separate cols\n",
    "        module_parts_df = self._df.module.str.split(\".\", n = -1, expand = True) \n",
    "        for i in range(len(module_parts_df.columns)):\n",
    "            self._df[f'module_part_{i}'] = module_parts_df[i]\n",
    "\n",
    "        # using module part 1, break up the functional area and arch into separate cols\n",
    "        module_part_3_df = self._df.module_part_3.str.split(\"_\", n = 1, expand = True) \n",
    "        self._df[['functional_area', 'arch']] = module_part_3_df\n",
    "        \n",
    "        # transformers >=4.5.x does \"auto\" differently; so remove it and \"utils\" from \"arch\" column\n",
    "        self._df = self._df[~self._df['arch'].isin(['auto', 'utils'])]\n",
    "        \n",
    "\n",
    "        # if functional area = modeling, pull out the task it is built for\n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.rsplit('For', \n",
    "                                                                                                 n=1, \n",
    "                                                                                                 expand=True)\n",
    "        \n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'For' + model_type_df[1].astype(str), \n",
    "                                    model_type_df[1])\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        self._df['model_task'] = self._df['model_task'].str.replace('For', '', n=1, case=True, regex=False)\n",
    "        \n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.rsplit('With', \n",
    "                                                                                                 n=1, \n",
    "                                                                                                 expand=True)\n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'With' + model_type_df[1].astype(str), \n",
    "                                    self._df[(self._df.functional_area == 'modeling')].model_task)\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        self._df['model_task'] = self._df['model_task'].str.replace('With', '', n=1, case=True, regex=False)\n",
    "        \n",
    "        # look at what we're going to remove (use to verify we're just getting rid of stuff we want too)\n",
    "        # df[~df['hf_class_type'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "        # only need these 3 functional areas for our querying purposes\n",
    "        self._df = self._df[self._df['functional_area'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "       \n",
    "    def get_tasks(\n",
    "        self, \n",
    "        arch:str=None # A transformer architecture (e.g., 'bert')\n",
    "    ):                # A list of tasks you can use\n",
    "        \"\"\"This method can be used to get a list of all tasks supported by your transformers install, or\n",
    "        just those available to a specific architecture\n",
    "        \"\"\"\n",
    "        query = ['model_task.notna()']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "\n",
    "        return sorted(self._df.query(' & '.join(query), engine='python').model_task.unique().tolist())\n",
    "    \n",
    "    def get_architectures(\n",
    "        self\n",
    "    ):            # Returns a list of architectures supported by your transformers install\n",
    "        return sorted(self._df[(self._df.arch.notna()) & \n",
    "                        (self._df.arch != None)].arch.unique().tolist())\n",
    "    \n",
    "    def get_models(\n",
    "        self, \n",
    "        arch:str=None, # A transformer architecture (e.g., 'bert')\n",
    "        task:str=None  # A transformer task (e.g., 'TokenClassification')\n",
    "    ):\n",
    "        \"\"\"The transformer models available for use (optional: by architecture | task)\"\"\"\n",
    "        query = ['functional_area == \"modeling\"']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "        if (task): query.append(f'model_task == \"{task}\"')\n",
    "\n",
    "        models = sorted(self._df.query(' & '.join(query)).class_name.tolist())\n",
    "        return models\n",
    "    \n",
    "    def get_model_architecture(\n",
    "        self, \n",
    "        model_name_or_enum\n",
    "    ):\n",
    "        \"\"\"Get the architecture for a given model name / enum\"\"\"\n",
    "        model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "        return self._df[self._df.class_name == model_name].arch.values[0]\n",
    "    \n",
    "    def get_hf_objects(\n",
    "        self, \n",
    "        # The name or path of the pretrained model\n",
    "        pretrained_model_name_or_path, \n",
    "        # The model class you want to use (e.g., AutoModelFor<task>)\n",
    "        model_cls,       \n",
    "        # A specific configuration instance you want to use. If None, a configuration object will be instantiated\n",
    "        # using the AutoConfig class along with any supplied `config_kwargs`\n",
    "        config=None,      \n",
    "        # A specific tokenizer instance you want to use.\n",
    "        tokenizer_cls=None,         \n",
    "        # Any kwargs you want to pass to the `AutoConfig` (only used if you do NOT pass int a config above)\n",
    "        config_kwargs={},  \n",
    "        # Any kwargs you want to pass in the creation of your tokenizer\n",
    "        tokenizer_kwargs={},    \n",
    "        # Any kwargs you want to pass in the creation of your model\n",
    "        model_kwargs={},               \n",
    "         # If you want to change the location Hugging Face objects are cached\n",
    "        cache_dir=None                \n",
    "    ):   # A tuple (architecture (str), config (obj), tokenizer (obj), and model (obj) \n",
    "        \"\"\"Given at minimum a `pretrained_model_name_or_path` and `model_cls (such as \n",
    "        `AutoModelForSequenceClassification\"), this method returns all the Hugging Face objects you need to train\n",
    "        a model using Blurr\n",
    "        \"\"\"\n",
    "        # config\n",
    "        if (config is None):\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                cache_dir=cache_dir, \n",
    "                                                **config_kwargs)\n",
    "        \n",
    "        # tokenizer (gpt2, roberta, bart (and maybe others) tokenizers require a prefix space)\n",
    "        if (any(s in pretrained_model_name_or_path for s in ['gpt2', 'roberta', 'bart', 'longformer'])):\n",
    "            tokenizer_kwargs = { **{'add_prefix_space': True}, **tokenizer_kwargs }\n",
    "\n",
    "        if (tokenizer_cls is None):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                      cache_dir=cache_dir, \n",
    "                                                      **tokenizer_kwargs)\n",
    "        else:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                      cache_dir=cache_dir, \n",
    "                                                      **tokenizer_kwargs)\n",
    "            \n",
    "        # model        \n",
    "        model = model_cls.from_pretrained(pretrained_model_name_or_path, \n",
    "                                          config=config, \n",
    "                                          cache_dir=cache_dir, \n",
    "                                          **model_kwargs)\n",
    "\n",
    "        #arch\n",
    "        arch = self.get_model_architecture(type(model).__name__)\n",
    "        \n",
    "        return (arch, config, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BlurrUtil` is a `Singleton` (there exists only one instance, and the same instance is returned upon subsequent instantiation requests).  You can get at via the `BLURR` constant below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = BlurrUtil()\n",
    "mh2 = BlurrUtil()\n",
    "test_eq(mh, mh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>module</th>\n",
       "      <th>module_part_0</th>\n",
       "      <th>module_part_1</th>\n",
       "      <th>module_part_2</th>\n",
       "      <th>module_part_3</th>\n",
       "      <th>functional_area</th>\n",
       "      <th>arch</th>\n",
       "      <th>model_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaptiveEmbedding</td>\n",
       "      <td>transformers.models.transfo_xl.modeling_transf...</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>modeling_transfo_xl</td>\n",
       "      <td>modeling</td>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AlbertConfig</td>\n",
       "      <td>transformers.models.albert.configuration_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>configuration_albert</td>\n",
       "      <td>configuration</td>\n",
       "      <td>albert</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AlbertForMaskedLM</td>\n",
       "      <td>transformers.models.albert.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>MaskedLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AlbertForMultipleChoice</td>\n",
       "      <td>transformers.models.albert.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>MultipleChoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AlbertForPreTraining</td>\n",
       "      <td>transformers.models.albert.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>PreTraining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 class_name  \\\n",
       "6         AdaptiveEmbedding   \n",
       "8              AlbertConfig   \n",
       "9         AlbertForMaskedLM   \n",
       "10  AlbertForMultipleChoice   \n",
       "11     AlbertForPreTraining   \n",
       "\n",
       "                                               module module_part_0  \\\n",
       "6   transformers.models.transfo_xl.modeling_transf...  transformers   \n",
       "8     transformers.models.albert.configuration_albert  transformers   \n",
       "9          transformers.models.albert.modeling_albert  transformers   \n",
       "10         transformers.models.albert.modeling_albert  transformers   \n",
       "11         transformers.models.albert.modeling_albert  transformers   \n",
       "\n",
       "   module_part_1 module_part_2         module_part_3 functional_area  \\\n",
       "6         models    transfo_xl   modeling_transfo_xl        modeling   \n",
       "8         models        albert  configuration_albert   configuration   \n",
       "9         models        albert       modeling_albert        modeling   \n",
       "10        models        albert       modeling_albert        modeling   \n",
       "11        models        albert       modeling_albert        modeling   \n",
       "\n",
       "          arch      model_task  \n",
       "6   transfo_xl            None  \n",
       "8       albert             NaN  \n",
       "9       albert        MaskedLM  \n",
       "10      albert  MultipleChoice  \n",
       "11      albert     PreTraining  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "display(mh._df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>module</th>\n",
       "      <th>module_part_0</th>\n",
       "      <th>module_part_1</th>\n",
       "      <th>module_part_2</th>\n",
       "      <th>module_part_3</th>\n",
       "      <th>functional_area</th>\n",
       "      <th>arch</th>\n",
       "      <th>model_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaptiveEmbedding</td>\n",
       "      <td>transformers.models.transfo_xl.modeling_transf...</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>modeling_transfo_xl</td>\n",
       "      <td>modeling</td>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AlbertConfig</td>\n",
       "      <td>transformers.models.albert.configuration_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>configuration_albert</td>\n",
       "      <td>configuration</td>\n",
       "      <td>albert</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AlbertForMaskedLM</td>\n",
       "      <td>transformers.models.albert.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>MaskedLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AlbertForMultipleChoice</td>\n",
       "      <td>transformers.models.albert.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>MultipleChoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AlbertForPreTraining</td>\n",
       "      <td>transformers.models.albert.modeling_albert</td>\n",
       "      <td>transformers</td>\n",
       "      <td>models</td>\n",
       "      <td>albert</td>\n",
       "      <td>modeling_albert</td>\n",
       "      <td>modeling</td>\n",
       "      <td>albert</td>\n",
       "      <td>PreTraining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 class_name  \\\n",
       "6         AdaptiveEmbedding   \n",
       "8              AlbertConfig   \n",
       "9         AlbertForMaskedLM   \n",
       "10  AlbertForMultipleChoice   \n",
       "11     AlbertForPreTraining   \n",
       "\n",
       "                                               module module_part_0  \\\n",
       "6   transformers.models.transfo_xl.modeling_transf...  transformers   \n",
       "8     transformers.models.albert.configuration_albert  transformers   \n",
       "9          transformers.models.albert.modeling_albert  transformers   \n",
       "10         transformers.models.albert.modeling_albert  transformers   \n",
       "11         transformers.models.albert.modeling_albert  transformers   \n",
       "\n",
       "   module_part_1 module_part_2         module_part_3 functional_area  \\\n",
       "6         models    transfo_xl   modeling_transfo_xl        modeling   \n",
       "8         models        albert  configuration_albert   configuration   \n",
       "9         models        albert       modeling_albert        modeling   \n",
       "10        models        albert       modeling_albert        modeling   \n",
       "11        models        albert       modeling_albert        modeling   \n",
       "\n",
       "          arch      model_task  \n",
       "6   transfo_xl            None  \n",
       "8       albert             NaN  \n",
       "9       albert        MaskedLM  \n",
       "10      albert  MultipleChoice  \n",
       "11      albert     PreTraining  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, nan, 'MaskedLM', 'MultipleChoice', 'PreTraining', 'QuestionAnswering', 'SequenceClassification', 'TokenClassification', 'CausalLM', 'ConditionalGeneration', 'NextSentencePrediction', 'ImageClassification', 'Teacher', 'QuestionAnsweringSimple', 'LMHeadModel', 'CTC', 'EntityClassification', 'EntityPairClassification', 'EntitySpanClassification', 'Classification', 'Generation', 'LMHead', 'merLayer', 'merModel', 'merPreTrainedModel', 'RegionToPhraseAlignment', 'VisualReasoning']\n",
      "\n",
      "['modeling', 'configuration', 'tokenization']\n",
      "\n",
      "['transfo_xl', 'albert', 'albert_fast', 'bart', 'bart_fast', 'barthez', 'barthez_fast', 'bert', 'bert_generation', 'bert_japanese', 'bert_fast', 'bertweet', 'big_bird', 'bigbird_pegasus', 'big_bird_fast', 'blenderbot', 'blenderbot_small', 'byt5', 'clip', 'clip_fast', 'ctrl', 'camembert', 'camembert_fast', 'canine', 'convbert', 'convbert_fast', 'cpm', 'dpr', 'dpr_fast', 'deberta', 'deberta_fast', 'deberta_v2', 'deit', 'detr', 'distilbert', 'distilbert_fast', 'electra', 'electra_fast', 'encoder_decoder', 'fsmt', 'flaubert', 'funnel', 'funnel_fast', 'gpt2', 'gpt2_fast', 'gpt_neo', 'herbert', 'herbert_fast', 'hubert', 'ibert', 'led', 'led_fast', 'layoutlm', 'layoutlm_fast', 'longformer', 'longformer_fast', 'luke', 'lxmert', 'lxmert_fast', 'm2m_100', 'mbart50', 'mbart50_fast', 'mbart', 'mbart_fast', 'mmbt', 'mpnet', 'mpnet_fast', 'mt5', 't5', 't5_fast', 'marian', 'megatron_bert', 'mobilebert', 'mobilebert_fast', 'openai', 'openai_fast', 'pegasus', 'pegasus_fast', 'phobert', 'prophetnet', 'rag', 'reformer', 'reformer_fast', 'retribert', 'retribert_fast', 'roformer', 'roformer_fast', 'roberta', 'roberta_fast', 'speech_to_text', 'squeezebert', 'squeezebert_fast', 'tapas', 'vit', 'visual_bert', 'wav2vec2', 'xlm', 'xlm_prophetnet', 'xlm_roberta', 'xlm_roberta_fast', 'xlnet', 'xlnet_fast']\n",
      "\n",
      "['modeling_transfo_xl', 'configuration_albert', 'modeling_albert', 'tokenization_albert', 'tokenization_albert_fast', 'configuration_bart', 'modeling_bart', 'tokenization_bart', 'tokenization_bart_fast', 'tokenization_barthez', 'tokenization_barthez_fast', 'tokenization_bert', 'configuration_bert', 'modeling_bert', 'configuration_bert_generation', 'modeling_bert_generation', 'tokenization_bert_generation', 'tokenization_bert_japanese', 'tokenization_bert_fast', 'tokenization_bertweet', 'configuration_big_bird', 'modeling_big_bird', 'configuration_bigbird_pegasus', 'modeling_bigbird_pegasus', 'tokenization_big_bird', 'tokenization_big_bird_fast', 'configuration_blenderbot', 'modeling_blenderbot', 'configuration_blenderbot_small', 'modeling_blenderbot_small', 'tokenization_blenderbot_small', 'tokenization_blenderbot', 'tokenization_byt5', 'configuration_clip', 'modeling_clip', 'tokenization_clip', 'tokenization_clip_fast', 'configuration_ctrl', 'modeling_ctrl', 'tokenization_ctrl', 'configuration_camembert', 'modeling_camembert', 'tokenization_camembert', 'tokenization_camembert_fast', 'configuration_canine', 'modeling_canine', 'tokenization_canine', 'configuration_convbert', 'modeling_convbert', 'tokenization_convbert', 'tokenization_convbert_fast', 'tokenization_cpm', 'configuration_dpr', 'modeling_dpr', 'tokenization_dpr', 'tokenization_dpr_fast', 'configuration_deberta', 'modeling_deberta', 'tokenization_deberta', 'tokenization_deberta_fast', 'configuration_deberta_v2', 'modeling_deberta_v2', 'tokenization_deberta_v2', 'configuration_deit', 'modeling_deit', 'configuration_detr', 'configuration_distilbert', 'modeling_distilbert', 'tokenization_distilbert', 'tokenization_distilbert_fast', 'configuration_electra', 'modeling_electra', 'tokenization_electra', 'tokenization_electra_fast', 'configuration_encoder_decoder', 'modeling_encoder_decoder', 'configuration_fsmt', 'modeling_fsmt', 'tokenization_fsmt', 'configuration_flaubert', 'modeling_flaubert', 'tokenization_flaubert', 'modeling_funnel', 'configuration_funnel', 'tokenization_funnel', 'tokenization_funnel_fast', 'configuration_gpt2', 'modeling_gpt2', 'tokenization_gpt2', 'tokenization_gpt2_fast', 'configuration_gpt_neo', 'modeling_gpt_neo', 'tokenization_herbert', 'tokenization_herbert_fast', 'configuration_hubert', 'modeling_hubert', 'configuration_ibert', 'modeling_ibert', 'configuration_led', 'modeling_led', 'tokenization_led', 'tokenization_led_fast', 'configuration_layoutlm', 'modeling_layoutlm', 'tokenization_layoutlm', 'tokenization_layoutlm_fast', 'configuration_longformer', 'modeling_longformer', 'tokenization_longformer', 'tokenization_longformer_fast', 'configuration_luke', 'modeling_luke', 'tokenization_luke', 'configuration_lxmert', 'modeling_lxmert', 'tokenization_lxmert', 'tokenization_lxmert_fast', 'configuration_m2m_100', 'modeling_m2m_100', 'tokenization_m2m_100', 'tokenization_mbart50', 'tokenization_mbart50_fast', 'configuration_mbart', 'modeling_mbart', 'tokenization_mbart', 'tokenization_mbart_fast', 'configuration_mmbt', 'modeling_mmbt', 'configuration_mpnet', 'modeling_mpnet', 'tokenization_mpnet', 'tokenization_mpnet_fast', 'configuration_mt5', 'modeling_mt5', 'tokenization_t5', 'tokenization_t5_fast', 'configuration_marian', 'modeling_marian', 'tokenization_marian', 'configuration_megatron_bert', 'modeling_megatron_bert', 'configuration_mobilebert', 'modeling_mobilebert', 'tokenization_mobilebert', 'tokenization_mobilebert_fast', 'configuration_openai', 'modeling_openai', 'tokenization_openai', 'tokenization_openai_fast', 'configuration_pegasus', 'modeling_pegasus', 'tokenization_pegasus', 'tokenization_pegasus_fast', 'tokenization_phobert', 'configuration_prophetnet', 'modeling_prophetnet', 'tokenization_prophetnet', 'configuration_rag', 'modeling_rag', 'tokenization_rag', 'modeling_reformer', 'configuration_reformer', 'tokenization_reformer', 'tokenization_reformer_fast', 'configuration_retribert', 'modeling_retribert', 'tokenization_retribert', 'tokenization_retribert_fast', 'configuration_roformer', 'modeling_roformer', 'tokenization_roformer', 'tokenization_roformer_fast', 'configuration_roberta', 'modeling_roberta', 'tokenization_roberta', 'tokenization_roberta_fast', 'configuration_speech_to_text', 'modeling_speech_to_text', 'tokenization_speech_to_text', 'configuration_squeezebert', 'modeling_squeezebert', 'tokenization_squeezebert', 'tokenization_squeezebert_fast', 'configuration_t5', 'modeling_t5', 'configuration_tapas', 'modeling_tapas', 'tokenization_tapas', 'configuration_transfo_xl', 'tokenization_transfo_xl', 'configuration_vit', 'modeling_vit', 'configuration_visual_bert', 'modeling_visual_bert', 'tokenization_wav2vec2', 'configuration_wav2vec2', 'modeling_wav2vec2', 'configuration_xlm', 'modeling_xlm', 'configuration_xlm_prophetnet', 'modeling_xlm_prophetnet', 'tokenization_xlm_prophetnet', 'configuration_xlm_roberta', 'modeling_xlm_roberta', 'tokenization_xlm_roberta', 'tokenization_xlm_roberta_fast', 'tokenization_xlm', 'configuration_xlnet', 'modeling_xlnet', 'tokenization_xlnet', 'tokenization_xlnet_fast']\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "display(mh._df.head())\n",
    "\n",
    "print(list(mh._df.model_task.unique()))\n",
    "print('')\n",
    "print(list(mh._df.functional_area.unique()))\n",
    "print('')\n",
    "print(list(mh._df.arch.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_3.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide global helper constant\n",
    "\n",
    "Users of this library can simply use `BLURR` to access all the `BlurrUtil` capabilities without having to fetch an instance themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BLURR = BlurrUtil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can get at the core Hugging Face objects you need to work with ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the ***task***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BlurrUtil.get_tasks\" class=\"doc_header\"><code>BlurrUtil.get_tasks</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BlurrUtil.get_tasks</code>(**`arch`**:`str`=*`None`*)\n",
       "\n",
       "This method can be used to get a list of all tasks supported by your transformers install, or\n",
       "just those available to a specific architecture\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`arch`** : *`<class 'str'>`*, *optional*\t<p>A transformer architecture (e.g., 'bert')</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BlurrUtil(BlurrUtil).get_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CTC', 'CausalLM', 'Classification', 'ConditionalGeneration', 'EntityClassification', 'EntityPairClassification', 'EntitySpanClassification', 'Generation', 'ImageClassification', 'LMHead', 'LMHeadModel', 'MaskedLM', 'MultipleChoice', 'NextSentencePrediction', 'PreTraining', 'QuestionAnswering', 'QuestionAnsweringSimple', 'RegionToPhraseAlignment', 'SequenceClassification', 'Teacher', 'TokenClassification', 'VisualReasoning', 'merLayer', 'merModel', 'merPreTrainedModel']\n",
      "\n",
      "['CausalLM', 'ConditionalGeneration', 'QuestionAnswering', 'SequenceClassification']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR.get_tasks())\n",
    "print('')\n",
    "print(BLURR.get_tasks('bart'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the ***architecture***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BlurrUtil.get_architectures\" class=\"doc_header\"><code>BlurrUtil.get_architectures</code><a href=\"__main__.py#L72\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BlurrUtil.get_architectures</code>()\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BlurrUtil(BlurrUtil).get_architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albert', 'albert_fast', 'bart', 'bart_fast', 'barthez', 'barthez_fast', 'bert', 'bert_fast', 'bert_generation', 'bert_japanese', 'bertweet', 'big_bird', 'big_bird_fast', 'bigbird_pegasus', 'blenderbot', 'blenderbot_small', 'byt5', 'camembert', 'camembert_fast', 'canine', 'clip', 'clip_fast', 'convbert', 'convbert_fast', 'cpm', 'ctrl', 'deberta', 'deberta_fast', 'deberta_v2', 'deit', 'detr', 'distilbert', 'distilbert_fast', 'dpr', 'dpr_fast', 'electra', 'electra_fast', 'encoder_decoder', 'flaubert', 'fsmt', 'funnel', 'funnel_fast', 'gpt2', 'gpt2_fast', 'gpt_neo', 'herbert', 'herbert_fast', 'hubert', 'ibert', 'layoutlm', 'layoutlm_fast', 'led', 'led_fast', 'longformer', 'longformer_fast', 'luke', 'lxmert', 'lxmert_fast', 'm2m_100', 'marian', 'mbart', 'mbart50', 'mbart50_fast', 'mbart_fast', 'megatron_bert', 'mmbt', 'mobilebert', 'mobilebert_fast', 'mpnet', 'mpnet_fast', 'mt5', 'openai', 'openai_fast', 'pegasus', 'pegasus_fast', 'phobert', 'prophetnet', 'rag', 'reformer', 'reformer_fast', 'retribert', 'retribert_fast', 'roberta', 'roberta_fast', 'roformer', 'roformer_fast', 'speech_to_text', 'squeezebert', 'squeezebert_fast', 't5', 't5_fast', 'tapas', 'transfo_xl', 'visual_bert', 'vit', 'wav2vec2', 'xlm', 'xlm_prophetnet', 'xlm_roberta', 'xlm_roberta_fast', 'xlnet', 'xlnet_fast']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR.get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BlurrUtil.get_model_architecture\" class=\"doc_header\"><code>BlurrUtil.get_model_architecture</code><a href=\"__main__.py#L91\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BlurrUtil.get_model_architecture</code>(**`model_name_or_enum`**)\n",
       "\n",
       "Get the architecture for a given model name / enum\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`model_name_or_enum`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BlurrUtil(BlurrUtil).get_model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n"
     ]
    }
   ],
   "source": [
    "print(BLURR.get_model_architecture('RobertaForSequenceClassification'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and lastly the ***models*** (optionally for a given task and/or architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BlurrUtil.get_models\" class=\"doc_header\"><code>BlurrUtil.get_models</code><a href=\"__main__.py#L78\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BlurrUtil.get_models</code>(**`arch`**:`str`=*`None`*, **`task`**:`str`=*`None`*)\n",
       "\n",
       "The transformer models available for use (optional: by architecture | task)\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`arch`** : *`<class 'str'>`*, *optional*\t<p>A transformer architecture (e.g., 'bert')</p>\n",
       "\n",
       "\n",
       " - **`task`** : *`<class 'str'>`*, *optional*\t<p>A transformer task (e.g., 'TokenClassification')</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BlurrUtil(BlurrUtil).get_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AdaptiveEmbedding', 'AlbertForMaskedLM', 'AlbertForMultipleChoice', 'AlbertForPreTraining', 'AlbertForQuestionAnswering']\n"
     ]
    }
   ],
   "source": [
    "print(L(BLURR.get_models())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BertForMaskedLM', 'BertForMultipleChoice', 'BertForNextSentencePrediction', 'BertForPreTraining', 'BertForQuestionAnswering']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR.get_models(arch='bert')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR.get_models(task='TokenClassification')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BertForTokenClassification']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR.get_models(arch='bert', task='TokenClassification'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some helpful enums to make it easier to get at the *task and architecture* you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_TASKS = Enum('HF_TASKS_ALL', BLURR.get_tasks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- all tasks ---\n",
      "[<HF_TASKS_ALL.CTC: 1>, <HF_TASKS_ALL.CausalLM: 2>, <HF_TASKS_ALL.Classification: 3>, <HF_TASKS_ALL.ConditionalGeneration: 4>, <HF_TASKS_ALL.EntityClassification: 5>, <HF_TASKS_ALL.EntityPairClassification: 6>, <HF_TASKS_ALL.EntitySpanClassification: 7>, <HF_TASKS_ALL.Generation: 8>, <HF_TASKS_ALL.ImageClassification: 9>, <HF_TASKS_ALL.LMHead: 10>, <HF_TASKS_ALL.LMHeadModel: 11>, <HF_TASKS_ALL.MaskedLM: 12>, <HF_TASKS_ALL.MultipleChoice: 13>, <HF_TASKS_ALL.NextSentencePrediction: 14>, <HF_TASKS_ALL.PreTraining: 15>, <HF_TASKS_ALL.QuestionAnswering: 16>, <HF_TASKS_ALL.QuestionAnsweringSimple: 17>, <HF_TASKS_ALL.RegionToPhraseAlignment: 18>, <HF_TASKS_ALL.SequenceClassification: 19>, <HF_TASKS_ALL.Teacher: 20>, <HF_TASKS_ALL.TokenClassification: 21>, <HF_TASKS_ALL.VisualReasoning: 22>, <HF_TASKS_ALL.merLayer: 23>, <HF_TASKS_ALL.merModel: 24>, <HF_TASKS_ALL.merPreTrainedModel: 25>]\n"
     ]
    }
   ],
   "source": [
    "print('--- all tasks ---')\n",
    "print(L(HF_TASKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HF_TASKS_ALL.Classification: 3>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_TASKS.Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_ARCHITECTURES = Enum('HF_ARCHITECTURES', BLURR.get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<HF_ARCHITECTURES.albert: 1>, <HF_ARCHITECTURES.albert_fast: 2>, <HF_ARCHITECTURES.bart: 3>, <HF_ARCHITECTURES.bart_fast: 4>, <HF_ARCHITECTURES.barthez: 5>]\n"
     ]
    }
   ],
   "source": [
    "print(L(HF_ARCHITECTURES)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get all your Hugging Face objects (arch, config, tokenizer, and model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BlurrUtil.get_hf_objects\" class=\"doc_header\"><code>BlurrUtil.get_hf_objects</code><a href=\"__main__.py#L99\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BlurrUtil.get_hf_objects</code>(**`pretrained_model_name_or_path`**, **`model_cls`**, **`config`**=*`None`*, **`tokenizer_cls`**=*`None`*, **`config_kwargs`**=*`{}`*, **`tokenizer_kwargs`**=*`{}`*, **`model_kwargs`**=*`{}`*, **`cache_dir`**=*`None`*)\n",
       "\n",
       "Given at minimum a `pretrained_model_name_or_path` and `model_cls (such as \n",
       "`AutoModelForSequenceClassification\"), this method returns all the Hugging Face objects you need to train\n",
       "a model using Blurr\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`pretrained_model_name_or_path`** : *`<class 'inspect._empty'>`*\t<p>The name or path of the pretrained model</p>\n",
       "\n",
       "\n",
       " - **`model_cls`** : *`<class 'inspect._empty'>`*\t<p>The model class you want to use (e.g., AutoModelFor<task>)</p>\n",
       "\n",
       "\n",
       " - **`config`** : *`<class 'NoneType'>`*, *optional*\t<p>A specific configuration instance you want to use. If None, a configuration object will be instantiated\n",
       "using the AutoConfig class along with any supplied `config_kwargs`</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_cls`** : *`<class 'NoneType'>`*, *optional*\t<p>A specific tokenizer instance you want to use.</p>\n",
       "\n",
       "\n",
       " - **`config_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>Any kwargs you want to pass to the `AutoConfig` (only used if you do NOT pass int a config above)</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>Any kwargs you want to pass in the creation of your tokenizer</p>\n",
       "\n",
       "\n",
       " - **`model_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>Any kwargs you want to pass in the creation of your model</p>\n",
       "\n",
       "\n",
       " - **`cache_dir`** : *`<class 'NoneType'>`*, *optional*\t<p>If you want to change the location Hugging Face objects are cached</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BlurrUtil(BlurrUtil).get_hf_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "arch, config, tokenizer, model = BLURR.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                      model_cls=AutoModelForMaskedLM)\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "arch, tokenizer, config, model = BLURR.get_hf_objects(\"fmikaelian/flaubert-base-uncased-squad\",\n",
    "                                                      model_cls=AutoModelForQuestionAnswering)\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "arch, tokenizer, config, model = BLURR.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                      config=None,\n",
    "                                                      tokenizer_cls=BertTokenizer, \n",
    "                                                      model_cls=BertForNextSentencePrediction)\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer)) \n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `BLURR` object is optional, as you're free to build your Hugging Face objects manually which sometimes may be required given your use case.  Most of the time, however, `BLURR` is sufficent to get you everything you need in one-line with a pretrained model name or path and task specific transformer model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
