{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import TextBlock, BlurrDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.modeling.core import PreCalculatedLoss, Blearner\n",
    "from blurr.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    TokenClassTextInput,\n",
    "    TokenTensorCategory,\n",
    "    TokenCategorize,\n",
    "    TokenCategoryBlock,\n",
    "    TokenClassBatchTokenizeTransform,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import BaseModelWrapper, BaseModelCallback, PreCalculatedLoss, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your BLURR code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dc6d6a5f7f4676870f0e0905ff9017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Make sure you set the `config.num_labels` attribute to the number of labels your model is predicting. The model will update its last layer accordingly as la transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Boston', 'B-ORG'), (\"'s\", 'O'), ('Roger', 'B-PER'), ('Clemens', 'I-PER'), ('(', 'O'), ('7-11', 'O'), (')', 'O'), ('was', 'O'), ('one', 'O'), ('out', 'O'), ('away', 'O'), ('from', 'O'), ('his', 'O'), ('second', 'O'), ('straight', 'O'), ('shutout', 'O'), ('when', 'O'), ('pinch-hitter', 'O'), ('Matt', 'B-PER'), ('Stairs', 'I-PER'), ('tripled', 'O'), ('over', 'O'), ('the', 'O'), ('head', 'O'), ('of', 'O'), ('centre', 'O'), ('fielder', 'O'), ('Lee', 'B-PER'), ('Tinsley', 'I-PER'), ('on', 'O'), ('an', 'O'), ('0-2', 'O'), ('pitch', 'O'), ('and', 'O'), ('pinch-hitter', 'O'), ('Terry', 'B-PER'), ('Steinbach', 'I-PER'), ('dunked', 'O'), ('a', 'O'), ('broken-bat', 'O'), ('single', 'O'), ('into', 'O'), ('right', 'O'), ('to', 'O'), ('lift', 'O'), ('Oakland', 'B-ORG'), ('into', 'O'), ('a', 'O'), ('1-1', 'O'), ('tie', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassMetricsCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), cbs=learn_cbs, splitter=blurr_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 156, 9]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), preds[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0013182567432522773, steep=0.0002754228771664202, valley=0.0002290867705596611, slide=0.0010000000474974513)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065136</td>\n",
       "      <td>0.043449</td>\n",
       "      <td>0.989530</td>\n",
       "      <td>0.937927</td>\n",
       "      <td>0.931568</td>\n",
       "      <td>0.934736</td>\n",
       "      <td>03:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.95      0.96      1441\n",
      "        MISC       0.85      0.84      0.85       677\n",
      "         ORG       0.92      0.91      0.91      1293\n",
      "         PER       0.98      0.98      0.98      1309\n",
      "\n",
      "   micro avg       0.94      0.93      0.93      4720\n",
      "   macro avg       0.93      0.92      0.92      4720\n",
      "weighted avg       0.94      0.93      0.93      4720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_results`\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets\n",
    "    y: TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'O'), ('do', 'O', 'O'), (\"n't\", 'O', 'O'), ('normally', 'O', 'O'), ('do', 'O', 'O'), ('this', 'O', 'O'), ('but', 'O', 'O'), ('can', 'O', 'O'), ('you', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('right-handed', 'O', 'O'), ('batsman', 'O', 'O'), ('will', 'O', 'O'), ('have', 'O', 'O'), ('to', 'O', 'O'), ('forfeit', 'O', 'O'), ('half', 'O', 'O'), ('the', 'O', 'O'), ('money', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(\"My name is Wayde and I live in San Diego\".split())\n",
    "print(res[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blurr_predict_tokens`\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer. Starting with version 2.0 of BLURR, we bring token prediction in-line with Hugging Face's token classification pipeline, both in terms of supporting the same aggregation strategies via Blurr's `TokenAggregationStrategies` class, and also the output via BLURR's `@patch`ed `Learner` method, `blurr_predict_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenAggregationStrategies():\n",
    "    \"\"\" \n",
    "    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various \n",
    "    token classication tasks (e.g, NER, POS, chunking, etc...)\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = \"O\") -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.valid_strategies = [\"simple\", \"first\", \"max\", \"average\"]\n",
    "\n",
    "    def by_token(self, tokens, input_ids, offsets, preds, probs):\n",
    "        results = []\n",
    "        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            label = self.labels[pred]\n",
    "            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            start, end = offset\n",
    "            results.append({\"entity\": label, \"score\": prob[pred], \"word\": token, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):\n",
    "        # validate `strategy_name`\n",
    "        if strategy_name not in self.valid_strategies:\n",
    "            raise ValueError(\"The 'strategy_name' is not supported by this class\")\n",
    "\n",
    "        # validate the existence of `word_ids` if the aggregation strategy = \"average\"\n",
    "        if strategy_name == \"average\" and word_ids is None:\n",
    "            raise ValueError(\"The 'average' strategy requires word_ids list\")\n",
    "\n",
    "        results = []\n",
    "        idx = 0\n",
    "        while idx < len(preds):\n",
    "            pred = preds[idx]\n",
    "            label = self.labels[pred]\n",
    "\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:\n",
    "                idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Remove the B- or I-\n",
    "            label = label[2:]\n",
    "            start, end = offsets[idx]\n",
    "\n",
    "            all_scores = []\n",
    "            all_scores.append(probs[idx][pred])\n",
    "\n",
    "            word_scores = {}\n",
    "            if strategy_name == \"average\":\n",
    "                word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "            while (\n",
    "                idx+1 < len(preds)\n",
    "                and self.labels[preds[idx+1]] == f\"I-{label}\"\n",
    "            ):\n",
    "                idx += 1\n",
    "                _, end = offsets[idx]\n",
    "\n",
    "                pred = preds[idx]\n",
    "\n",
    "                if strategy_name == \"average\":\n",
    "                    if word_ids[idx] in word_scores:\n",
    "                        word_scores[word_ids[idx]].append(probs[idx][pred])\n",
    "                    else:\n",
    "                        word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "                \n",
    "                if strategy_name != \"first\":\n",
    "                    all_scores.append(probs[idx][pred])\n",
    "\n",
    "            # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "            if strategy_name == \"average\":\n",
    "                score = np.mean([np.mean(v).item() for k,v in word_scores.items()])\n",
    "            else:\n",
    "                score = np.max(all_scores).item() if strategy_name == 'max' else np.mean(all_scores).item()\n",
    "\n",
    "            word = text[start:end]\n",
    "            results.append({\"entity_group\": label, \"score\": score, \"word\": word, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # How entities are grouped and scored\n",
    "    aggregation_strategy: str = \"simple\",\n",
    "    # The label used to idendity non-entity related words/tokens\n",
    "    non_entity_label: str = \"O\",\n",
    "    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a \n",
    "    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the \n",
    "    # equavlient of fast tokenizer's `word_ids``\n",
    "    slow_word_ids_func: Optional[Callable] = None,\n",
    "):\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)\n",
    "\n",
    "    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_offsets = inputs[\"offset_mapping\"]\n",
    "    inputs_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # run inputs through model\n",
    "    model_inputs = {k: v.to(self.model.hf_model.device) for k, v in inputs.items()}\n",
    "    outputs = self.model(model_inputs)\n",
    "\n",
    "    # fetch probabilities and predictions\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).tolist()\n",
    "    predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    # build our results\n",
    "    results = []\n",
    "    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)):\n",
    "        # build our results for the current input\n",
    "        tokens = inputs.tokens(input_idx)\n",
    "        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)\n",
    "\n",
    "        if aggregation_strategy == \"token\":\n",
    "            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))\n",
    "        else:\n",
    "            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, `List`\\[`str`\\]\\], **`aggregation_strategy`**:`str`=*`'simple'`*, **`non_entity_label`**:`str`=*`'O'`*, **`slow_word_ids_func`**:`Optional`\\[`Callable`\\]=*`None`*)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`items`** : *`typing.Union[str, typing.List[str]]`*\t<p>The str (or list of strings) you want to get token classification predictions for</p>\n",
       "\n",
       "\n",
       " - **`aggregation_strategy`** : *`<class 'str'>`*, *optional*\t<p>How entities are grouped and scored</p>\n",
       "\n",
       "\n",
       " - **`non_entity_label`** : *`<class 'str'>`*, *optional*\t<p>The label used to idendity non-entity related words/tokens</p>\n",
       "\n",
       "\n",
       " - **`slow_word_ids_func`** : *`typing.Optional[typing.Callable]`*, *optional*\t<p>If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a \n",
       "tokenizzer, example index, and a batch encoding as arguments and in turn returnes the \n",
       "equavlient of fast tokenizer's `word_ids``</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[{'entity_group': 'ORG', 'score': 0.9951454997062683, 'word': 'Bayern Munich', 'start': 0, 'end': 13}, {'entity_group': 'LOC', 'score': 0.9986792206764221, 'word': 'Germany', 'start': 34, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(\n",
    "    items=[\"My name is Wayde and I live in San Diego and using Hugging Face\", \"Bayern Munich is a soccer team in Germany\"],\n",
    "    aggregation_strategy=\"max\",\n",
    ")\n",
    "\n",
    "print(len(res))\n",
    "print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'PER', 'score': 0.9391538500785828, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.47220221161842346, 'word': 'oh', 'start': 34, 'end': 36}, {'entity_group': 'ORG', 'score': 0.5089981555938721, 'word': 'ow', 'start': 38, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9922611117362976, 'word': 'California', 'start': 56, 'end': 66}]]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9391538500785828, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.47220221161842346, 'word': 'oh', 'start': 34, 'end': 36}, {'entity_group': 'ORG', 'score': 0.5089981555938721, 'word': 'ow', 'start': 38, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9922611117362976, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'LOC', 'score': 0.9983587861061096, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.989933043718338, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9345605373382568, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9391539245843887, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.47220125794410706, 'word': 'oh', 'start': 34, 'end': 36}, {'entity_group': 'ORG', 'score': 0.5089980363845825, 'word': 'ow', 'start': 38, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9922613501548767, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'LOC', 'score': 0.9983586668968201, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9899331629276276, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9345607757568359, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "results = inf_learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls,\n",
    "        # Your raw dataset\n",
    "        data,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # if we need to preprocess the raw data before creating our DataLoaders\n",
    "        if preprocess_func:\n",
    "            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, tokens_attr, token_labels_attr, labels)\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # build getters\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            get_x = ColReader(tokens_attr)\n",
    "            get_y = ColReader(token_labels_attr)\n",
    "        else:\n",
    "            get_x = ItemGetter(tokens_attr)\n",
    "            get_y = ItemGetter(token_labels_attr)\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        blocks = (\n",
    "            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "            TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls,\n",
    "        # Your pandas DataFrame\n",
    "        df: pd.DataFrame,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            labels = sorted(list(set([lbls for sublist in df[token_labels_attr].tolist() for lbls in sublist])))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            df,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls,\n",
    "        # The path to your csv file\n",
    "        csv_file: Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict ={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        return cls.from_dataframe(\n",
    "            df,\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            preprocess_func=preprocess_func,\n",
    "            tokens_attr=tokens_attr,\n",
    "            token_labels_attr=token_labels_attr,\n",
    "            labels=labels,\n",
    "            dblock_splitter=dblock_splitter,\n",
    "            dl_kwargs=dl_kwargs,\n",
    "            learner_kwargs=learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls,\n",
    "        # A list of dictionaries\n",
    "        ds: List[Dict],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict ={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict ={}\n",
    "    ):\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            all_labels = []\n",
    "            for item in ds:\n",
    "                all_labels += item[token_labels_attr]\n",
    "            labels = sorted(list(set(all_labels)))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            ds,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_dataframe(\n",
    "    conll2003_df,\n",
    "    \"roberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dblock_splitter=RandomSplitter(),\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('But', 'O'), ('Nutricia', 'B-ORG'), ('shrugged', 'O'), ('off', 'O'), ('its', 'O'), ('ex-div', 'O'), ('tag', 'O'), ('to', 'O'), ('soar', 'O'), ('a', 'O'), ('further', 'O'), ('4.10', 'O'), ('guilders', 'O'), ('to', 'O'), ('214.40', 'O'), ('continuing', 'O'), ('its', 'O'), ('explosive', 'O'), ('rally', 'O'), ('sparked', 'O'), ('by', 'O'), ('the', 'O'), ('51', 'O'), ('percent', 'O'), ('jump', 'O'), ('in', 'O'), ('first', 'O'), ('half', 'O'), ('net', 'O'), ('profits', 'O'), ('last', 'O'), ('week', 'O'), (',', 'O'), ('which', 'O'), ('set', 'O'), ('the', 'O'), ('market', 'O'), ('alight', 'O'), ('on', 'O'), ('Friday', 'O'), (',', 'O'), ('sending', 'O'), ('the', 'O'), ('shares', 'O'), ('up', 'O'), ('18.40', 'O'), ('at', 'O'), ('210.00', 'O'), ('by', 'O'), ('the', 'O'), ('close', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.054646</td>\n",
       "      <td>0.052829</td>\n",
       "      <td>0.987834</td>\n",
       "      <td>0.933219</td>\n",
       "      <td>0.927246</td>\n",
       "      <td>0.930223</td>\n",
       "      <td>06:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Hurte', 'B-PER', 'B-PER'), ('Sierd', 'I-PER', 'I-PER'), ('Zylstra', 'I-PER', 'I-PER'), ('and', 'O', 'O'), ('his', 'O', 'O'), ('wife', 'O', 'O'), (',', 'O', 'O'), ('Jetsi', 'B-PER', 'B-PER'), ('Hendrika', 'I-PER', 'I-PER'), ('Coers', 'I-PER', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Proud', 'O', 'O'), ('of', 'O', 'O'), ('its', 'O', 'O'), ('record', 'O', 'O'), ('in', 'O', 'O'), ('promptly', 'O', 'O'), ('joining', 'O', 'O'), ('both', 'O', 'O'), ('the', 'O', 'O'), ('Council', 'B-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.96      1412\n",
      "        MISC       0.85      0.89      0.87       671\n",
      "         ORG       0.91      0.88      0.89      1330\n",
      "         PER       0.98      0.96      0.97      1274\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      4687\n",
      "   macro avg       0.92      0.92      0.92      4687\n",
      "weighted avg       0.93      0.93      0.93      4687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9054279625415802, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5879698296387991, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.998404324054718, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.8456946611404419, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'PER', 'score': 0.9973038733005524, 'word': 'Lewandowski', 'start': 39, 'end': 50}, {'entity_group': 'ORG', 'score': 0.9921272993087769, 'word': 'Bayern Munich', 'start': 77, 'end': 90}, {'entity_group': 'MISC', 'score': 0.9669867157936096, 'word': 'Bundesliga', 'start': 98, 'end': 108}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'NystromformerForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'YosoForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids \n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test           \n",
    "    \"google/mobilebert-uncased\",\n",
    "    'google/rembert',\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",                 \n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c9b27516154e38a758e367296eeaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-ORG'), ('we', 'O', 'B-ORG'), ('have', 'O', 'B-PER'), ('no', 'O', 'B-PER'), ('doubt', 'O', 'B-ORG'), ('that', 'O', 'I-ORG'), ('this', 'O', 'I-MISC'), ('is', 'O', 'B-PER'), ('one', 'O', 'I-MISC'), ('of', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('showers', 'O', 'B-PER'), ('and', 'O', 'B-ORG'), ('rain', 'O', 'I-MISC'), ('0.25-1.00', 'O', 'B-MISC'), ('inch', 'O', 'O'), ('(', 'O', 'I-LOC'), ('6-25', 'O', 'B-LOC'), ('mm', 'O', 'I-ORG'), (')', 'O', 'O'), ('and', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'O'), ('in', 'O', 'O'), ('slough,', 'B-ORG', 'B-PER'), ('which', 'O', 'O'), ('earlier', 'O', 'O'), ('announced', 'O', 'I-ORG'), ('a', 'O', 'I-PER'), ('14', 'O', 'O'), ('percent', 'O', 'I-ORG'), ('rise', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('-', 'O', 'I-ORG'), ('president', 'O', 'I-ORG'), ('dos', 'B-PER', 'O'), ('santos', 'I-PER', 'B-MISC'), ('proposes', 'O', 'I-ORG'), ('the', 'O', 'I-LOC'), ('establishment', 'O', 'I-ORG'), ('by', 'O', 'I-ORG'), ('un', 'B-ORG', 'I-ORG'), ('security', 'I-ORG', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Squad', 'O', 'I-PER'), (':', 'O', 'I-PER'), ('Alan', 'B-PER', 'I-PER'), ('Kelly', 'I-PER', 'I-PER'), (',', 'O', 'I-PER'), ('Shay', 'B-PER', 'I-PER'), ('Given', 'I-PER', 'I-PER'), (',', 'O', 'I-PER'), ('Denis', 'B-PER', 'I-PER'), ('Irwin', 'I-PER', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-PER'), ('We', 'O', 'I-PER'), ('are', 'O', 'I-PER'), ('in', 'O', 'I-PER'), ('the', 'O', 'I-ORG'), ('late', 'O', 'I-PER'), ('stages', 'O', 'I-MISC'), ('of', 'O', 'I-PER'), ('the', 'O', 'I-ORG'), ('weaker', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'I-LOC'), ('though', 'O', 'O'), ('more', 'O', 'O'), ('than', 'O', 'O'), ('40', 'O', 'I-LOC'), ('million', 'O', 'O'), ('copies', 'O', 'O'), ('of', 'O', 'B-MISC'), ('Windows', 'B-MISC', 'O'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'I-LOC'), ('New', 'B-LOC', 'O'), ('York', 'I-LOC', 'O'), (',', 'O', 'I-LOC'), ('Garret', 'B-PER', 'I-LOC'), ('Anderson', 'I-PER', 'O'), ('and', 'O', 'I-LOC'), ('Gary', 'B-PER', 'I-MISC'), ('DiSarcina', 'I-PER', 'I-LOC'), ('drove', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('squad', 'O', 'O'), (':', 'O', 'O'), ('alan', 'B-PER', 'O'), ('kelly,', 'I-PER', 'O'), ('shay', 'B-PER', 'O'), ('given,', 'I-PER', 'O'), ('denis', 'B-PER', 'O'), ('irwin,', 'I-PER', 'O'), ('phil', 'B-PER', 'O'), ('babb,', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('even', 'O', 'O'), ('though', 'O', 'O'), ('more', 'O', 'O'), ('than', 'O', 'O'), ('40', 'O', 'O'), ('million', 'O', 'O'), ('copies', 'O', 'O'), ('of', 'O', 'O'), ('windows', 'B-MISC', 'O'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-ORG'), ('I', 'O', 'I-ORG'), ('do', 'O', 'B-MISC'), (\"n't\", 'O', 'B-ORG'), ('normally', 'O', 'I-LOC'), ('do', 'O', 'B-MISC'), ('this', 'O', 'I-ORG'), ('but', 'O', 'B-ORG'), ('can', 'O', 'B-LOC'), ('you', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'O'), ('the', 'O', 'B-PER'), ('central', 'O', 'I-MISC'), ('bank', 'O', 'B-PER'), (\"'s\", 'O', 'B-ORG'), ('latest', 'O', 'O'), ('bid', 'O', 'B-LOC'), ('to', 'O', 'I-ORG'), ('jumpstart', 'O', 'I-MISC'), ('Taiwan', 'B-LOC', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'I-MISC'), ('with', 'O', 'I-MISC'), ('the', 'O', 'B-LOC'), ('end', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('last', 'O', 'I-MISC'), ('year,', 'O', 'I-MISC'), ('when', 'O', 'I-MISC'), ('T', 'B-ORG', 'I-MISC'), ('&amp;', '[xIGNx]', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Iran', 'B-LOC', 'I-MISC'), ('has', 'O', 'B-LOC'), ('warned', 'O', 'I-MISC'), ('Germany', 'B-LOC', 'I-MISC'), ('that', 'O', 'I-MISC'), ('bilateral', 'O', 'I-MISC'), ('relations', 'O', 'I-MISC'), ('could', 'O', 'I-MISC'), ('suffer', 'O', 'I-MISC'), ('if', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'B-LOC'), ('talk', 'O', 'B-LOC'), ('-', 'O', 'B-LOC'), ('usda', 'B-ORG', 'B-LOC'), ('net', 'O', 'B-LOC'), ('change', 'O', 'B-LOC'), ('in', 'O', 'B-LOC'), ('weekly', 'O', 'B-LOC'), ('export', 'O', 'B-ORG'), ('commitments', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'B-LOC'), ('new', 'O', 'B-PER'), ('company', 'O', 'B-LOC'), ('is', 'O', 'B-LOC'), ('called', 'O', 'I-ORG'), ('adon', 'B-ORG', 'I-MISC'), ('gmbh', 'I-ORG', 'B-LOC'), ('and', 'O', 'B-LOC'), ('is', 'O', 'B-LOC'), ('located', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'I-MISC'), ('talk', 'O', 'B-LOC'), ('-', 'O', 'B-LOC'), ('usda', 'B-ORG', 'O'), ('net', 'O', 'B-LOC'), ('change', 'O', 'B-LOC'), ('in', 'O', 'B-LOC'), ('weekly', 'O', 'B-LOC'), ('export', 'O', 'B-LOC'), ('commitments', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('brunswijk', 'B-PER', 'O'), ('turned', 'O', 'B-LOC'), ('himself', 'O', 'B-LOC'), ('into', 'O', 'B-LOC'), ('police', 'O', 'O'), ('after', 'O', 'B-LOC'), ('freddy', 'B-PER', 'O'), ('pinas,', 'I-PER', 'O'), ('a', 'O', 'B-LOC'), ('surinamese', 'B-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('The', 'O', 'B-LOC'), ('presence', 'O', 'I-PER'), ('of', 'O', 'B-LOC'), ('Takemura', 'B-PER', 'I-PER'), (',', 'O', 'B-LOC'), ('whose', 'O', 'I-PER'), ('role', 'O', 'I-PER'), ('as', 'O', 'B-LOC'), ('finance', 'O', 'I-PER'), ('minister', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'I-PER'), ('New', 'B-LOC', 'I-PER'), ('York', 'I-LOC', 'I-PER'), (',', 'O', 'B-LOC'), ('Wally', 'B-PER', 'I-PER'), ('Whitehurst', 'I-PER', 'I-PER'), ('allowed', 'O', 'I-PER'), ('two', 'O', 'I-PER'), ('runs', 'O', 'I-PER'), ('over', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('slough,', 'B-ORG', 'I-MISC'), ('which', 'O', 'I-MISC'), ('earlier', 'O', 'I-MISC'), ('announced', 'O', 'I-MISC'), ('a', 'O', 'I-MISC'), ('14', 'O', 'B-LOC'), ('percent', 'O', 'I-MISC'), ('rise', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'I-MISC'), ('dreaded', 'O', 'B-LOC'), ('\"', 'O', 'I-MISC'), ('chupabolos', 'O', 'I-MISC'), ('\"', 'O', 'I-MISC'), ('-', 'O', 'I-MISC'), ('-', '[xIGNx]', 'I-MISC'), ('\"', 'O', 'I-MISC'), ('drunksucker', 'O', 'I-MISC'), ('\"', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'B-LOC'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'I-LOC'), ('of', 'O', 'I-LOC'), ('last', 'O', 'I-LOC'), ('year', 'O', 'O'), (',', 'O', 'O'), ('when', 'O', 'O'), ('T&amp;N', 'B-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'B-LOC'), ('think', 'O', 'O'), ('the', 'O', 'O'), ('figure', 'O', 'B-LOC'), ('of', 'O', 'O'), ('9.5', 'O', 'I-LOC'), ('percent', 'O', 'I-LOC'), ('on', 'O', 'O'), ('the', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('compared', 'O', 'O'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'O'), ('of', 'O', 'O'), ('last', 'O', 'O'), ('year,', 'O', 'O'), ('when', 'O', 'O'), ('t', 'B-ORG', 'O'), ('&amp;', '[xIGNx]', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('despite', 'O', 'O'), ('a', 'O', 'O'), ('mood', 'O', 'O'), ('of', 'O', 'O'), ('compromise', 'O', 'O'), ('in', 'O', 'O'), ('the', 'O', 'O'), ('region', 'O', 'O'), ('after', 'O', 'O'), ('some', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-MISC'), ('TALK', 'O', 'I-MISC'), ('-', 'O', 'I-MISC'), ('USDA', 'B-ORG', 'I-MISC'), ('net', 'O', 'I-MISC'), ('change', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('weekly', 'O', 'I-MISC'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Slough', 'B-ORG', 'I-MISC'), (\"'s\", 'O', 'I-ORG'), ('chairman', 'O', 'I-ORG'), ('Sir', 'O', 'I-MISC'), ('Nigel', 'B-PER', 'I-MISC'), ('Mobbs', 'I-PER', 'O'), ('added', 'O', 'I-MISC'), ('to', 'O', 'I-ORG'), ('the', 'O', 'I-MISC'), ('bullish', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'O'), ('talk', 'O', 'I-LOC'), ('-', 'O', 'B-ORG'), ('usda', 'B-ORG', 'B-LOC'), ('net', 'O', 'B-LOC'), ('change', 'O', 'B-MISC'), ('in', 'O', 'B-LOC'), ('weekly', 'O', 'O'), ('export', 'O', 'I-LOC'), ('commitments', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'B-LOC'), ('salang', 'B-LOC', 'B-LOC'), ('highway,', 'O', 'I-MISC'), (\"afghanistan's\", 'B-LOC', 'I-MISC'), ('main', 'O', 'O'), ('route', 'O', 'O'), ('to', 'O', 'B-LOC'), ('central', 'B-LOC', 'I-MISC'), ('asia,', 'I-LOC', 'I-ORG'), ('has', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'O'), ('though', 'O', 'O'), ('more', 'O', 'O'), ('than', 'O', 'O'), ('40', 'O', 'O'), ('million', 'O', 'O'), ('copies', 'O', 'B-LOC'), ('of', 'O', 'O'), ('Windows', 'B-MISC', 'B-LOC'), ('95', 'I-MISC', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Tapie', 'B-PER', 'O'), (',', 'O', 'B-LOC'), ('the', 'O', 'O'), ('target', 'O', 'O'), ('of', 'O', 'O'), ('a', 'O', 'O'), ('blizzard', 'O', 'O'), ('of', 'O', 'O'), ('legal', 'O', 'O'), ('actions', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('even', 'O', 'O'), ('though', 'O', 'B-PER'), ('more', 'O', 'B-PER'), ('than', 'O', 'B-PER'), ('40', 'O', 'B-PER'), ('million', 'O', 'B-PER'), ('copies', 'O', 'O'), ('of', 'O', 'B-PER'), ('windows', 'B-MISC', 'B-PER'), ('95', 'I-MISC', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('ieng', 'B-PER', 'B-ORG'), ('sary', 'I-PER', 'B-ORG'), ('was', 'O', 'B-ORG'), ('sentenced', 'O', 'B-ORG'), ('to', 'O', 'I-PER'), ('death', 'O', 'B-PER'), ('in', 'O', 'B-PER'), ('absentia', 'O', 'I-PER'), ('for', 'O', 'B-PER'), ('his', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('The', 'O', 'I-PER'), ('agreement', 'O', 'I-LOC'), ('resolved', 'O', 'O'), ('a', 'O', 'I-PER'), ('dispute', 'O', 'O'), ('that', 'O', 'I-PER'), ('arose', 'O', 'O'), ('in', 'O', 'O'), ('June', 'O', 'O'), ('when', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-PER'), ('Until', 'O', 'I-LOC'), ('this', 'O', 'O'), ('is', 'O', 'O'), ('cleared', 'O', 'O'), ('up', 'O', 'O'), ('by', 'O', 'O'), ('the', 'O', 'O'), ('European', 'B-ORG', 'I-LOC'), ('Union', 'I-ORG', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('in', 'O', 'O'), ('his', 'O', 'O'), ('opinion', 'O', 'O'), ('the', 'O', 'B-LOC'), ('quartering', 'O', 'O'), ('of', 'O', 'B-LOC'), ('unita', 'B-ORG', 'O'), ('forces', 'O', 'O'), ('must', 'O', 'O'), ('be', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('porto,', 'B-ORG', 'O'), ('who', 'O', 'O'), ('are', 'O', 'O'), ('fighting', 'O', 'I-MISC'), ('to', 'O', 'I-LOC'), ('take', 'O', 'I-LOC'), ('their', 'O', 'O'), ('third', 'O', 'I-LOC'), ('consecutive', 'O', 'O'), ('title', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'O'), ('TALK', 'O', 'O'), ('-', 'O', 'O'), ('USDA', 'B-ORG', 'O'), ('net', 'O', 'O'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'O'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Croft', 'B-PER', 'O'), (',', 'O', 'O'), ('who', 'O', 'O'), ('was', 'O', 'O'), ('one', 'O', 'O'), ('of', 'O', 'O'), ('the', 'O', 'O'), ('few', 'O', 'O'), ('Englishmen', 'B-MISC', 'O'), ('to', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'O'), ('TALK', 'O', 'O'), ('-', 'O', 'O'), ('USDA', 'B-ORG', 'O'), ('net', 'O', 'O'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'O'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Even', 'O', 'I-MISC'), ('though', 'O', 'I-MISC'), ('more', 'O', 'I-MISC'), ('than', 'O', 'I-MISC'), ('40', 'O', 'I-MISC'), ('million', 'O', 'I-MISC'), ('copies', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('Windows', 'B-MISC', 'I-MISC'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if 'deberta' in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz)\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[BaseModelCallback], splitter=blurr_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[ShortEpochCallback(pct=0.1, short_valid=True), TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n",
      "  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
