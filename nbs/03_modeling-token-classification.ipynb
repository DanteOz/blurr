{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, PreTrainedTokenizerBase, PreTrainedModel, logging\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import TextBlock, BlurrDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.modeling.core import PreCalculatedCrossEntropyLoss, Blearner\n",
    "from blurr.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    TokenClassTextInput,\n",
    "    TokenTensorCategory,\n",
    "    TokenCategorize,\n",
    "    TokenCategoryBlock,\n",
    "    TokenClassBatchTokenizeTransform,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import BaseModelWrapper, BaseModelCallback, PreCalculatedLoss, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your BLURR code for token classification\n",
    "\n",
    "**Note**: Make sure you set the `config.num_labels` attribute to the number of labels your model is predicting. The model will update its last layer accordingly as la transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba37d91bf6c94319b8810f63ab526dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O'), ('quake', 'O'), ('struck', 'O'), ('at', 'O'), ('11.16', 'O'), ('a.m.', 'O'), ('(', 'O'), ('1716', 'O'), ('GMT', 'B-MISC'), (')', 'O'), ('and', 'O'), ('was', 'O'), ('centred', 'O'), ('10', 'O'), ('miles', 'O'), ('(', 'O'), ('16', 'O'), ('km', 'O'), (')', 'O'), ('south', 'O'), ('of', 'O'), ('the', 'O'), ('port', 'O'), ('of', 'O'), ('Quepos', 'B-LOC'), (',', 'O'), ('which', 'O'), ('is', 'O'), ('90', 'O'), ('miles', 'O'), ('(', 'O'), ('140', 'O'), ('km', 'O'), (')', 'O'), ('south', 'O'), ('of', 'O'), ('the', 'O'), ('capital', 'O'), ('San', 'B-LOC'), ('Jose', 'I-LOC'), (',', 'O'), ('the', 'O'), ('Costa', 'B-ORG'), ('Rican', 'I-ORG'), ('Volcanic', 'I-ORG'), ('and', 'I-ORG'), ('Seismologicial', 'I-ORG'), ('Observatory', 'I-ORG'), ('said', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassMetricsCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), cbs=learn_cbs, splitter=blurr_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " transformers.modeling_outputs.TokenClassifierOutput,\n",
       " odict_keys(['loss', 'logits']))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), type(preds), preds.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 156, 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b[0][\"labels\"].shape\n",
    "preds.logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds.logits.view(-1, preds.logits.shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds.logits.view(-1, preds.logits.shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0015848932787775993, steep=5.248074739938602e-05, valley=0.00015848931798245758, slide=0.0008317637839354575)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.040407</td>\n",
       "      <td>0.054171</td>\n",
       "      <td>0.989757</td>\n",
       "      <td>0.940065</td>\n",
       "      <td>0.934585</td>\n",
       "      <td>0.937317</td>\n",
       "      <td>03:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.96      0.96      1432\n",
      "        MISC       0.84      0.88      0.86       630\n",
      "         ORG       0.93      0.90      0.92      1304\n",
      "         PER       0.97      0.97      0.97      1266\n",
      "\n",
      "   micro avg       0.94      0.93      0.94      4632\n",
      "   macro avg       0.93      0.93      0.93      4632\n",
      "weighted avg       0.94      0.93      0.94      4632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets\n",
    "    y: TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'O'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'O'), ('of', 'O', 'O'), ('last', 'O', 'O'), ('year', 'O', 'O'), (',', 'O', 'O'), ('when', 'O', 'O'), ('T&amp;N', 'B-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Brussels', 'B-LOC', 'B-LOC'), ('received', 'O', 'O'), ('5.6', 'O', 'O'), ('cm', 'O', 'O'), ('(', 'O', 'O'), ('2.24', 'O', 'O'), ('inches', 'O', 'O'), (')', 'O', 'O'), ('of', 'O', 'O'), ('water', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(\"My name is Wayde and I live in San Diego\".split())\n",
    "print(res[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer. Starting with version 2.0 of BLURR, we bring token prediction in-line with Hugging Face's token classification pipeline, both in terms of supporting the same aggregation strategies via Blurr's `TokenAggregationStrategies` class, and also the output via BLURR's `@patch`ed `Learner` method, `blurr_predict_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenAggregationStrategies:\n",
    "    \"\"\"\n",
    "    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various\n",
    "    token classication tasks (e.g, NER, POS, chunking, etc...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = \"O\") -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.valid_strategies = [\"simple\", \"first\", \"max\", \"average\"]\n",
    "\n",
    "    def by_token(self, tokens, input_ids, offsets, preds, probs):\n",
    "        results = []\n",
    "        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            label = self.labels[pred]\n",
    "            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            start, end = offset\n",
    "            results.append({\"entity\": label, \"score\": prob[pred], \"word\": token, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):\n",
    "        # validate `strategy_name`\n",
    "        if strategy_name not in self.valid_strategies:\n",
    "            raise ValueError(\"The 'strategy_name' is not supported by this class\")\n",
    "\n",
    "        # validate the existence of `word_ids` if the aggregation strategy = \"average\"\n",
    "        if strategy_name == \"average\" and word_ids is None:\n",
    "            raise ValueError(\"The 'average' strategy requires word_ids list\")\n",
    "\n",
    "        results = []\n",
    "        idx = 0\n",
    "        while idx < len(preds):\n",
    "            pred = preds[idx]\n",
    "            label = self.labels[pred]\n",
    "\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            # Remove the B- or I-\n",
    "            label = label[2:]\n",
    "            start, end = offsets[idx]\n",
    "\n",
    "            all_scores = []\n",
    "            all_scores.append(probs[idx][pred])\n",
    "\n",
    "            word_scores = {}\n",
    "            if strategy_name == \"average\":\n",
    "                word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "            while idx + 1 < len(preds) and self.labels[preds[idx + 1]] == f\"I-{label}\":\n",
    "                idx += 1\n",
    "                _, end = offsets[idx]\n",
    "\n",
    "                pred = preds[idx]\n",
    "\n",
    "                if strategy_name == \"average\":\n",
    "                    if word_ids[idx] in word_scores:\n",
    "                        word_scores[word_ids[idx]].append(probs[idx][pred])\n",
    "                    else:\n",
    "                        word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "                if strategy_name != \"first\":\n",
    "                    all_scores.append(probs[idx][pred])\n",
    "\n",
    "            # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "            if strategy_name == \"average\":\n",
    "                score = np.mean([np.mean(v).item() for k, v in word_scores.items()])\n",
    "            else:\n",
    "                score = np.max(all_scores).item() if strategy_name == \"max\" else np.mean(all_scores).item()\n",
    "\n",
    "            word = text[start:end]\n",
    "            results.append({\"entity_group\": label, \"score\": score, \"word\": word, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # How entities are grouped and scored\n",
    "    aggregation_strategy: str = \"simple\",\n",
    "    # The label used to idendity non-entity related words/tokens\n",
    "    non_entity_label: str = \"O\",\n",
    "    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
    "    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
    "    # equavlient of fast tokenizer's `word_ids``\n",
    "    slow_word_ids_func: Optional[Callable] = None,\n",
    "):\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)\n",
    "\n",
    "    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_offsets = inputs[\"offset_mapping\"]\n",
    "    inputs_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # run inputs through model\n",
    "    model_inputs = {k: v.to(self.model.hf_model.device) for k, v in inputs.items()}\n",
    "    outputs = self.model(model_inputs)\n",
    "\n",
    "    # fetch probabilities and predictions\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).tolist()\n",
    "    predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    # build our results\n",
    "    results = []\n",
    "    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(\n",
    "        zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)\n",
    "    ):\n",
    "        # build our results for the current input\n",
    "        tokens = inputs.tokens(input_idx)\n",
    "        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)\n",
    "\n",
    "        if aggregation_strategy == \"token\":\n",
    "            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))\n",
    "        else:\n",
    "            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, `List`\\[`str`\\]\\], **`aggregation_strategy`**:`str`=*`'simple'`*, **`non_entity_label`**:`str`=*`'O'`*, **`slow_word_ids_func`**:`Optional`\\[`Callable`\\]=*`None`*)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`items`** : *`typing.Union[str, typing.List[str]]`*\t<p>The str (or list of strings) you want to get token classification predictions for</p>\n",
       "\n",
       "\n",
       " - **`aggregation_strategy`** : *`<class 'str'>`*, *optional*\t<p>How entities are grouped and scored</p>\n",
       "\n",
       "\n",
       " - **`non_entity_label`** : *`<class 'str'>`*, *optional*\t<p>The label used to idendity non-entity related words/tokens</p>\n",
       "\n",
       "\n",
       " - **`slow_word_ids_func`** : *`typing.Optional[typing.Callable]`*, *optional*\t<p>If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
       "tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
       "equavlient of fast tokenizer's `word_ids``</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[{'entity_group': 'ORG', 'score': 0.993960976600647, 'word': 'Bayern Munich', 'start': 0, 'end': 13}, {'entity_group': 'LOC', 'score': 0.9982794523239136, 'word': 'Germany', 'start': 34, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(\n",
    "    items=[\"My name is Wayde and I live in San Diego and using Hugging Face\", \"Bayern Munich is a soccer team in Germany\"],\n",
    "    aggregation_strategy=\"max\",\n",
    ")\n",
    "\n",
    "print(len(res))\n",
    "print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'PER', 'score': 0.9967057108879089, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.9114477038383484, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5454980631669363, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9930922985076904, 'word': 'California', 'start': 56, 'end': 66}]]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9967057108879089, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.9114477038383484, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5454980631669363, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9930922985076904, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.9248302578926086, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'PER', 'score': 0.5330370664596558, 'word': 'id', 'start': 10, 'end': 12}, {'entity_group': 'LOC', 'score': 0.9941179752349854, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9918646514415741, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9875409007072449, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9967057108879089, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.9114476044972738, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5454985499382019, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9930922985076904, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.9248308539390564, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'PER', 'score': 0.5330378413200378, 'word': 'id', 'start': 10, 'end': 12}, {'entity_group': 'LOC', 'score': 0.9941179752349854, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9918645918369293, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9875409007072449, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "results = inf_learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls,\n",
    "        # Your raw dataset\n",
    "        data,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {},\n",
    "    ):\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # build getters\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            get_x = ColReader(tokens_attr)\n",
    "            get_y = ColReader(token_labels_attr)\n",
    "        else:\n",
    "            get_x = ItemGetter(tokens_attr)\n",
    "            get_y = ItemGetter(token_labels_attr)\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        blocks = (\n",
    "            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "            TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls,\n",
    "        # Your pandas DataFrame\n",
    "        df: pd.DataFrame,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {},\n",
    "    ):\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            labels = sorted(list(set([lbls for sublist in df[token_labels_attr].tolist() for lbls in sublist])))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            df,\n",
    "            pretrained_model_name_or_path,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls,\n",
    "        # The path to your csv file\n",
    "        csv_file: Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {},\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        return cls.from_dataframe(\n",
    "            df,\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            tokens_attr=tokens_attr,\n",
    "            token_labels_attr=token_labels_attr,\n",
    "            labels=labels,\n",
    "            dblock_splitter=dblock_splitter,\n",
    "            dl_kwargs=dl_kwargs,\n",
    "            learner_kwargs=learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls,\n",
    "        # A list of dictionaries\n",
    "        ds: List[Dict],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {},\n",
    "    ):\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            all_labels = []\n",
    "            for item in ds:\n",
    "                all_labels += item[token_labels_attr]\n",
    "            labels = sorted(list(set(all_labels)))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            ds,\n",
    "            pretrained_model_name_or_path,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your `Blearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_dataframe(\n",
    "    conll2003_df,\n",
    "    \"distilroberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dblock_splitter=RandomSplitter(),\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Germany', 'B-LOC'), (\"'s\", 'O'), ('Michael', 'B-PER'), ('Stich', 'I-PER'), (',', 'O'), ('the', 'O'), ('1991', 'O'), ('Wimbledon', 'B-MISC'), ('champion', 'O'), (',', 'O'), ('and', 'O'), ('two-time', 'O'), ('French', 'B-MISC'), ('Open', 'I-MISC'), ('winner', 'O'), ('Sergi', 'B-PER'), ('Bruguera', 'I-PER'), ('of', 'O'), ('Spain', 'B-LOC'), ('will', 'O'), ('face', 'O'), ('each', 'O'), ('other', 'O'), ('next', 'O'), ('after', 'O'), ('beating', 'O'), ('German', 'B-MISC'), ('Tommy', 'B-PER'), ('Haas', 'I-PER'), ('6-3', 'O'), ('1-6', 'O'), ('6-1', 'O'), ('7-5', 'O'), (',', 'O'), ('and', 'O'), ('Belgian', 'B-MISC'), ('Kris', 'B-PER'), ('Goossens', 'I-PER'), ('6-2', 'O'), ('6-0', 'O'), ('7-6', 'O'), ('(', 'O'), ('7-1', 'O'), (')', 'O'), (',', 'O'), ('respectively', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.056801</td>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.986724</td>\n",
       "      <td>0.929547</td>\n",
       "      <td>0.924809</td>\n",
       "      <td>0.927172</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'B-PER'), ('Cullen', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'B-PER'), ('Wilson', 'I-PER', 'I-PER'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('decree', 'O', 'O'), ('plugs', 'O', 'O'), ('a', 'O', 'O'), ('legal', 'O', 'O'), ('void', 'O', 'O'), ('in', 'O', 'O'), ('which', 'O', 'O'), ('magistrates', 'O', 'O'), ('could', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.96      0.96      1432\n",
      "        MISC       0.84      0.87      0.86       711\n",
      "         ORG       0.92      0.88      0.90      1310\n",
      "         PER       0.96      0.97      0.97      1255\n",
      "\n",
      "   micro avg       0.93      0.92      0.93      4708\n",
      "   macro avg       0.92      0.92      0.92      4708\n",
      "weighted avg       0.93      0.92      0.93      4708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.903404951095581, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'LOC', 'score': 0.9978183507919312, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.9393174350261688, 'word': 'Lewandowski', 'start': 39, 'end': 50}, {'entity_group': 'ORG', 'score': 0.9926474094390869, 'word': 'Bayern Munich', 'start': 77, 'end': 90}, {'entity_group': 'MISC', 'score': 0.9490360021591187, 'word': 'Bundesliga', 'start': 98, 'end': 108}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'NystromformerForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'YosoForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids\n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test\n",
    "    \"google/mobilebert-uncased\",\n",
    "    \"google/rembert\",\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3816789ac9b94f55894ebed7597f8de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('squad', 'O', 'I-MISC'), (':', 'O', 'I-PER'), ('alan', 'B-PER', 'I-PER'), ('kelly', 'I-PER', 'B-LOC'), (',', 'O', 'I-PER'), ('shay', 'B-PER', 'I-PER'), ('given', 'I-PER', 'I-MISC'), (',', 'O', 'I-PER'), ('denis', 'B-PER', 'I-PER'), ('irwin', 'I-PER', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('but', 'O', 'I-MISC'), ('nutricia', 'B-ORG', 'I-MISC'), ('shrugged', 'O', 'I-PER'), ('off', 'O', 'I-MISC'), ('its', 'O', 'B-MISC'), ('ex-div', 'O', 'I-MISC'), ('tag', 'O', 'I-MISC'), ('to', 'O', 'I-MISC'), ('soar', 'O', 'B-MISC'), ('a', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'O'), ('in', 'O', 'B-ORG'), ('slough,', 'B-ORG', 'O'), ('which', 'O', 'I-LOC'), ('earlier', 'O', 'I-LOC'), ('announced', 'O', 'I-LOC'), ('a', 'O', 'I-LOC'), ('14', 'O', 'B-ORG'), ('percent', 'O', 'I-LOC'), ('rise', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('mccurry', 'B-PER', 'I-LOC'), ('said', 'O', 'O'), ('that', 'O', 'I-LOC'), ('when', 'O', 'I-LOC'), ('clinton', 'B-PER', 'I-LOC'), ('delivers', 'O', 'O'), ('his', 'O', 'I-MISC'), ('acceptance', 'O', 'B-PER'), ('address', 'O', 'B-PER'), ('on', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'B-PER'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'O'), ('of', 'O', 'O'), ('last', 'O', 'B-ORG'), ('year', 'O', 'O'), (',', 'O', 'B-LOC'), ('when', 'O', 'O'), ('T&amp;N', 'B-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('According', 'O', 'B-PER'), ('to', 'O', 'I-MISC'), ('a', 'O', 'O'), ('new', 'O', 'O'), ('release', 'O', 'B-ORG'), ('from', 'O', 'O'), ('the', 'O', 'O'), ('governor', 'O', 'O'), (',', 'O', 'O'), ('Wisconsin', 'B-LOC', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'O'), ('though', 'O', 'O'), ('more', 'O', 'O'), ('than', 'O', 'O'), ('40', 'O', 'O'), ('million', 'O', 'O'), ('copies', 'O', 'O'), ('of', 'O', 'O'), ('Windows', 'B-MISC', 'O'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Blair', 'B-PER', 'O'), ('Larsen', 'I-PER', 'B-LOC'), ('or', 'O', 'O'), ('the', 'O', 'O'), ('uncapped', 'O', 'I-PER'), ('Glenn', 'B-PER', 'O'), ('Taylor', 'I-PER', 'O'), ('are', 'O', 'O'), ('on', 'O', 'O'), ('standby', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'B-MISC'), ('christian', 'B-PER', 'B-MISC'), ('cullen,', 'I-PER', 'B-MISC'), ('14', 'O', 'B-MISC'), ('-', 'O', 'O'), ('jeff', 'B-PER', 'O'), ('wilson,', 'I-PER', 'O'), ('13', 'O', 'B-MISC'), ('-', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('but', 'O', 'B-ORG'), ('nutricia', 'B-ORG', 'B-MISC'), ('shrugged', 'O', 'B-MISC'), ('off', 'O', 'B-MISC'), ('its', 'O', 'O'), ('ex', 'O', 'B-MISC'), ('-', '[xIGNx]', 'B-MISC'), ('div', '[xIGNx]', 'I-PER'), ('tag', 'O', 'B-MISC'), ('to', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'B-ORG'), ('though', 'O', 'B-ORG'), ('more', 'O', 'I-ORG'), ('than', 'O', 'I-LOC'), ('40', 'O', 'B-MISC'), ('million', 'O', 'I-PER'), ('copies', 'O', 'I-PER'), ('of', 'O', 'B-ORG'), ('Windows', 'B-MISC', 'B-ORG'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-MISC'), ('There', 'O', 'B-ORG'), ('are', 'O', 'B-MISC'), ('no', 'O', 'O'), ('parks', 'O', 'B-PER'), ('or', 'O', 'I-MISC'), ('empty', 'O', 'I-PER'), ('areas', 'O', 'I-ORG'), ('of', 'O', 'B-ORG'), ('land', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-MISC'), ('TALK', 'O', 'I-MISC'), ('-', 'O', 'I-MISC'), ('USDA', 'B-ORG', 'I-MISC'), ('net', 'O', 'I-MISC'), ('change', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('weekly', 'O', 'I-MISC'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Hurte', 'B-PER', 'I-MISC'), ('Sierd', 'I-PER', 'B-LOC'), ('Zylstra', 'I-PER', 'I-MISC'), ('and', 'O', 'I-MISC'), ('his', 'O', 'I-MISC'), ('wife,', 'O', 'I-MISC'), ('Jetsi', 'B-PER', 'I-MISC'), ('Hendrika', 'I-PER', 'B-LOC'), ('Coers,', 'I-PER', 'B-LOC'), ('both', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('compared', 'O', 'I-MISC'), ('with', 'O', 'B-MISC'), ('the', 'O', 'B-PER'), ('end', 'O', 'I-LOC'), ('of', 'O', 'B-LOC'), ('last', 'O', 'I-MISC'), ('year,', 'O', 'I-LOC'), ('when', 'O', 'I-ORG'), ('t', 'B-ORG', 'B-PER'), ('&amp;', '[xIGNx]', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('shares', 'O', 'B-ORG'), ('in', 'O', 'B-MISC'), ('slough,', 'B-ORG', 'I-PER'), ('which', 'O', 'B-MISC'), ('earlier', 'O', 'I-ORG'), ('announced', 'O', 'B-LOC'), ('a', 'O', 'B-MISC'), ('14', 'O', 'B-MISC'), ('percent', 'O', 'B-PER'), ('rise', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-PER'), ('we', 'O', 'B-PER'), ('have', 'O', 'I-PER'), ('no', 'O', 'B-LOC'), ('doubt', 'O', 'B-LOC'), ('that', 'O', 'I-PER'), ('this', 'O', 'O'), ('is', 'O', 'B-LOC'), ('one', 'O', 'O'), ('of', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'B-PER'), ('sixth', 'O', 'B-LOC'), ('-', '[xIGNx]', 'B-PER'), ('ranked', '[xIGNx]', 'B-PER'), ('ivanisevic,', 'B-PER', 'B-PER'), ('who', 'O', 'O'), ('lost', 'O', 'I-PER'), ('in', 'O', 'B-PER'), ('the', 'O', 'I-PER'), ('final', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-PER'), ('We', 'O', 'B-PER'), ('have', 'O', 'B-PER'), ('no', 'O', 'I-PER'), ('doubt', 'O', 'I-PER'), ('that', 'O', 'B-PER'), ('this', 'O', 'B-PER'), ('is', 'O', 'O'), ('one', 'O', 'B-PER'), ('of', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O', 'I-PER'), ('chain-smoking', 'O', 'B-PER'), ('former', 'O', 'B-PER'), ('paratroop', 'O', 'B-PER'), ('general', 'O', 'B-PER'), ('with', 'O', 'B-PER'), ('a', 'O', 'B-PER'), ('sharp', 'O', 'B-PER'), ('line', 'O', 'B-PER'), ('in', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('innocent', 'B-PER', 'O'), ('butare,', 'I-PER', 'O'), ('executive', 'O', 'O'), ('secretary', 'O', 'O'), ('of', 'O', 'O'), ('the', 'O', 'O'), ('rally', 'B-ORG', 'O'), ('for', 'I-ORG', 'O'), ('the', 'I-ORG', 'O'), ('return', 'I-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('blair', 'B-PER', 'O'), ('larsen', 'I-PER', 'O'), ('or', 'O', 'O'), ('the', 'O', 'O'), ('uncapped', 'O', 'O'), ('glenn', 'B-PER', 'O'), ('taylor', 'I-PER', 'O'), ('are', 'O', 'O'), ('on', 'O', 'O'), ('standby', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-MISC'), ('TALK', 'O', 'I-ORG'), ('-', 'O', 'I-MISC'), ('USDA', 'B-ORG', 'I-ORG'), ('net', 'O', 'I-ORG'), ('change', 'O', 'I-ORG'), ('in', 'O', 'I-ORG'), ('weekly', 'O', 'I-ORG'), ('export', 'O', 'O'), ('commitments', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Now', 'O', 'I-MISC'), (',', 'O', 'I-MISC'), ('U.S.', 'B-LOC', 'I-MISC'), ('District', 'O', 'I-MISC'), ('Judge', 'O', 'I-MISC'), ('Mark', 'B-PER', 'I-MISC'), ('Wolf', 'I-PER', 'I-ORG'), ('has', 'O', 'I-MISC'), ('ordered', 'O', 'O'), ('the', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-MISC'), ('i', 'O', 'I-MISC'), (\"don't\", 'O', 'B-ORG'), ('normally', '[xIGNx]', 'B-PER'), ('do', 'O', 'I-MISC'), ('this', 'O', 'B-ORG'), ('but', 'O', 'I-MISC'), ('can', 'O', 'I-MISC'), ('you', 'O', 'O'), ('please', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('on', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), (\"women's\", 'O', 'I-MISC'), ('side,', 'O', 'I-MISC'), ('second', 'O', 'I-MISC'), ('seed', 'O', 'I-MISC'), ('monica', 'B-PER', 'I-MISC'), ('seles', 'I-PER', 'I-MISC'), ('got', 'O', 'I-MISC'), ('off', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'B-PER'), ('TALK', 'O', 'B-PER'), ('-', 'O', 'B-PER'), ('USDA', 'B-ORG', 'B-PER'), ('net', 'O', 'B-PER'), ('change', 'O', 'B-PER'), ('in', 'O', 'B-PER'), ('weekly', 'O', 'B-PER'), ('export', 'O', 'B-PER'), ('commitments', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-PER'), ('We', 'O', 'B-PER'), ('are', 'O', 'B-PER'), ('in', 'O', 'B-PER'), ('the', 'O', 'B-PER'), ('late', 'O', 'B-PER'), ('stages', 'O', 'B-PER'), ('of', 'O', 'B-PER'), ('the', 'O', 'B-PER'), ('weaker', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'B-ORG'), ('in', 'O', 'B-ORG'), ('slough,', 'B-ORG', 'B-ORG'), ('which', 'O', 'B-ORG'), ('earlier', 'O', 'B-ORG'), ('announced', 'O', 'B-ORG'), ('a', 'O', 'B-ORG'), ('14', 'O', 'I-PER'), ('percent', 'O', 'B-PER'), ('rise', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('it', 'O', 'I-PER'), ('explained', 'O', 'B-MISC'), ('the', 'O', 'B-ORG'), ('\"', 'O', 'I-PER'), ('addition', 'O', 'B-ORG'), ('of', 'O', 'B-ORG'), ('a', 'O', 'B-ORG'), ('new', 'B-LOC', 'B-PER'), ('york', 'I-LOC', 'I-ORG'), ('license', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-LOC'), ('TALK', 'O', 'I-ORG'), ('-', 'O', 'I-ORG'), ('USDA', 'B-ORG', 'O'), ('net', 'O', 'I-LOC'), ('change', 'O', 'I-LOC'), ('in', 'O', 'I-LOC'), ('weekly', 'O', 'I-LOC'), ('export', 'O', 'I-LOC'), ('commitments', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Rangarajan', 'B-PER', 'B-PER'), ('said', 'O', 'I-LOC'), ('a', 'O', 'I-LOC'), ('current', 'O', 'I-LOC'), ('account', 'O', 'I-ORG'), ('deficit', 'O', 'I-LOC'), ('of', 'O', 'I-LOC'), ('two', 'O', 'I-LOC'), ('percent', 'O', 'I-ORG'), ('brought', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'O'), ('talk', 'O', 'O'), ('-', 'O', 'O'), ('usda', 'B-ORG', 'O'), ('net', 'O', 'O'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'O'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('captain', 'O', 'O'), ('firmin', 'B-PER', 'O'), ('gatera,', 'I-PER', 'O'), ('spokesman', 'O', 'O'), ('for', 'O', 'O'), ('the', 'O', 'O'), ('tutsi', 'B-MISC', 'O'), ('-', '[xIGNx]', 'O'), ('dominated', '[xIGNx]', 'I-LOC'), ('rwandan', 'B-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('Christian', 'B-PER', 'B-ORG'), ('Cullen', 'I-PER', 'B-ORG'), (',', 'O', 'B-ORG'), ('14', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('Jeff', 'B-PER', 'B-ORG'), ('Wilson', 'I-PER', 'B-ORG'), (',', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O', 'B-ORG'), ('trio', 'O', 'B-ORG'), ('of', 'O', 'B-ORG'), ('hungry', 'O', 'B-ORG'), ('fans', 'O', 'B-ORG'), ('at', 'O', 'B-ORG'), ('the', 'O', 'B-ORG'), ('food', 'O', 'B-ORG'), ('court', 'O', 'B-ORG'), ('who', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('christian', 'B-PER', 'I-PER'), ('cullen,', 'I-PER', 'I-PER'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('jeff', 'B-PER', 'O'), ('wilson,', 'I-PER', 'B-MISC'), ('13', 'O', 'O'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('basuki,', 'B-PER', 'I-ORG'), ('a', 'O', 'B-MISC'), ('first', 'O', 'B-MISC'), ('-', '[xIGNx]', 'O'), ('round', '[xIGNx]', 'B-PER'), ('loser', 'O', 'I-MISC'), ('here', 'O', 'I-ORG'), ('for', 'O', 'B-MISC'), ('the', 'O', 'B-MISC'), ('fifth', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Squad', 'O', 'O'), (':', 'O', 'O'), ('Alan', 'B-PER', 'O'), ('Kelly', 'I-PER', 'O'), (',', 'O', 'O'), ('Shay', 'B-PER', 'O'), ('Given', 'I-PER', 'O'), (',', 'O', 'O'), ('Denis', 'B-PER', 'O'), ('Irwin', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Reuters', 'B-ORG', 'O'), ('cameraman', 'O', 'O'), ('Liutauras', 'B-PER', 'O'), ('Stremaitis', 'I-PER', 'O'), ('said', 'O', 'O'), ('a', 'O', 'O'), ('column', 'O', 'O'), ('of', 'O', 'O'), ('around', 'O', 'O'), ('40', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'B-LOC'), ('though', 'O', 'B-LOC'), ('more', 'O', 'I-LOC'), ('than', 'O', 'I-LOC'), ('40', 'O', 'I-LOC'), ('million', 'O', 'I-LOC'), ('copies', 'O', 'B-LOC'), ('of', 'O', 'I-LOC'), ('Windows', 'B-MISC', 'B-LOC'), ('95', 'I-MISC', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Germany', 'B-LOC', 'B-LOC'), (\"'s\", 'O', 'B-LOC'), ('Michael', 'B-PER', 'B-LOC'), ('Stich', 'I-PER', 'B-LOC'), (',', 'O', 'B-LOC'), ('the', 'O', 'B-LOC'), ('1991', 'O', 'B-LOC'), ('Wimbledon', 'B-MISC', 'B-LOC'), ('champion', 'O', 'B-LOC'), (',', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if \"deberta\" in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "        model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(\n",
    "            hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz\n",
    "        )\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[BaseModelCallback], splitter=blurr_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(\n",
    "            1,\n",
    "            lr_max=3e-5,\n",
    "            moms=(0.8, 0.7, 0.8),\n",
    "            cbs=[ShortEpochCallback(pct=0.1, short_valid=True), TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])],\n",
    "        )\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n",
      "  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
