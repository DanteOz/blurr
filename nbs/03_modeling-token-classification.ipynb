{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import HF_TextBlock, BlurrDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.modeling.core import HF_PreCalculatedLoss, Blearner\n",
    "from blurr.data.token_classification import (\n",
    "    align_labels_with_tokens,\n",
    "    align_labels_with_words,\n",
    "    HF_TokenClassInput,\n",
    "    HF_TokenTensorCategory,\n",
    "    HF_TokenCategorize,\n",
    "    HF_TokenCategoryBlock,\n",
    "    HF_TokenClassBeforeBatchTransform,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import HF_BaseModelWrapper, HF_BaseModelCallback, HF_PreCalculatedLoss, hf_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), HF_TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Air', 'O'), ('traffic', 'O'), ('officials', 'O'), ('said', 'O'), ('they', 'O'), ('had', 'O'), ('lost', 'O'), ('contact', 'O'), ('with', 'O'), ('the', 'O'), ('flight', 'O'), (',', 'O'), ('scheduled', 'O'), ('to', 'O'), ('arrive', 'O'), ('at', 'O'), ('around', 'O'), ('10.15', 'O'), ('a.m.', 'O'), ('(', 'O'), ('0815', 'O'), ('GMT', 'B-MISC'), (')', 'O'), (',', 'O'), ('shortly', 'O'), ('before', 'O'), ('it', 'O'), ('was', 'O'), ('due', 'O'), ('to', 'O'), ('land', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Scunthorpe', 'B-ORG'), ('0', 'O'), ('Scarborough', 'B-ORG'), ('2', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the HF_TokenClassBeforeBatchTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=HF_TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "learn_cbs = [HF_BaseModelCallback]\n",
    "fit_cbs = [HF_TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), cbs=learn_cbs, splitter=hf_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 30, 9]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), preds[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, torch.Size([4, 30]), 4, torch.Size([4, 30]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 9]) torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0009120108559727668, steep=3.630780702224001e-05, valley=0.00010964782268274575, slide=0.0003311311302240938)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060209</td>\n",
       "      <td>0.058938</td>\n",
       "      <td>0.985133</td>\n",
       "      <td>0.941315</td>\n",
       "      <td>0.936435</td>\n",
       "      <td>0.938869</td>\n",
       "      <td>03:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.96      0.96      2365\n",
      "        MISC       0.88      0.85      0.87      1078\n",
      "         ORG       0.92      0.92      0.92      2363\n",
      "         PER       0.97      0.97      0.97      2296\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8102\n",
      "   macro avg       0.93      0.93      0.93      8102\n",
      "weighted avg       0.94      0.94      0.94      8102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `HF_TokenClassInput` typed inputs\n",
    "    x: HF_TokenClassInput,\n",
    "    # This typedispatched `show_results` will be called for `HF_TokenTensorCategory` typed targets\n",
    "    y: HF_TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = align_labels_with_tokens(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = align_labels_with_words(hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('(', 'O', 'O'), ('Parent', 'O', 'O'), (')', 'O', 'O'), ('FORECAST', 'O', 'O'), ('YEAR-AGO', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Witnesses', 'O', 'O'), ('said', 'O', 'O'), ('wrecked', 'O', 'O'), ('tanks', 'O', 'O'), ('and', 'O', 'O'), ('vehicles', 'O', 'O'), ('littered', 'O', 'O'), ('both', 'O', 'O'), ('sides', 'O', 'O'), ('of', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(\"My name is Wayde and I live in San Diego\".split())\n",
    "print(res[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _blurr_predict_tokens(\n",
    "    # The function to do the base predictions (default: self.blurr_predict)\n",
    "    predict_func: Callable,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # The Blurr Transform with information about the Hugging Face objects used in your training\n",
    "    tfm: Transform,\n",
    "):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.blurr_predict` or `blurrONNX.predict.\n",
    "    Aligns the predicted labels, label ids, and probabilities with what you passed in excluding subword tokens\n",
    "    \"\"\"\n",
    "    # grab the Hugging Face tokenizer from the learner's dls.tfms\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    tok_kwargs = tfm.tok_kwargs\n",
    "\n",
    "    if isinstance(items[0], str):\n",
    "        items = [items]\n",
    "\n",
    "    outs = []\n",
    "    for inp, res in zip(items, predict_func(items)):\n",
    "        # `blurr_predict returns`` a list for each, we only doing one at a time so git first element of each\n",
    "        pred_lbls, pred_lbl_ids, probs = res[0][0], res[1][0], res[2][0]\n",
    "\n",
    "        # calculate the number of subtokens per raw/input token so that we can determine what predictions to return\n",
    "        subtoks_per_raw_tok = [(entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp]\n",
    "\n",
    "        # very similar to what HF_BatchTransform does with the exception that we are also grabbing the `special_tokens_mask` \n",
    "        # to help with getting rid or irelevant predicts for any special tokens (e.g., [CLS], [SEP], etc...)\n",
    "        res = hf_tokenizer(\n",
    "            inp,\n",
    "            None,\n",
    "            max_length=tfm.max_length,\n",
    "            padding=tfm.padding,\n",
    "            truncation=tfm.truncation,\n",
    "            is_split_into_words=tfm.is_split_into_words,\n",
    "            **tok_kwargs\n",
    "        )\n",
    "\n",
    "        special_toks_msk = L(res[\"special_tokens_mask\"])\n",
    "        actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "\n",
    "        # using the indexes to the actual tokens, get that info from the results returned above\n",
    "        pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "        actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "        actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "        actual_probs = probs[actual_tok_idxs]\n",
    "\n",
    "        # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "        # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "        offset = 0\n",
    "        raw_trg_idxs = []\n",
    "        for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok):\n",
    "            raw_trg_idxs.append(idx + offset)\n",
    "            offset += sub_tok_count - 1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "        outs.append((inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]))\n",
    "\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # Keyword arguments for `blurr_predict_tokens`\n",
    "    **kwargs\n",
    "):\n",
    "    tfm = first_blurr_tfm(self.dls, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "    return _blurr_predict_tokens(self.blurr_predict, items, tfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, `List`\\[`str`\\]\\], **\\*\\*`kwargs`**)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`items`** : *`typing.Union[str, typing.List[str]]`*\t<p>The str (or list of strings) you want to get token classification predictions for</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-MISC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-MISC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLearnerForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return HF_TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls,\n",
    "        # Your raw dataset\n",
    "        data,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # if we need to preprocess the raw data before creating our DataLoaders\n",
    "        if preprocess_func:\n",
    "            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, tokens_attr, token_labels_attr, labels)\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # build getters\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            get_x = ColReader(tokens_attr)\n",
    "            get_y = ColReader(token_labels_attr)\n",
    "        else:\n",
    "            get_x = ItemGetter(tokens_attr)\n",
    "            get_y = ItemGetter(token_labels_attr)\n",
    "\n",
    "        before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        blocks = (\n",
    "            HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput),\n",
    "            HF_TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls,\n",
    "        # Your pandas DataFrame\n",
    "        df: pd.DataFrame,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            labels = sorted(list(set([lbls for sublist in df[token_labels_attr].tolist() for lbls in sublist])))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            df,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls,\n",
    "        # The path to your csv file\n",
    "        csv_file: Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        return cls.from_dataframe(\n",
    "            df,\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            preprocess_func=preprocess_func,\n",
    "            tokens_attr=tokens_attr,\n",
    "            token_labels_attr=token_labels_attr,\n",
    "            labels=labels,\n",
    "            dblock_splitter=dblock_splitter,\n",
    "            dl_kwargs=dl_kwargs,\n",
    "            learner_kwargs=learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls,\n",
    "        # A list of dictionaries\n",
    "        ds: List[Dict],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            all_labels = []\n",
    "            for item in ds:\n",
    "                all_labels += item[token_labels_attr]\n",
    "            labels = sorted(list(set(all_labels)))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            ds,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_dataframe(\n",
    "    conll2003_df,\n",
    "    \"roberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dblock_splitter=RandomSplitter(),\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Batumi', 'B-ORG'), ('(', 'O'), ('Georgia', 'B-LOC'), (')', 'O'), ('3', 'O'), ('(', 'O'), ('0-2', 'O'), (')', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Miscellaneous', 'O'), ('charges', 'O'), ('---', 'O'), ('16', 'O'), ('---', 'O'), ('30', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060851</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>0.984616</td>\n",
       "      <td>0.939797</td>\n",
       "      <td>0.932982</td>\n",
       "      <td>0.936378</td>\n",
       "      <td>06:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Summaries', 'O', 'O'), ('of', 'O', 'O'), ('German', 'B-MISC', 'O'), ('first', 'O', 'O'), ('division', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Result', 'O', 'O'), ('of', 'O', 'O'), ('Dutch', 'B-MISC', 'B-MISC'), ('first', 'O', 'O'), ('division', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.95      0.96      2511\n",
      "        MISC       0.85      0.85      0.85      1137\n",
      "         ORG       0.92      0.92      0.92      2469\n",
      "         PER       0.97      0.97      0.97      2433\n",
      "\n",
      "   micro avg       0.94      0.93      0.94      8550\n",
      "   macro avg       0.93      0.92      0.92      8550\n",
      "weighted avg       0.94      0.93      0.94      8550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'B-PER'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('watch', 'O'), ('Lewandowski', 'B-PER'), ('score', 'O'), ('some', 'O'), ('more', 'O'), ('goals', 'O'), ('for', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-MISC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                 word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                     word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                                forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                   word_ids \n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                 could not test           \n",
    "    \"google/mobilebert-uncased\",\n",
    "    'google/rembert',\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",                 \n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                 word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('reuters', 'B-ORG', 'O'), ('has', 'O', 'I-ORG'), ('not', 'O', 'I-ORG'), ('verified', 'O', 'I-ORG'), ('these', 'O', 'I-MISC'), ('stories', 'O', 'I-ORG'), ('and', 'O', 'I-ORG'), ('does', 'O', 'B-MISC'), ('not', 'O', 'B-PER'), ('vouch', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('israeli', 'B-MISC', 'I-MISC'), ('prime', 'O', 'I-MISC'), ('minister', 'O', 'I-LOC'), ('benjamin', 'B-PER', 'I-ORG'), ('netanyahu', 'I-PER', 'O'), ('has', 'O', 'I-ORG'), ('accused', 'O', 'I-MISC'), ('opposition', 'O', 'O'), ('leader', 'O', 'O'), ('peres', 'B-PER', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('wimbledon', 'B-ORG', 'B-ORG'), ('2', 'O', 'B-PER'), ('0', 'O', 'B-MISC'), ('0', 'O', 'B-MISC'), ('2', 'O', 'I-MISC'), ('0', 'O', 'I-MISC'), ('5', 'O', 'B-PER'), ('0', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('pohang', 'B-ORG', 'B-PER'), ('3', 'O', 'I-ORG'), ('chonbuk', 'B-ORG', 'B-PER'), ('2', 'O', 'I-LOC'), ('(', 'O', 'I-ORG'), ('halftime', 'O', 'I-ORG'), ('0', 'O', 'B-PER'), ('-', 'O', 'B-PER'), ('0', 'O', 'B-PER'), (')', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('--', 'O', 'B-LOC'), ('Jonathan', 'B-PER', 'I-MISC'), ('Birt', 'I-PER', 'I-MISC'), (',', 'O', 'O'), ('London', 'B-ORG', 'O'), ('Newsroom', 'I-ORG', 'I-MISC'), ('+44', 'O', 'I-MISC'), ('171', 'O', 'O'), ('542', 'O', 'B-LOC'), ('7717', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('JOHANNESBURG', 'B-LOC', 'I-MISC'), ('1996-08-26', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('NEW', 'B-LOC', 'I-LOC'), ('YORK', 'I-LOC', 'O'), ('1996-08-22', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('--', 'O', 'I-LOC'), ('Amsterdam', 'B-LOC', 'B-LOC'), ('newsdesk', 'O', 'B-LOC'), ('+31', 'O', 'I-LOC'), ('20', 'O', 'O'), ('504', 'O', 'O'), ('5000', 'O', 'O'), ('(', 'O', 'O'), ('Fax', 'O', 'O'), ('020-504-5040', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('dynamo', 'B-ORG', 'I-PER'), ('kiev', 'I-ORG', 'I-PER'), ('5', 'O', 'B-MISC'), ('kremin', 'B-ORG', 'B-MISC'), ('kremenchuk', 'I-ORG', 'B-MISC'), ('0', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('paris', 'B-LOC', 'I-PER'), ('1996', 'O', 'B-MISC'), ('-', 'O', 'I-PER'), ('08', 'O', 'B-MISC'), ('-', 'O', 'I-PER'), ('27', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Ulsan', 'B-ORG', 'O'), ('1', 'O', 'B-MISC'), ('0', 'O', 'B-ORG'), ('1', 'O', 'O'), ('6', 'O', 'I-LOC'), ('6', 'O', 'B-ORG'), ('3', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('-', 'O', 'I-LOC'), ('Palestinian', 'B-MISC', 'I-LOC'), ('Minister', 'O', 'B-MISC'), ('Erekat', 'B-PER', 'I-LOC'), ('says', 'O', 'I-ORG'), ('Israel-PLO', 'B-MISC', 'I-LOC'), ('talks', 'O', 'B-ORG'), ('will', 'O', 'B-MISC'), ('begin', 'O', 'B-MISC'), ('by', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('2.', 'O', 'I-MISC'), ('Tim', 'B-PER', 'I-MISC'), ('Forsyth', 'I-PER', 'B-LOC'), ('(', 'O', 'I-MISC'), ('Australia', 'B-LOC', 'I-MISC'), (')', 'O', 'I-MISC'), ('2.', 'O', 'I-MISC'), ('30', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Tsang', 'B-PER', 'I-MISC'), ('said', 'O', 'I-MISC'), ('three', 'O', 'I-MISC'), ('sets', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('meetings', 'O', 'I-MISC'), ('with', 'O', 'I-MISC'), ('Chinese', 'B-MISC', 'I-MISC'), ('authorities', 'O', 'I-MISC'), ('on', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('7.', 'O', 'B-PER'), ('olga', 'B-PER', 'B-PER'), ('chernyavskaya', 'I-PER', 'B-MISC'), ('(', 'O', 'I-MISC'), ('russia', 'B-LOC', 'B-MISC'), (')', 'O', 'B-LOC'), ('60.', 'O', 'B-MISC'), ('46', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-PER'), ('indian', 'B-MISC', 'B-MISC'), ('fishermen', 'O', 'B-ORG'), ('come', 'O', 'B-MISC'), ('right', 'O', 'B-MISC'), ('up', 'O', 'B-LOC'), ('to', 'O', 'B-PER'), ('pesalai', 'B-LOC', 'I-PER'), ('to', 'O', 'O'), ('fish,', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('fortuna', 'B-ORG', 'B-PER'), ('duesseldorf', 'I-ORG', 'B-PER'), ('3', 'O', 'B-PER'), ('1', 'O', 'B-PER'), ('0', 'O', 'B-PER'), ('2', 'O', 'I-PER'), ('1', 'O', 'I-PER'), ('7', 'O', 'B-PER'), ('3', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('it', 'O', 'B-ORG'), ('was', 'O', 'B-PER'), ('mostly', 'O', 'B-PER'), ('short', 'O', 'B-PER'), ('points.', 'O', 'I-LOC'), ('\"', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('CLOSE', 'O', 'I-PER'), ('IN', 'O', 'I-PER'), ('POINTS', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('LONDON', 'B-LOC', 'I-PER'), ('1996-08-27', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('but', 'O', 'B-ORG'), ('analysts', 'O', 'B-ORG'), ('said', 'O', 'B-ORG'), ('the', 'O', 'O'), ('row', 'O', 'O'), ('was', 'O', 'O'), ('not', 'O', 'O'), ('expected', 'O', 'B-ORG'), ('to', 'O', 'B-ORG'), ('immediately', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('two', 'O', 'I-PER'), ('dead', 'O', 'B-ORG'), ('in', 'O', 'B-MISC'), ('cambodia', 'B-LOC', 'O'), ('helicopter', 'O', 'I-ORG'), ('crash.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('BELGRADE', 'B-LOC', 'B-PER'), ('1996-08-29', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Cowdenbeath', 'B-ORG', 'B-PER'), ('1', 'O', 'B-PER'), ('Monstrose', 'B-ORG', 'B-PER'), ('0', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('playing', 'O', 'O'), ('at', 'O', 'O'), ('night', 'O', 'O'), ('was', 'O', 'O'), ('not', 'O', 'O'), (\"muster's\", 'B-PER', 'O'), ('preference.', 'O', 'O'), ('\"', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('london', 'B-ORG', 'O'), ('international', 'I-ORG', 'O'), ('financial', 'I-ORG', 'O'), ('futures', 'I-ORG', 'O'), ('exchange', 'I-ORG', 'O'), ('automated', 'O', 'O'), ('pit', 'O', 'O'), ('trading', 'O', 'O'), ('(', 'O', 'O'), ('apt', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('PRESS', 'O', 'I-LOC'), ('DIGEST', 'O', 'I-LOC'), ('-', 'O', 'B-LOC'), ('Malta', 'B-LOC', 'I-LOC'), ('-', 'O', 'I-LOC'), ('Aug', 'O', 'I-LOC'), ('26', 'O', 'I-LOC'), ('.', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'I-LOC'), ('thefts', 'O', 'I-LOC'), ('occurred', 'O', 'I-LOC'), ('Sunday', 'O', 'I-LOC'), ('when', 'O', 'I-LOC'), ('the', 'O', 'I-LOC'), ('victim', 'O', 'I-LOC'), (',', 'O', 'B-LOC'), ('New', 'B-LOC', 'I-LOC'), ('York', 'I-LOC', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('voters', 'O', 'I-MISC'), ('will', 'O', 'I-PER'), ('be', 'O', 'B-PER'), ('choosing', 'O', 'I-LOC'), ('a', 'O', 'B-MISC'), ('three', 'O', 'I-MISC'), ('-', 'O', 'B-PER'), ('member', 'O', 'B-PER'), ('presidency', 'O', 'B-MISC'), ('and', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('benevolent', 'B-ORG', 'I-MISC'), ('assoc.', 'I-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(\"'\", 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Men', 'O', 'I-LOC'), (\"'s\", 'O', 'B-ORG'), ('800', 'O', 'O'), ('metres', 'O', 'B-ORG'), (':', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('glasgow', 'B-ORG', 'I-PER'), ('rangers', 'I-ORG', 'I-PER'), ('striker', 'O', 'O'), ('ally', 'B-PER', 'I-PER'), ('mccoist,', 'I-PER', 'B-PER'), ('another', 'O', 'I-PER'), ('man', 'O', 'I-PER'), ('in', 'O', 'B-ORG'), ('form', 'O', 'I-PER'), ('after', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('bench', 'O', 'O'), ('coach', 'O', 'O'), ('andy', 'B-PER', 'O'), ('etchebarren', 'I-PER', 'B-LOC'), ('took', 'O', 'O'), ('his', 'O', 'O'), ('place.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('QADISSIYA', 'B-ORG', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('No', 'O', 'O'), ('new', 'O', 'O'), ('deficit', 'O', 'O'), ('forecast', 'O', 'O'), ('has', 'O', 'O'), ('been', 'O', 'O'), ('issued', 'O', 'O'), ('so', 'O', 'O'), ('far', 'O', 'O'), ('.', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('he', 'O', 'I-LOC'), ('said', 'O', 'I-LOC'), ('his', 'O', 'I-LOC'), ('government', 'O', 'O'), ('had', 'O', 'O'), ('just', 'O', 'O'), ('asked', 'O', 'I-LOC'), ('u.', 'B-ORG', 'O'), ('n.', 'B-ORG', 'O'), ('human', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('amsterdam', 'B-LOC', 'O'), ('1996', 'O', 'O'), ('-', 'O', 'O'), ('08', 'O', 'O'), ('-', 'O', 'O'), ('26', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('KIRKCALDY', 'B-LOC', 'B-MISC'), (',', 'O', 'B-MISC'), ('Scotland', 'B-LOC', 'B-MISC'), ('1996-08-27', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('It', 'O', 'B-MISC'), ('is', 'O', 'B-MISC'), ('a', 'O', 'B-ORG'), ('holy', 'O', 'B-MISC'), ('war', 'O', 'B-MISC'), ('for', 'O', 'B-MISC'), ('us', 'O', 'B-ORG'), (',', 'O', 'B-ORG'), ('\"', 'O', 'B-MISC'), ('he', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('He', 'O', 'B-PER'), ('also', 'O', 'B-PER'), ('said', 'O', 'B-PER'), ('the', 'O', 'O'), ('company', 'O', 'O'), ('expect', 'O', 'O'), ('to', 'O', 'O'), ('record', 'O', 'O'), ('a', 'O', 'O'), ('83', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Deutsche', 'B-ORG', 'B-PER'), ('Bahn', 'I-ORG', 'B-PER'), ('H1', 'O', 'O'), ('pre-tax', 'O', 'O'), ('profit', 'O', 'B-PER'), ('up', 'O', 'B-PER'), ('17.5', 'O', 'B-PER'), ('pct', 'O', 'B-PER'), ('.', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if 'deberta' in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz)\n",
    "        blocks = (HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), HF_TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = HF_BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[HF_BaseModelCallback], splitter=hf_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[ShortEpochCallback(pct=0.1, short_valid=True), HF_TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
