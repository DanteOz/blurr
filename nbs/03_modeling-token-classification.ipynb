{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import TextBlock, BlurrDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.modeling.core import PreCalculatedLoss, Blearner\n",
    "from blurr.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    TokenClassTextInput,\n",
    "    TokenTensorCategory,\n",
    "    TokenCategorize,\n",
    "    TokenCategoryBlock,\n",
    "    TokenClassBatchTokenizeTransform,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import BaseModelWrapper, BaseModelCallback, PreCalculatedLoss, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47022993b405430d9392628bb0e7a3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Shares', 'O'), ('in', 'O'), ('Slough', 'B-ORG'), (',', 'O'), ('which', 'O'), ('earlier', 'O'), ('announced', 'O'), ('a', 'O'), ('14', 'O'), ('percent', 'O'), ('rise', 'O'), ('in', 'O'), ('first-half', 'O'), ('pretax', 'O'), ('profit', 'O'), ('to', 'O'), ('37.4', 'O'), ('million', 'O'), ('stg', 'O'), (',', 'O'), ('climbed', 'O'), ('nearly', 'O'), ('six', 'O'), ('percent', 'O'), (',', 'O'), ('or', 'O'), ('14p', 'O'), ('to', 'O'), ('250', 'O'), ('pence', 'O'), ('at', 'O'), ('1009', 'O'), ('GMT', 'B-MISC'), (',', 'O'), ('while', 'O'), ('British', 'B-ORG'), ('Land', 'I-ORG'), ('added', 'O'), ('12-1', 'O'), ('/', 'O'), ('2p', 'O'), ('to', 'O'), ('468p', 'O'), (',', 'O'), ('Land', 'B-ORG'), ('Securities', 'I-ORG'), ('rose', 'O'), ('5-1', 'O'), ('/', 'O'), ('2p', 'O'), ('to', 'O'), ('691p', 'O'), ('and', 'O'), ('Hammerson', 'B-ORG'), ('was', 'O'), ('8p', 'O'), ('higher', 'O'), ('at', 'O'), ('390', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassMetricsCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), cbs=learn_cbs, splitter=blurr_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 156, 9]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), preds[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.002754228748381138, steep=7.585775892948732e-05, valley=0.0003981071640737355, slide=0.001737800776027143)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.068905</td>\n",
       "      <td>0.055924</td>\n",
       "      <td>0.988376</td>\n",
       "      <td>0.937606</td>\n",
       "      <td>0.931478</td>\n",
       "      <td>0.934532</td>\n",
       "      <td>03:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.96      0.96      1475\n",
      "        MISC       0.85      0.90      0.88       661\n",
      "         ORG       0.93      0.89      0.91      1301\n",
      "         PER       0.97      0.96      0.96      1306\n",
      "\n",
      "   micro avg       0.94      0.93      0.93      4743\n",
      "   macro avg       0.93      0.93      0.93      4743\n",
      "weighted avg       0.94      0.93      0.93      4743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_results`\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets\n",
    "    y: TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'O'), ('do', 'O', 'O'), (\"n't\", 'O', 'O'), ('normally', 'O', 'O'), ('do', 'O', 'O'), ('this', 'O', 'O'), ('but', 'O', 'O'), ('can', 'O', 'O'), ('you', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'O'), ('New', 'B-LOC', 'B-LOC'), ('York', 'I-LOC', 'I-LOC'), (',', 'O', 'O'), ('Wally', 'B-PER', 'B-PER'), ('Whitehurst', 'I-PER', 'I-PER'), ('allowed', 'O', 'O'), ('two', 'O', 'O'), ('runs', 'O', 'O'), ('over', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(\"My name is Wayde and I live in San Diego\".split())\n",
    "print(res[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blurr_predict_tokens`\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer. Starting with version 2.0 of Blurr, we bring token prediction in-line with Hugging Face's token classification pipeline, both in terms of supporting the same aggregation strategies via Blurr's `TokenAggregationStrategies` class, and also the output via Blurr's `@patch`ed `Learner` method, `blurr_predict_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenAggregationStrategies():\n",
    "    \"\"\" \n",
    "    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various \n",
    "    token classication tasks (e.g, NER, POS, chunking, etc...)\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = \"O\") -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.valid_strategies = [\"simple\", \"first\", \"max\", \"average\"]\n",
    "\n",
    "    def by_token(self, tokens, input_ids, offsets, preds, probs):\n",
    "        results = []\n",
    "        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            label = self.labels[pred]\n",
    "            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            start, end = offset\n",
    "            results.append({\"entity\": label, \"score\": prob[pred], \"word\": token, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):\n",
    "        # validate `strategy_name`\n",
    "        if strategy_name not in self.valid_strategies:\n",
    "            raise ValueError(\"The 'strategy_name' is not supported by this class\")\n",
    "\n",
    "        # validate the existence of `word_ids` if the aggregation strategy = \"average\"\n",
    "        if strategy_name == \"average\" and word_ids is None:\n",
    "            raise ValueError(\"The 'average' strategy requires word_ids list\")\n",
    "\n",
    "        results = []\n",
    "        idx = 0\n",
    "        while idx < len(preds):\n",
    "            pred = preds[idx]\n",
    "            label = self.labels[pred]\n",
    "\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:\n",
    "                idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Remove the B- or I-\n",
    "            label = label[2:]\n",
    "            start, end = offsets[idx]\n",
    "\n",
    "            all_scores = []\n",
    "            all_scores.append(probs[idx][pred])\n",
    "\n",
    "            word_scores = {}\n",
    "            if strategy_name == \"average\":\n",
    "                word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "            while (\n",
    "                idx+1 < len(preds)\n",
    "                and self.labels[preds[idx+1]] == f\"I-{label}\"\n",
    "            ):\n",
    "                idx += 1\n",
    "                _, end = offsets[idx]\n",
    "\n",
    "                pred = preds[idx]\n",
    "\n",
    "                if strategy_name == \"average\":\n",
    "                    if word_ids[idx] in word_scores:\n",
    "                        word_scores[word_ids[idx]].append(probs[idx][pred])\n",
    "                    else:\n",
    "                        word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "                \n",
    "                if strategy_name != \"first\":\n",
    "                    all_scores.append(probs[idx][pred])\n",
    "\n",
    "            # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "            if strategy_name == \"average\":\n",
    "                score = np.mean([np.mean(v).item() for k,v in word_scores.items()])\n",
    "            else:\n",
    "                score = np.max(all_scores).item() if strategy_name == 'max' else np.mean(all_scores).item()\n",
    "\n",
    "            word = text[start:end]\n",
    "            results.append({\"entity_group\": label, \"score\": score, \"word\": word, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # How entities are grouped and scored\n",
    "    aggregation_strategy: str = \"simple\",\n",
    "    # The label used to idendity non-entity related words/tokens\n",
    "    non_entity_label: str = \"O\",\n",
    "    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a \n",
    "    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the \n",
    "    # equavlient of fast tokenizer's `word_ids``\n",
    "    slow_word_ids_func: Optional[Callable] = None,\n",
    "):\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)\n",
    "\n",
    "    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_offsets = inputs[\"offset_mapping\"]\n",
    "    inputs_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # run inputs through model\n",
    "    model_inputs = {k: v.to(learn.model.hf_model.device) for k, v in inputs.items()}\n",
    "    outputs = learn.model(model_inputs)\n",
    "\n",
    "    # fetch probabilities and predictions\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).tolist()\n",
    "    predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    # build our results\n",
    "    results = []\n",
    "    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)):\n",
    "        # build our results for the current input\n",
    "        tokens = inputs.tokens(input_idx)\n",
    "        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)\n",
    "\n",
    "        if aggregation_strategy == \"token\":\n",
    "            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))\n",
    "        else:\n",
    "            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, `List`\\[`str`\\]\\], **`aggregation_strategy`**:`str`=*`'simple'`*, **`non_entity_label`**:`str`=*`'O'`*, **`slow_word_ids_func`**:`Optional`\\[`Callable`\\]=*`None`*)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`items`** : *`typing.Union[str, typing.List[str]]`*\t<p>The str (or list of strings) you want to get token classification predictions for</p>\n",
       "\n",
       "\n",
       " - **`aggregation_strategy`** : *`<class 'str'>`*, *optional*\t<p>How entities are grouped and scored</p>\n",
       "\n",
       "\n",
       " - **`non_entity_label`** : *`<class 'str'>`*, *optional*\t<p>The label used to idendity non-entity related words/tokens</p>\n",
       "\n",
       "\n",
       " - **`slow_word_ids_func`** : *`typing.Optional[typing.Callable]`*, *optional*\t<p>If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a \n",
       "tokenizzer, example index, and a batch encoding as arguments and in turn returnes the \n",
       "equavlient of fast tokenizer's `word_ids``</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[{'entity_group': 'ORG', 'score': 0.9941564798355103, 'word': 'Bayern Munich', 'start': 0, 'end': 13}, {'entity_group': 'LOC', 'score': 0.9980176687240601, 'word': 'Germany', 'start': 34, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(\n",
    "    items=[\"My name is Wayde and I live in San Diego and using Hugging Face\", \"Bayern Munich is a soccer team in Germany\"],\n",
    "    aggregation_strategy=\"max\",\n",
    ")\n",
    "\n",
    "print(len(res))\n",
    "print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'PER', 'score': 0.925978273153305, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.7118993699550629, 'word': 'ohmeow.', 'start': 34, 'end': 41}, {'entity_group': 'LOC', 'score': 0.9915635585784912, 'word': 'California', 'start': 56, 'end': 66}]]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.925978273153305, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.7118993699550629, 'word': 'ohmeow.', 'start': 34, 'end': 41}, {'entity_group': 'LOC', 'score': 0.9915635585784912, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.6852618455886841, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'LOC', 'score': 0.9978002905845642, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.992070883512497, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.7141481041908264, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.925978273153305, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.7118993699550629, 'word': 'ohmeow.', 'start': 34, 'end': 41}, {'entity_group': 'LOC', 'score': 0.9915635585784912, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.6852618455886841, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'LOC', 'score': 0.9978002905845642, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.992070883512497, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.7141481041908264, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "results = inf_learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls,\n",
    "        # Your raw dataset\n",
    "        data,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # if we need to preprocess the raw data before creating our DataLoaders\n",
    "        if preprocess_func:\n",
    "            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, tokens_attr, token_labels_attr, labels)\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # build getters\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            get_x = ColReader(tokens_attr)\n",
    "            get_y = ColReader(token_labels_attr)\n",
    "        else:\n",
    "            get_x = ItemGetter(tokens_attr)\n",
    "            get_y = ItemGetter(token_labels_attr)\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        blocks = (\n",
    "            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "            TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls,\n",
    "        # Your pandas DataFrame\n",
    "        df: pd.DataFrame,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            labels = sorted(list(set([lbls for sublist in df[token_labels_attr].tolist() for lbls in sublist])))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            df,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls,\n",
    "        # The path to your csv file\n",
    "        csv_file: Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict ={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        return cls.from_dataframe(\n",
    "            df,\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            preprocess_func=preprocess_func,\n",
    "            tokens_attr=tokens_attr,\n",
    "            token_labels_attr=token_labels_attr,\n",
    "            labels=labels,\n",
    "            dblock_splitter=dblock_splitter,\n",
    "            dl_kwargs=dl_kwargs,\n",
    "            learner_kwargs=learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls,\n",
    "        # A list of dictionaries\n",
    "        ds: List[Dict],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict ={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict ={}\n",
    "    ):\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            all_labels = []\n",
    "            for item in ds:\n",
    "                all_labels += item[token_labels_attr]\n",
    "            labels = sorted(list(set(all_labels)))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            ds,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_dataframe(\n",
    "    conll2003_df,\n",
    "    \"roberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dblock_splitter=RandomSplitter(),\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O'), ('-', 'O'), ('Christian', 'B-PER'), ('Cullen', 'I-PER'), (',', 'O'), ('14', 'O'), ('-', 'O'), ('Jeff', 'B-PER'), ('Wilson', 'I-PER'), (',', 'O'), ('13', 'O'), ('-', 'O'), ('Walter', 'B-PER'), ('Little', 'I-PER'), (',', 'O'), ('12', 'O'), ('-', 'O'), ('Frank', 'B-PER'), ('Bunce', 'I-PER'), (',', 'O'), ('11', 'O'), ('-', 'O'), ('Glen', 'B-PER'), ('Osborne', 'I-PER'), (';', 'O'), ('10', 'O'), ('-', 'O'), ('Andrew', 'B-PER'), ('Mehrtens', 'I-PER'), (',', 'O'), ('9', 'O'), ('-', 'O'), ('Justin', 'B-PER'), ('Marshall', 'I-PER'), (';', 'O'), ('8', 'O'), ('-', 'O'), ('Zinzan', 'B-PER'), ('Brooke', 'I-PER'), (',', 'O'), ('7', 'O'), ('-', 'O'), ('Josh', 'B-PER'), ('Kronfeld', 'I-PER'), (',', 'O'), ('6', 'O'), ('-', 'O'), ('Michael', 'B-PER'), ('Jones', 'I-PER'), (',', 'O'), ('5', 'O'), ('-', 'O'), ('Ian', 'B-PER'), ('Jones', 'I-PER'), (',', 'O'), ('4', 'O'), ('-', 'O'), ('Robin', 'B-PER'), ('Brooke', 'I-PER'), (',', 'O'), ('3', 'O'), ('-', 'O'), ('Olo', 'B-PER'), ('Brown', 'I-PER'), (',', 'O'), ('2', 'O'), ('-', 'O'), ('Sean', 'B-PER'), ('Fitzpatrick', 'I-PER'), ('(', 'O'), ('captain', 'O'), (')', 'O'), (',', 'O'), ('1', 'O'), ('-', 'O'), ('Craig', 'B-PER'), ('Dowd', 'I-PER'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('It', 'O'), ('also', 'O'), ('cited', 'O'), ('the', 'O'), ('potential', 'O'), ('problem', 'O'), ('of', 'O'), ('a', 'O'), ('two-tiered', 'O'), ('market', 'O'), ('in', 'O'), ('which', 'O'), ('market', 'O'), ('makers', 'O'), ('quote', 'O'), ('one', 'O'), ('price', 'O'), ('to', 'O'), ('public', 'O'), ('investors', 'O'), ('while', 'O'), ('quoting', 'O'), ('better', 'O'), ('prices', 'O'), ('in', 'O'), ('private', 'O'), ('systems', 'O'), (',', 'O'), ('thus', 'O'), ('robbing', 'O'), ('investors', 'O'), ('without', 'O'), ('access', 'O'), ('to', 'O'), ('\"', 'O'), ('hidden', 'O'), ('\"', 'O'), ('quotes', 'O'), ('the', 'O'), ('benefit', 'O'), ('of', 'O'), ('the', 'O'), ('best', 'O'), ('available', 'O'), ('prices', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.987623</td>\n",
       "      <td>0.936598</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.932986</td>\n",
       "      <td>06:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'O'), ('TALK', 'O', 'O'), ('-', 'O', 'O'), ('USDA', 'B-ORG', 'B-ORG'), ('net', 'O', 'O'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'O'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Speaking', 'O', 'O'), ('only', 'O', 'O'), ('hours', 'O', 'O'), ('after', 'O', 'O'), ('Chinese', 'B-MISC', 'B-MISC'), ('state', 'O', 'O'), ('media', 'O', 'O'), ('said', 'O', 'O'), ('the', 'O', 'O'), ('time', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.96      1499\n",
      "        MISC       0.86      0.87      0.87       691\n",
      "         ORG       0.92      0.89      0.90      1292\n",
      "         PER       0.98      0.97      0.97      1334\n",
      "\n",
      "   micro avg       0.94      0.93      0.93      4816\n",
      "   macro avg       0.93      0.92      0.92      4816\n",
      "weighted avg       0.94      0.93      0.93      4816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.99704509973526, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.9166025122006735, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.47588221232096356, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9938427209854126, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.977600634098053, 'word': 'Lewandowski', 'start': 39, 'end': 50}, {'entity_group': 'ORG', 'score': 0.991537481546402, 'word': 'Bayern Munich', 'start': 77, 'end': 90}, {'entity_group': 'MISC', 'score': 0.984983503818512, 'word': 'Bundesliga', 'start': 98, 'end': 108}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids \n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test           \n",
    "    \"google/mobilebert-uncased\",\n",
    "    'google/rembert',\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",                 \n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b581a98c621f4e89a92b5d2a1f7c2d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('the', 'O', 'B-LOC'), ('agreement', 'O', 'I-ORG'), ('resolved', 'O', 'B-LOC'), ('a', 'O', 'I-LOC'), ('dispute', 'O', 'B-LOC'), ('that', 'O', 'B-LOC'), ('arose', 'O', 'B-LOC'), ('in', 'O', 'O'), ('june', 'O', 'I-LOC'), ('when', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'B-LOC'), ('brady', 'B-PER', 'I-PER'), ('bill', 'O', 'B-LOC'), (',', 'O', 'B-MISC'), ('calling', 'O', 'I-LOC'), ('for', 'O', 'O'), ('a', 'O', 'B-PER'), ('waiting', 'O', 'B-LOC'), ('period', 'O', 'B-LOC'), ('before', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'O'), ('talk', 'O', 'O'), ('-', 'O', 'B-PER'), ('usda', 'B-ORG', 'O'), ('net', 'O', 'I-ORG'), ('change', 'O', 'O'), ('in', 'O', 'B-PER'), ('weekly', 'O', 'B-PER'), ('export', 'O', 'O'), ('commitments', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('on', 'O', 'O'), ('the', 'O', 'O'), ('back', 'O', 'O'), ('nine', 'O', 'O'), ('mickelson', 'B-PER', 'O'), ('began', 'O', 'O'), ('driving', 'O', 'O'), ('erratically,', 'O', 'O'), ('and', 'O', 'O'), ('poor', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-LOC'), ('We', 'O', 'I-ORG'), ('have', 'O', 'B-ORG'), ('no', 'O', 'B-ORG'), ('doubt', 'O', 'B-ORG'), ('that', 'O', 'B-PER'), ('this', 'O', 'I-ORG'), ('is', 'O', 'I-ORG'), ('one', 'O', 'I-ORG'), ('of', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('At', 'O', 'B-PER'), ('California', 'B-LOC', 'B-PER'), (',', 'O', 'O'), ('Tim', 'B-PER', 'I-ORG'), ('Wakefield', 'I-PER', 'B-ORG'), ('pitched', 'O', 'B-ORG'), ('a', 'O', 'B-ORG'), ('six-hitter', 'O', 'B-ORG'), ('for', 'O', 'B-ORG'), ('his', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-ORG'), ('-', 'O', 'I-LOC'), ('Christian', 'B-PER', 'I-ORG'), ('Cullen', 'I-PER', 'B-PER'), (',', 'O', 'O'), ('14', 'O', 'I-ORG'), ('-', 'O', 'I-LOC'), ('Jeff', 'B-PER', 'O'), ('Wilson', 'I-PER', 'O'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'B-PER'), ('do', 'O', 'O'), (\"n't\", 'O', 'I-ORG'), ('normally', 'O', 'I-ORG'), ('do', 'O', 'B-LOC'), ('this', 'O', 'B-PER'), ('but', 'O', 'I-ORG'), ('can', 'O', 'I-ORG'), ('you', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-LOC'), ('-', 'O', 'B-MISC'), ('christian', 'B-PER', 'B-LOC'), ('cullen,', 'I-PER', 'B-LOC'), ('14', 'O', 'O'), ('-', 'O', 'B-LOC'), ('jeff', 'B-PER', 'B-ORG'), ('wilson,', 'I-PER', 'B-LOC'), ('13', 'O', 'O'), ('-', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'I-LOC'), ('presence', 'O', 'B-LOC'), ('of', 'O', 'B-MISC'), ('takemura,', 'B-PER', 'B-LOC'), ('whose', 'O', 'O'), ('role', 'O', 'B-MISC'), ('as', 'O', 'B-LOC'), ('finance', 'O', 'B-LOC'), ('minister', 'O', 'B-LOC'), ('in', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Now', 'O', 'B-ORG'), (',', 'O', 'B-ORG'), ('U.S.', 'B-LOC', 'I-LOC'), ('District', 'O', 'B-MISC'), ('Judge', 'O', 'B-LOC'), ('Mark', 'B-PER', 'B-ORG'), ('Wolf', 'I-PER', 'B-ORG'), ('has', 'O', 'B-MISC'), ('ordered', 'O', 'O'), ('the', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'B-PER'), ('the', 'O', 'B-PER'), ('central', 'O', 'B-ORG'), ('bank', 'O', 'I-ORG'), (\"'s\", 'O', 'B-MISC'), ('latest', 'O', 'B-PER'), ('bid', 'O', 'I-LOC'), ('to', 'O', 'I-PER'), ('jumpstart', 'O', 'O'), ('Taiwan', 'B-LOC', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-MISC'), ('One', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), ('police', 'O', 'I-MISC'), ('versions', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('the', 'O', 'B-LOC'), ('case', 'O', 'I-MISC'), ('of', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('They', 'O', 'I-MISC'), ('said', 'O', 'I-MISC'), ('461', 'O', 'B-LOC'), ('mm', 'O', 'I-MISC'), ('(', 'O', 'I-MISC'), ('18', 'O', 'I-MISC'), ('inches', 'O', 'B-LOC'), (')', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('rain', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('even', 'O', 'I-ORG'), ('though', 'O', 'O'), ('more', 'O', 'I-LOC'), ('than', 'O', 'I-PER'), ('40', 'O', 'I-LOC'), ('million', 'O', 'B-ORG'), ('copies', 'O', 'B-MISC'), ('of', 'O', 'B-ORG'), ('windows', 'B-MISC', 'I-ORG'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('a', 'O', 'I-LOC'), ('chain', 'O', 'O'), ('-', '[xIGNx]', 'I-LOC'), ('smoking', '[xIGNx]', 'B-ORG'), ('former', 'O', 'B-PER'), ('paratroop', 'O', 'O'), ('general', 'O', 'I-LOC'), ('with', 'O', 'I-LOC'), ('a', 'O', 'B-PER'), ('sharp', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('we', 'O', 'I-MISC'), ('have', 'O', 'I-MISC'), ('no', 'O', 'I-MISC'), ('doubt', 'O', 'I-MISC'), ('that', 'O', 'I-MISC'), ('this', 'O', 'O'), ('is', 'O', 'I-MISC'), ('one', 'O', 'I-MISC'), ('of', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'I-MISC'), ('credibility', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), ('buenos', 'B-LOC', 'I-LOC'), ('aires', 'I-LOC', 'B-ORG'), ('provincial', 'O', 'I-MISC'), ('police,', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), ('largest', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-MISC'), ('TALK', 'O', 'I-MISC'), ('-', 'O', 'I-MISC'), ('USDA', 'B-ORG', 'I-MISC'), ('net', 'O', 'I-MISC'), ('change', 'O', 'B-PER'), ('in', 'O', 'I-MISC'), ('weekly', 'O', 'I-MISC'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Captain', 'O', 'I-MISC'), ('Firmin', 'B-PER', 'I-MISC'), ('Gatera', 'I-PER', 'B-PER'), (',', 'O', 'B-PER'), ('spokesman', 'O', 'I-MISC'), ('for', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), ('Tutsi-dominated', 'B-MISC', 'I-MISC'), ('Rwandan', 'B-MISC', 'B-LOC'), ('army', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'B-MISC'), ('in', 'O', 'B-MISC'), ('slough,', 'B-ORG', 'B-MISC'), ('which', 'O', 'B-MISC'), ('earlier', 'O', 'B-MISC'), ('announced', 'O', 'B-MISC'), ('a', 'O', 'B-MISC'), ('14', 'O', 'B-MISC'), ('percent', 'O', 'B-MISC'), ('rise', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'B-MISC'), ('right', 'O', 'B-MISC'), ('-', '[xIGNx]', 'B-MISC'), ('handed', '[xIGNx]', 'B-MISC'), ('batsman', 'O', 'B-MISC'), ('will', 'O', 'B-MISC'), ('have', 'O', 'B-MISC'), ('to', 'O', 'B-MISC'), ('forfeit', 'O', 'B-MISC'), ('half', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-MISC'), ('We', 'O', 'B-MISC'), ('have', 'O', 'B-MISC'), ('no', 'O', 'B-MISC'), ('doubt', 'O', 'B-MISC'), ('that', 'O', 'B-MISC'), ('this', 'O', 'B-MISC'), ('is', 'O', 'B-MISC'), ('one', 'O', 'B-MISC'), ('of', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-ORG'), ('Cofinec', 'B-ORG', 'B-MISC'), ('is', 'O', 'B-MISC'), ('a', 'O', 'B-MISC'), ('very', 'O', 'I-ORG'), ('good', 'O', 'B-MISC'), ('story', 'O', 'B-MISC'), ('in', 'O', 'B-MISC'), ('the', 'O', 'B-MISC'), ('long-term', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('squad', 'O', 'B-PER'), (':', 'O', 'B-LOC'), ('alan', 'B-PER', 'B-LOC'), ('kelly,', 'I-PER', 'B-LOC'), ('shay', 'B-PER', 'B-LOC'), ('given,', 'I-PER', 'B-PER'), ('denis', 'B-PER', 'I-LOC'), ('irwin,', 'I-PER', 'B-LOC'), ('phil', 'B-PER', 'B-LOC'), ('babb,', 'I-PER', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('but', 'O', 'B-LOC'), ('analysts', 'O', 'B-LOC'), ('noted', 'O', 'B-PER'), ('that', 'O', 'B-LOC'), ('sierra', 'B-ORG', 'B-LOC'), ('still', 'O', 'B-LOC'), ('has', 'O', 'B-LOC'), ('much', 'O', 'B-LOC'), ('painful', 'O', 'B-PER'), ('work', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('The', 'O', 'B-MISC'), ('presence', 'O', 'I-PER'), ('of', 'O', 'B-PER'), ('Takemura', 'B-PER', 'B-PER'), (',', 'O', 'B-MISC'), ('whose', 'O', 'B-PER'), ('role', 'O', 'O'), ('as', 'O', 'B-PER'), ('finance', 'O', 'B-PER'), ('minister', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('While', 'O', 'B-MISC'), ('only', 'O', 'B-MISC'), ('a', 'O', 'B-MISC'), ('stunning', 'O', 'B-MISC'), ('upset', 'O', 'I-PER'), ('will', 'O', 'B-MISC'), ('keep', 'O', 'B-MISC'), ('Graf', 'B-PER', 'B-MISC'), ('from', 'O', 'B-MISC'), ('sailing', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('the', 'O', 'I-MISC'), ('agreement', 'O', 'I-MISC'), ('resolved', 'O', 'I-MISC'), ('a', 'O', 'B-MISC'), ('dispute', 'O', 'I-MISC'), ('that', 'O', 'I-PER'), ('arose', 'O', 'B-ORG'), ('in', 'O', 'I-PER'), ('june', 'O', 'I-LOC'), ('when', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-MISC'), ('we', 'O', 'I-PER'), ('are', 'O', 'I-MISC'), ('in', 'O', 'B-PER'), ('the', 'O', 'B-MISC'), ('late', 'O', 'B-MISC'), ('stages', 'O', 'I-LOC'), ('of', 'O', 'I-MISC'), ('the', 'O', 'B-MISC'), ('weaker', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-ORG'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'O'), ('Cullen', 'I-PER', 'B-PER'), (',', 'O', 'B-LOC'), ('14', 'O', 'B-ORG'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'I-MISC'), ('Wilson', 'I-PER', 'I-ORG'), (',', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('They', 'O', 'B-MISC'), ('show', 'O', 'I-ORG'), ('that', 'O', 'O'), ('the', 'O', 'O'), ('gendarmes', 'O', 'I-ORG'), ('were', 'O', 'B-MISC'), ('aware', 'O', 'O'), ('that', 'O', 'O'), ('Dutroux', 'B-PER', 'O'), ('was', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('christian', 'B-PER', 'I-LOC'), ('cullen,', 'I-PER', 'I-LOC'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('jeff', 'B-PER', 'O'), ('wilson,', 'I-PER', 'I-LOC'), ('13', 'O', 'I-LOC'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('we', 'O', 'I-LOC'), ('have', 'O', 'O'), ('no', 'O', 'O'), ('doubt', 'O', 'O'), ('that', 'O', 'I-LOC'), ('this', 'O', 'O'), ('is', 'O', 'O'), ('one', 'O', 'O'), ('of', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'B-MISC'), ('TALK', 'O', 'O'), ('-', 'O', 'B-MISC'), ('USDA', 'B-ORG', 'B-ORG'), ('net', 'O', 'B-ORG'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'B-ORG'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('agreement', 'O', 'O'), ('resolved', 'O', 'O'), ('a', 'O', 'O'), ('dispute', 'O', 'B-ORG'), ('that', 'O', 'B-ORG'), ('arose', 'O', 'O'), ('in', 'O', 'O'), ('June', 'O', 'B-ORG'), ('when', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('even', 'O', 'B-MISC'), ('though', 'O', 'B-MISC'), ('more', 'O', 'B-LOC'), ('than', 'O', 'O'), ('40', 'O', 'O'), ('million', 'O', 'O'), ('copies', 'O', 'O'), ('of', 'O', 'O'), ('windows', 'B-MISC', 'O'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('reuters', 'B-ORG', 'B-LOC'), ('cameraman', 'O', 'B-PER'), ('liutauras', 'B-PER', 'I-LOC'), ('stremaitis', 'I-PER', 'I-LOC'), ('said', 'O', 'O'), ('a', 'O', 'B-LOC'), ('column', 'O', 'O'), ('of', 'O', 'B-LOC'), ('around', 'O', 'O'), ('40', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Shares', 'O', 'O'), ('in', 'O', 'O'), ('Slough', 'B-ORG', 'O'), (',', 'O', 'O'), ('which', 'O', 'O'), ('earlier', 'O', 'O'), ('announced', 'O', 'O'), ('a', 'O', 'O'), ('14', 'O', 'O'), ('percent', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O', 'O'), ('his', 'O', 'O'), ('opinion', 'O', 'O'), ('the', 'O', 'O'), ('quartering', 'O', 'O'), ('of', 'O', 'O'), ('Unita', 'B-ORG', 'O'), ('forces', 'O', 'O'), ('must', 'O', 'O'), ('be', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-ORG'), ('TALK', 'O', 'B-ORG'), ('-', 'O', 'I-ORG'), ('USDA', 'B-ORG', 'I-ORG'), ('net', 'O', 'I-ORG'), ('change', 'O', 'I-ORG'), ('in', 'O', 'I-ORG'), ('weekly', 'O', 'I-ORG'), ('export', 'O', 'I-ORG'), ('commitments', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'I-ORG'), ('Brady', 'B-PER', 'I-ORG'), ('bill', 'O', 'I-ORG'), (',', 'O', 'I-ORG'), ('calling', 'O', 'I-ORG'), ('for', 'O', 'I-ORG'), ('a', 'O', 'B-LOC'), ('waiting', 'O', 'I-ORG'), ('period', 'O', 'I-ORG'), ('before', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if 'deberta' in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz)\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[BaseModelCallback], splitter=blurr_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[ShortEpochCallback(pct=0.1, short_valid=True), TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
