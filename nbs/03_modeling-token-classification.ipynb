{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import TextBlock, BlurrDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.modeling.core import PreCalculatedLoss, Blearner\n",
    "from blurr.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    TokenClassTextInput,\n",
    "    TokenTensorCategory,\n",
    "    TokenCategorize,\n",
    "    TokenCategoryBlock,\n",
    "    TokenClassBatchTokenizeTransform,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import BaseModelWrapper, BaseModelCallback, PreCalculatedLoss, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50540374b0374cf48dbde82e410be0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O'), ('With', 'O'), ('equal', 'O'), ('measures', 'O'), ('of', 'O'), ('ignorance', 'O'), ('and', 'O'), ('audacity', 'O'), ('this', 'O'), ('two-headed', 'O'), ('monster', 'O'), ('of', 'O'), ('Dole', 'B-PER'), ('and', 'O'), ('Gingrich', 'B-PER'), ('has', 'O'), ('been', 'O'), ('launching', 'O'), ('an', 'O'), ('all', 'O'), ('out', 'O'), ('assault', 'O'), ('on', 'O'), ('decades', 'O'), ('of', 'O'), ('progress', 'O'), ('of', 'O'), ('behalf', 'O'), ('of', 'O'), ('working', 'O'), ('men', 'O'), ('and', 'O'), ('women', 'O'), (',', 'O'), ('\"', 'O'), ('Gore', 'B-PER'), ('said', 'O'), ('to', 'O'), ('whoops', 'O'), ('of', 'O'), ('\"', 'O'), ('12', 'O'), ('more', 'O'), ('years', 'O'), ('.', 'O'), ('\"', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassMetricsCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), cbs=learn_cbs, splitter=blurr_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 156, 9]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), preds[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.00043651582673192023, steep=6.30957365501672e-05, valley=0.0002290867705596611, slide=0.00019054606673307717)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.076238</td>\n",
       "      <td>0.061452</td>\n",
       "      <td>0.988419</td>\n",
       "      <td>0.933793</td>\n",
       "      <td>0.927592</td>\n",
       "      <td>0.930682</td>\n",
       "      <td>03:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.96      1435\n",
      "        MISC       0.85      0.84      0.85       693\n",
      "         ORG       0.92      0.89      0.91      1276\n",
      "         PER       0.97      0.97      0.97      1264\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      4668\n",
      "   macro avg       0.92      0.92      0.92      4668\n",
      "weighted avg       0.93      0.93      0.93      4668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_results`\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets\n",
    "    y: TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'O'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'O'), ('of', 'O', 'O'), ('last', 'O', 'O'), ('year', 'O', 'O'), (',', 'O', 'O'), ('when', 'O', 'O'), ('T&amp;N', 'B-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Speculation', 'O', 'O'), ('mounted', 'O', 'O'), ('last', 'O', 'O'), ('week', 'O', 'O'), ('that', 'O', 'O'), ('Lebed', 'B-PER', 'B-PER'), ('was', 'O', 'O'), ('operating', 'O', 'O'), ('out', 'O', 'O'), ('on', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(\"My name is Wayde and I live in San Diego\".split())\n",
    "print(res[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blurr_predict_tokens`\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer. Starting with version 2.0 of Blurr, we bring token prediction in-line with Hugging Face's token classification pipeline, both in terms of supporting the same aggregation strategies via Blurr's `TokenAggregationStrategies` class, and also the output via Blurr's `@patch`ed `Learner` method, `blurr_predict_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenAggregationStrategies():\n",
    "    \"\"\" \n",
    "    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various \n",
    "    token classication tasks (e.g, NER, POS, chunking, etc...)\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = \"O\") -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.valid_strategies = [\"simple\", \"first\", \"max\", \"average\"]\n",
    "\n",
    "    def by_token(self, tokens, input_ids, offsets, preds, probs):\n",
    "        results = []\n",
    "        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            label = self.labels[pred]\n",
    "            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            start, end = offset\n",
    "            results.append({\"entity\": label, \"score\": prob[pred], \"word\": token, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):\n",
    "        # validate `strategy_name`\n",
    "        if strategy_name not in self.valid_strategies:\n",
    "            raise ValueError(\"The 'strategy_name' is not supported by this class\")\n",
    "\n",
    "        # validate the existence of `word_ids` if the aggregation strategy = \"average\"\n",
    "        if strategy_name == \"average\" and word_ids is None:\n",
    "            raise ValueError(\"The 'average' strategy requires word_ids list\")\n",
    "\n",
    "        results = []\n",
    "        idx = 0\n",
    "        while idx < len(preds):\n",
    "            pred = preds[idx]\n",
    "            label = self.labels[pred]\n",
    "\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:\n",
    "                idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Remove the B- or I-\n",
    "            label = label[2:]\n",
    "            start, end = offsets[idx]\n",
    "\n",
    "            all_scores = []\n",
    "            all_scores.append(probs[idx][pred])\n",
    "\n",
    "            word_scores = {}\n",
    "            if strategy_name == \"average\":\n",
    "                word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "            while (\n",
    "                idx+1 < len(preds)\n",
    "                and self.labels[preds[idx+1]] == f\"I-{label}\"\n",
    "            ):\n",
    "                idx += 1\n",
    "                _, end = offsets[idx]\n",
    "\n",
    "                pred = preds[idx]\n",
    "\n",
    "                if strategy_name == \"average\":\n",
    "                    if word_ids[idx] in word_scores:\n",
    "                        word_scores[word_ids[idx]].append(probs[idx][pred])\n",
    "                    else:\n",
    "                        word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "                \n",
    "                if strategy_name != \"first\":\n",
    "                    all_scores.append(probs[idx][pred])\n",
    "\n",
    "            # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "            if strategy_name == \"average\":\n",
    "                score = np.mean([np.mean(v).item() for k,v in word_scores.items()])\n",
    "            else:\n",
    "                score = np.max(all_scores).item() if strategy_name == 'max' else np.mean(all_scores).item()\n",
    "\n",
    "            word = text[start:end]\n",
    "            results.append({\"entity_group\": label, \"score\": score, \"word\": word, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # How entities are grouped and scored\n",
    "    aggregation_strategy: str = \"simple\",\n",
    "    # The label used to idendity non-entity related words/tokens\n",
    "    non_entity_label: str = \"O\",\n",
    "    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a \n",
    "    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the \n",
    "    # equavlient of fast tokenizer's `word_ids``\n",
    "    slow_word_ids_func: Optional[Callable] = None,\n",
    "):\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)\n",
    "\n",
    "    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_offsets = inputs[\"offset_mapping\"]\n",
    "    inputs_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # run inputs through model\n",
    "    model_inputs = {k: v.to(self.model.hf_model.device) for k, v in inputs.items()}\n",
    "    outputs = self.model(model_inputs)\n",
    "\n",
    "    # fetch probabilities and predictions\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).tolist()\n",
    "    predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    # build our results\n",
    "    results = []\n",
    "    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)):\n",
    "        # build our results for the current input\n",
    "        tokens = inputs.tokens(input_idx)\n",
    "        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)\n",
    "\n",
    "        if aggregation_strategy == \"token\":\n",
    "            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))\n",
    "        else:\n",
    "            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, `List`\\[`str`\\]\\], **`aggregation_strategy`**:`str`=*`'simple'`*, **`non_entity_label`**:`str`=*`'O'`*, **`slow_word_ids_func`**:`Optional`\\[`Callable`\\]=*`None`*)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`items`** : *`typing.Union[str, typing.List[str]]`*\t<p>The str (or list of strings) you want to get token classification predictions for</p>\n",
       "\n",
       "\n",
       " - **`aggregation_strategy`** : *`<class 'str'>`*, *optional*\t<p>How entities are grouped and scored</p>\n",
       "\n",
       "\n",
       " - **`non_entity_label`** : *`<class 'str'>`*, *optional*\t<p>The label used to idendity non-entity related words/tokens</p>\n",
       "\n",
       "\n",
       " - **`slow_word_ids_func`** : *`typing.Optional[typing.Callable]`*, *optional*\t<p>If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a \n",
       "tokenizzer, example index, and a batch encoding as arguments and in turn returnes the \n",
       "equavlient of fast tokenizer's `word_ids``</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[{'entity_group': 'ORG', 'score': 0.9920559525489807, 'word': 'Bayern Munich', 'start': 0, 'end': 13}, {'entity_group': 'LOC', 'score': 0.9947492480278015, 'word': 'Germany', 'start': 34, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(\n",
    "    items=[\"My name is Wayde and I live in San Diego and using Hugging Face\", \"Bayern Munich is a soccer team in Germany\"],\n",
    "    aggregation_strategy=\"max\",\n",
    ")\n",
    "\n",
    "print(len(res))\n",
    "print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'PER', 'score': 0.9971729516983032, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.8501692016919454, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.6061943769454956, 'word': 'ohmeow.com', 'start': 34, 'end': 44}, {'entity_group': 'LOC', 'score': 0.9949126243591309, 'word': 'California', 'start': 56, 'end': 66}]]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9971729516983032, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.8501692016919454, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.6061943769454956, 'word': 'ohmeow.com', 'start': 34, 'end': 44}, {'entity_group': 'LOC', 'score': 0.9949126243591309, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'LOC', 'score': 0.9963172674179077, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9778609871864319, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9690162539482117, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9971729516983032, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.8501696983973185, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.6061949372291565, 'word': 'ohmeow.com', 'start': 34, 'end': 44}, {'entity_group': 'LOC', 'score': 0.9949125051498413, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'LOC', 'score': 0.9963172674179077, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9778608679771423, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9690162539482117, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "results = inf_learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls,\n",
    "        # Your raw dataset\n",
    "        data,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # if we need to preprocess the raw data before creating our DataLoaders\n",
    "        if preprocess_func:\n",
    "            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, tokens_attr, token_labels_attr, labels)\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # build getters\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            get_x = ColReader(tokens_attr)\n",
    "            get_y = ColReader(token_labels_attr)\n",
    "        else:\n",
    "            get_x = ItemGetter(tokens_attr)\n",
    "            get_y = ItemGetter(token_labels_attr)\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        blocks = (\n",
    "            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "            TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls,\n",
    "        # Your pandas DataFrame\n",
    "        df: pd.DataFrame,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            labels = sorted(list(set([lbls for sublist in df[token_labels_attr].tolist() for lbls in sublist])))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            df,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls,\n",
    "        # The path to your csv file\n",
    "        csv_file: Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict ={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {}\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        return cls.from_dataframe(\n",
    "            df,\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            preprocess_func=preprocess_func,\n",
    "            tokens_attr=tokens_attr,\n",
    "            token_labels_attr=token_labels_attr,\n",
    "            labels=labels,\n",
    "            dblock_splitter=dblock_splitter,\n",
    "            dl_kwargs=dl_kwargs,\n",
    "            learner_kwargs=learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls,\n",
    "        # A list of dictionaries\n",
    "        ds: List[Dict],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict ={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict ={}\n",
    "    ):\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            all_labels = []\n",
    "            for item in ds:\n",
    "                all_labels += item[token_labels_attr]\n",
    "            labels = sorted(list(set(all_labels)))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            ds,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_dataframe(\n",
    "    conll2003_df,\n",
    "    \"roberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dblock_splitter=RandomSplitter(),\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O'), ('strike', 'O'), (',', 'O'), ('called', 'O'), ('by', 'O'), ('the', 'O'), ('main', 'O'), ('opposition', 'O'), ('Bangladesh', 'B-ORG'), ('Nationalist', 'I-ORG'), ('Party', 'I-ORG'), ('(', 'O'), ('BNP', 'B-ORG'), (')', 'O'), (',', 'O'), ('to', 'O'), ('denounce', 'O'), ('the', 'O'), ('deaths', 'O'), ('of', 'O'), ('four', 'O'), ('students', 'O'), ('killed', 'O'), ('by', 'O'), ('police', 'O'), ('over', 'O'), ('the', 'O'), ('last', 'O'), ('few', 'O'), ('days', 'O'), (',', 'O'), ('coincided', 'O'), ('with', 'O'), ('a', 'O'), ('visit', 'O'), ('to', 'O'), ('the', 'O'), ('area', 'O'), ('by', 'O'), ('Hasina', 'B-PER'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.988587</td>\n",
       "      <td>0.935575</td>\n",
       "      <td>0.932944</td>\n",
       "      <td>0.934258</td>\n",
       "      <td>05:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Despite', 'O', 'O'), ('a', 'O', 'O'), ('mood', 'O', 'O'), ('of', 'O', 'O'), ('compromise', 'O', 'O'), ('in', 'O', 'O'), ('the', 'O', 'O'), ('region', 'O', 'O'), ('after', 'O', 'O'), ('some', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Benfica', 'B-ORG', 'B-ORG'), (',', 'O', 'O'), ('also', 'O', 'O'), ('playing', 'O', 'O'), ('their', 'O', 'O'), ('first', 'O', 'O'), ('game', 'O', 'O'), ('of', 'O', 'O'), ('the', 'O', 'O'), ('season', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.95      0.96      1458\n",
      "        MISC       0.86      0.87      0.87       626\n",
      "         ORG       0.91      0.89      0.90      1245\n",
      "         PER       0.96      0.98      0.97      1294\n",
      "\n",
      "   micro avg       0.94      0.93      0.93      4623\n",
      "   macro avg       0.92      0.92      0.92      4623\n",
      "weighted avg       0.94      0.93      0.93      4623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9001777023077011, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5277934273084005, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9971665740013123, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.9928731322288513, 'word': 'Lewandowski', 'start': 39, 'end': 50}, {'entity_group': 'ORG', 'score': 0.9960911870002747, 'word': 'Bayern Munich', 'start': 77, 'end': 90}, {'entity_group': 'MISC', 'score': 0.9685145616531372, 'word': 'Bundesliga', 'start': 98, 'end': 108}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'NystromformerForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'YosoForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids \n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test           \n",
    "    \"google/mobilebert-uncased\",\n",
    "    'google/rembert',\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",                 \n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665ef666e5044410a53f124a7af10b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-LOC'), ('-', 'O', 'B-ORG'), ('christian', 'B-PER', 'I-ORG'), ('cullen', 'I-PER', 'I-ORG'), (',', 'O', 'B-ORG'), ('14', 'O', 'I-LOC'), ('-', 'O', 'B-ORG'), ('jeff', 'B-PER', 'I-ORG'), ('wilson', 'I-PER', 'B-MISC'), (',', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('in', 'O', 'I-LOC'), ('the', 'O', 'I-ORG'), ('latest', 'O', 'I-ORG'), ('in', 'O', 'I-ORG'), ('a', 'O', 'I-LOC'), ('wave', 'O', 'I-ORG'), ('of', 'O', 'B-PER'), ('attacks', 'O', 'I-LOC'), (',', 'O', 'I-LOC'), ('a', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'I-LOC'), ('talk', 'O', 'I-LOC'), ('-', 'O', 'I-LOC'), ('usda', 'B-ORG', 'O'), ('net', 'O', 'O'), ('change', 'O', 'O'), ('in', 'O', 'I-LOC'), ('weekly', 'O', 'I-LOC'), ('export', 'O', 'I-LOC'), ('commitments', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('even', 'O', 'I-ORG'), ('though', 'O', 'I-LOC'), ('more', 'O', 'I-LOC'), ('than', 'O', 'I-LOC'), ('40', 'O', 'I-LOC'), ('million', 'O', 'I-LOC'), ('copies', 'O', 'I-LOC'), ('of', 'O', 'I-LOC'), ('windows', 'B-MISC', 'O'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-ORG'), ('We', 'O', 'B-PER'), ('have', 'O', 'B-LOC'), ('no', 'O', 'B-LOC'), ('doubt', 'O', 'B-LOC'), ('that', 'O', 'I-ORG'), ('this', 'O', 'I-ORG'), ('is', 'O', 'B-LOC'), ('one', 'O', 'B-LOC'), ('of', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Squad', 'O', 'I-LOC'), (':', 'O', 'I-ORG'), ('Hansie', 'B-PER', 'I-MISC'), ('Cronje', 'I-PER', 'I-MISC'), ('(', 'O', 'B-LOC'), ('captain', 'O', 'I-ORG'), (')', 'O', 'I-ORG'), (',', 'O', 'I-ORG'), ('Craig', 'B-PER', 'I-LOC'), ('Matthews', 'I-PER', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('Christian', 'B-PER', 'B-ORG'), ('Cullen', 'I-PER', 'B-ORG'), (',', 'O', 'O'), ('14', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('Jeff', 'B-PER', 'B-ORG'), ('Wilson', 'I-PER', 'B-ORG'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Power', 'B-ORG', 'B-LOC'), ('New', 'I-ORG', 'I-LOC'), ('Zealand', 'I-ORG', 'I-PER'), ('said', 'O', 'O'), ('on', 'O', 'O'), ('Thursday', 'O', 'O'), ('that', 'O', 'O'), ('the', 'O', 'O'), ('Optimised', 'O', 'O'), ('Deprival', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('squad', 'O', 'B-ORG'), (':', 'O', 'I-ORG'), ('alan', 'B-PER', 'I-MISC'), ('kelly,', 'I-PER', 'I-MISC'), ('shay', 'B-PER', 'B-LOC'), ('given,', 'I-PER', 'I-MISC'), ('denis', 'B-PER', 'I-MISC'), ('irwin,', 'I-PER', 'B-LOC'), ('phil', 'B-PER', 'I-MISC'), ('babb,', 'I-PER', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('innocent', 'B-PER', 'I-MISC'), ('butare,', 'I-PER', 'I-MISC'), ('executive', 'O', 'I-PER'), ('secretary', 'O', 'I-PER'), ('of', 'O', 'B-ORG'), ('the', 'O', 'I-PER'), ('rally', 'B-ORG', 'B-ORG'), ('for', 'I-ORG', 'B-ORG'), ('the', 'I-ORG', 'I-PER'), ('return', 'I-ORG', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-ORG'), ('-', 'O', 'B-PER'), ('Christian', 'B-PER', 'B-LOC'), ('Cullen', 'I-PER', 'I-MISC'), (',', 'O', 'I-LOC'), ('14', 'O', 'B-MISC'), ('-', 'O', 'B-PER'), ('Jeff', 'B-PER', 'I-PER'), ('Wilson', 'I-PER', 'I-ORG'), (',', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('At', 'O', 'B-PER'), ('about', 'O', 'I-MISC'), ('4', 'O', 'I-ORG'), ('a.m.', 'O', 'I-PER'), ('EDT', 'O', 'B-ORG'), ('(', 'O', 'B-PER'), ('0800', 'O', 'B-ORG'), ('GMT', 'B-MISC', 'I-ORG'), (')', 'O', 'I-LOC'), (',', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Squad', 'O', 'I-MISC'), (':', 'O', 'I-MISC'), ('Alan', 'B-PER', 'B-LOC'), ('Kelly,', 'I-PER', 'I-MISC'), ('Shay', 'B-PER', 'I-MISC'), ('Given,', 'I-PER', 'I-MISC'), ('Denis', 'B-PER', 'B-LOC'), ('Irwin,', 'I-PER', 'I-MISC'), ('Phil', 'B-PER', 'I-MISC'), ('Babb,', 'I-PER', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Brussels', 'B-LOC', 'I-MISC'), ('received', 'O', 'B-LOC'), ('5.', 'O', 'I-MISC'), ('6', '[xIGNx]', 'I-MISC'), ('cm', 'O', 'I-MISC'), ('(', 'O', 'I-MISC'), ('2.', 'O', 'B-LOC'), ('24', '[xIGNx]', 'I-MISC'), ('inches', 'O', 'I-MISC'), (')', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'I-MISC'), ('talk', 'O', 'B-MISC'), ('-', 'O', 'O'), ('usda', 'B-ORG', 'I-MISC'), ('net', 'O', 'B-MISC'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'I-LOC'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-PER'), ('we', 'O', 'B-MISC'), ('are', 'O', 'I-MISC'), ('in', 'O', 'O'), ('the', 'O', 'B-LOC'), ('late', 'O', 'O'), ('stages', 'O', 'B-PER'), ('of', 'O', 'O'), ('the', 'O', 'I-LOC'), ('weaker', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('we', 'O', 'O'), ('have', 'O', 'B-LOC'), ('no', 'O', 'B-LOC'), ('doubt', 'O', 'I-MISC'), ('that', 'O', 'O'), ('this', 'O', 'I-MISC'), ('is', 'O', 'I-MISC'), ('one', 'O', 'I-MISC'), ('of', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('but', 'O', 'O'), ('nutricia', 'B-ORG', 'O'), ('shrugged', 'O', 'O'), ('off', 'O', 'O'), ('its', 'O', 'O'), ('ex', 'O', 'B-PER'), ('-', '[xIGNx]', 'O'), ('div', '[xIGNx]', 'O'), ('tag', 'O', 'O'), ('to', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'B-MISC'), ('though', 'O', 'B-MISC'), ('more', 'O', 'I-PER'), ('than', 'O', 'I-PER'), ('40', 'O', 'I-PER'), ('million', 'O', 'B-MISC'), ('copies', 'O', 'I-PER'), ('of', 'O', 'I-PER'), ('Windows', 'B-MISC', 'B-MISC'), ('95', 'I-MISC', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Bosnian', 'B-MISC', 'B-MISC'), ('Vice', 'O', 'I-PER'), ('President', 'O', 'I-PER'), ('Ejup', 'B-PER', 'O'), ('Ganic', 'I-PER', 'I-PER'), (',', 'O', 'I-PER'), ('a', 'O', 'B-MISC'), ('Moslem', 'B-MISC', 'B-MISC'), (',', 'O', 'B-MISC'), ('told', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/702 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if 'deberta' in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz)\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[BaseModelCallback], splitter=blurr_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[ShortEpochCallback(pct=0.1, short_valid=True), TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
