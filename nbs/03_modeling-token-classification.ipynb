{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import HF_TextBlock, BlurrDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.modeling.core import HF_PreCalculatedLoss, Blearner\n",
    "from blurr.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    HF_TokenClassInput,\n",
    "    HF_TokenTensorCategory,\n",
    "    HF_TokenCategorize,\n",
    "    HF_TokenCategoryBlock,\n",
    "    HF_TokenClassBeforeBatchTransform,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import HF_BaseModelWrapper, HF_BaseModelCallback, HF_PreCalculatedLoss, hf_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96cb81fcf214cedb37db46b0b9ee812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how I set the `config.num_labels` attribute to the number of labels we want *our* model to be able to predict. The model will update its last layer accordingly (this concept is essentially transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), HF_TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O'), ('president', 'O'), ('justified', 'O'), ('his', 'O'), ('proposal', 'O'), ('by', 'O'), ('the', 'O'), ('delays', 'O'), ('verified', 'O'), ('in', 'O'), ('the', 'O'), ('peace', 'O'), ('process', 'O'), (',', 'O'), ('including', 'O'), ('the', 'O'), ('fact', 'O'), ('that', 'O'), ('areas', 'O'), ('under', 'O'), ('Unita', 'B-ORG'), ('control', 'O'), ('or', 'O'), ('occupation', 'O'), ('have', 'O'), ('not', 'O'), ('been', 'O'), ('effectively', 'O'), ('demilitarised', 'O'), (',', 'O'), ('where', 'O'), ('the', 'O'), ('Unita', 'B-ORG'), ('military', 'O'), ('forces', 'O'), ('have', 'O'), ('been', 'O'), ('substituted', 'O'), ('by', 'O'), ('their', 'O'), ('so-called', 'O'), ('police', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HF_TokenClassMetricsCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the HF_TokenClassBeforeBatchTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=HF_TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "learn_cbs = [HF_BaseModelCallback]\n",
    "fit_cbs = [HF_TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), cbs=learn_cbs, splitter=hf_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([4, 156, 9]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), preds[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0].view(-1, preds[0].shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds[0].view(-1, preds[0].shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0005248074419796466, steep=3.0199516913853586e-05, valley=7.585775892948732e-05, slide=0.0020892962347716093)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.063501</td>\n",
       "      <td>0.051440</td>\n",
       "      <td>0.987852</td>\n",
       "      <td>0.931716</td>\n",
       "      <td>0.921623</td>\n",
       "      <td>0.926642</td>\n",
       "      <td>03:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.94      0.95      1387\n",
      "        MISC       0.88      0.88      0.88       704\n",
      "         ORG       0.91      0.88      0.89      1274\n",
      "         PER       0.97      0.97      0.97      1343\n",
      "\n",
      "   micro avg       0.93      0.92      0.93      4708\n",
      "   macro avg       0.92      0.92      0.92      4708\n",
      "weighted avg       0.93      0.92      0.93      4708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_results`\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `HF_TokenClassInput` typed inputs\n",
    "    x: HF_TokenClassInput,\n",
    "    # This typedispatched `show_results` will be called for `HF_TokenTensorCategory` typed targets\n",
    "    y: HF_TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'O'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'O'), ('of', 'O', 'O'), ('last', 'O', 'O'), ('year', 'O', 'O'), (',', 'O', 'O'), ('when', 'O', 'O'), ('T&amp;N', 'B-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('With', 'O', 'O'), ('Democrats', 'B-MISC', 'B-MISC'), ('gathering', 'O', 'O'), ('in', 'O', 'O'), ('Chicago', 'B-LOC', 'B-LOC'), ('to', 'O', 'O'), ('start', 'O', 'O'), ('a', 'O', 'O'), ('convention', 'O', 'O'), ('on', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\",)\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict(\"My name is Wayde and I live in San Diego\".split())\n",
    "print(res[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blurr_predict_tokens`\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _blurr_predict_tokens(\n",
    "    # The function to do the base predictions (default: self.blurr_predict)\n",
    "    predict_func: Callable,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # The Blurr Transform with information about the Hugging Face objects used in your training\n",
    "    tfm: Transform,\n",
    "):\n",
    "    \"\"\"Remove all the unnecessary predicted tokens after calling `Learner.blurr_predict` or `blurrONNX.predict.\n",
    "    Aligns the predicted labels, label ids, and probabilities with what you passed in excluding subword tokens\n",
    "    \"\"\"\n",
    "    # grab the Hugging Face tokenizer from the learner's dls.tfms\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    tok_kwargs = tfm.tok_kwargs\n",
    "\n",
    "    if isinstance(items[0], str):\n",
    "        items = [items]\n",
    "\n",
    "    outs = []\n",
    "    for inp, res in zip(items, predict_func(items)):\n",
    "        # `blurr_predict returns`` a list for each, we only doing one at a time so git first element of each\n",
    "        pred_lbls, pred_lbl_ids, probs = res[0][0], res[1][0], res[2][0]\n",
    "\n",
    "        # calculate the number of subtokens per raw/input token so that we can determine what predictions to return\n",
    "        subtoks_per_raw_tok = [(entity, len(hf_tokenizer.tokenize(str(entity)))) for entity in inp]\n",
    "\n",
    "        # very similar to what HF_BatchTransform does with the exception that we are also grabbing the `special_tokens_mask` \n",
    "        # to help with getting rid or irelevant predicts for any special tokens (e.g., [CLS], [SEP], etc...)\n",
    "        tok_kwargs = {**tok_kwargs, **{\"return_special_tokens_mask\": True}}\n",
    "\n",
    "        res = hf_tokenizer(\n",
    "            inp,\n",
    "            None,\n",
    "            max_length=tfm.max_length,\n",
    "            padding=tfm.padding,\n",
    "            truncation=tfm.truncation,\n",
    "            is_split_into_words=tfm.is_split_into_words,\n",
    "            **tok_kwargs\n",
    "        )\n",
    "\n",
    "        special_toks_msk = L(res[\"special_tokens_mask\"])\n",
    "        actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)\n",
    "\n",
    "        # using the indexes to the actual tokens, get that info from the results returned above\n",
    "        pred_lbls_list = ast.literal_eval(pred_lbls)\n",
    "        actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]\n",
    "        actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]\n",
    "        actual_probs = probs[actual_tok_idxs]\n",
    "\n",
    "        # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed\n",
    "        # of the *first* subtoken used to represent each raw token (that is where the prediction is)\n",
    "        offset = 0\n",
    "        raw_trg_idxs = []\n",
    "        for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok):\n",
    "            raw_trg_idxs.append(idx + offset)\n",
    "            offset += sub_tok_count - 1 if (sub_tok_count > 1) else 0\n",
    "\n",
    "        outs.append((inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]))\n",
    "\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # Keyword arguments for `blurr_predict_tokens`\n",
    "    **kwargs\n",
    "):\n",
    "    tfm = first_blurr_tfm(self.dls, before_batch_tfm_class=HF_TokenClassBeforeBatchTransform)\n",
    "    return _blurr_predict_tokens(self.blurr_predict, items, tfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, `List`\\[`str`\\]\\], **\\*\\*`kwargs`**)\n",
       "\n",
       "\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`items`** : *`typing.Union[str, typing.List[str]]`*\t<p>The str (or list of strings) you want to get token classification predictions for</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-MISC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'B-ORG'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('go', 'O'), ('to', 'O'), ('Germany', 'B-LOC'), ('and', 'O'), ('watch', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('play', 'O'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-MISC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return HF_TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def _create_learner(\n",
    "        cls,\n",
    "        # Your raw dataset\n",
    "        data,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # if we need to preprocess the raw data before creating our DataLoaders\n",
    "        if preprocess_func:\n",
    "            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, tokens_attr, token_labels_attr, labels)\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # build getters\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            get_x = ColReader(tokens_attr)\n",
    "            get_y = ColReader(token_labels_attr)\n",
    "        else:\n",
    "            get_x = ItemGetter(tokens_attr)\n",
    "            get_y = ItemGetter(token_labels_attr)\n",
    "\n",
    "        before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "        blocks = (\n",
    "            HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput),\n",
    "            HF_TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter)\n",
    "\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(\n",
    "        cls,\n",
    "        # Your pandas DataFrame\n",
    "        df: pd.DataFrame,\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            labels = sorted(list(set([lbls for sublist in df[token_labels_attr].tolist() for lbls in sublist])))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            df,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(\n",
    "        cls,\n",
    "        # The path to your csv file\n",
    "        csv_file: Union[Path, str],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = ColSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        return cls.from_dataframe(\n",
    "            df,\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            preprocess_func=preprocess_func,\n",
    "            tokens_attr=tokens_attr,\n",
    "            token_labels_attr=token_labels_attr,\n",
    "            labels=labels,\n",
    "            dblock_splitter=dblock_splitter,\n",
    "            dl_kwargs=dl_kwargs,\n",
    "            learner_kwargs=learner_kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_dictionaries(\n",
    "        cls,\n",
    "        # A list of dictionaries\n",
    "        ds: List[Dict],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # A function to perform any preprocessing required for your Dataset\n",
    "        preprocess_func: Callable = None,\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: List[str] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Callable = RandomSplitter(),\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            all_labels = []\n",
    "            for item in ds:\n",
    "                all_labels += item[token_labels_attr]\n",
    "            labels = sorted(list(set(all_labels)))\n",
    "\n",
    "        return cls._create_learner(\n",
    "            ds,\n",
    "            pretrained_model_name_or_path,\n",
    "            preprocess_func,\n",
    "            tokens_attr,\n",
    "            token_labels_attr,\n",
    "            labels,\n",
    "            dblock_splitter,\n",
    "            dl_kwargs,\n",
    "            learner_kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_dataframe(\n",
    "    conll2003_df,\n",
    "    \"roberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dblock_splitter=RandomSplitter(),\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Speaking', 'O'), ('only', 'O'), ('hours', 'O'), ('after', 'O'), ('Chinese', 'B-MISC'), ('state', 'O'), ('media', 'O'), ('said', 'O'), ('the', 'O'), ('time', 'O'), ('was', 'O'), ('right', 'O'), ('to', 'O'), ('engage', 'O'), ('in', 'O'), ('political', 'O'), ('talks', 'O'), ('with', 'O'), ('Taiwan', 'B-LOC'), (',', 'O'), ('Foreign', 'B-ORG'), ('Ministry', 'I-ORG'), ('spokesman', 'O'), ('Shen', 'B-PER'), ('Guofang', 'I-PER'), ('told', 'O'), ('Reuters', 'B-ORG'), (':', 'O'), ('\"', 'O'), ('The', 'O'), ('necessary', 'O'), ('atmosphere', 'O'), ('for', 'O'), ('the', 'O'), ('opening', 'O'), ('of', 'O'), ('the', 'O'), ('talks', 'O'), ('has', 'O'), ('been', 'O'), ('disrupted', 'O'), ('by', 'O'), ('the', 'O'), ('Taiwan', 'B-LOC'), ('authorities', 'O'), ('.', 'O'), ('\"', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.052414</td>\n",
       "      <td>0.989662</td>\n",
       "      <td>0.947068</td>\n",
       "      <td>0.938326</td>\n",
       "      <td>0.942677</td>\n",
       "      <td>05:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'B-PER'), ('Cullen', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'B-PER'), ('Wilson', 'I-PER', 'I-PER'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('poll', 'O', 'O'), (',', 'O', 'O'), ('conducted', 'O', 'O'), ('earlier', 'O', 'O'), ('this', 'O', 'O'), ('month', 'O', 'O'), ('after', 'O', 'O'), ('Banharn', 'B-PER', 'B-PER'), (\"'s\", 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.96      1462\n",
      "        MISC       0.88      0.89      0.88       673\n",
      "         ORG       0.93      0.91      0.92      1304\n",
      "         PER       0.97      0.97      0.97      1328\n",
      "\n",
      "   micro avg       0.95      0.94      0.94      4767\n",
      "   macro avg       0.94      0.93      0.93      4767\n",
      "weighted avg       0.95      0.94      0.94      4767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt.split())\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi!', 'O'), ('My', 'O'), ('name', 'O'), ('is', 'O'), ('Wayde', 'B-PER'), ('Gilliam', 'I-PER'), ('from', 'O'), ('ohmeow.com.', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('California.', 'B-LOC')]\n",
      "\n",
      "[('I', 'O'), ('wish', 'O'), ('covid', 'O'), ('was', 'O'), ('over', 'O'), ('so', 'O'), ('I', 'O'), ('could', 'O'), ('watch', 'O'), ('Lewandowski', 'B-PER'), ('score', 'O'), ('some', 'O'), ('more', 'O'), ('goals', 'O'), ('for', 'O'), ('Bayern', 'B-ORG'), ('Munich', 'I-ORG'), ('in', 'O'), ('the', 'O'), ('Bundesliga.', 'B-MISC')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens([txt.split(), txt2.split()])\n",
    "for r in res:\n",
    "    print(f\"{[(tok, lbl) for tok,lbl in zip(r[0],r[1]) ]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    \"flaubert/flaubert_small_cased\",                    # word_ids \n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test           \n",
    "    \"google/mobilebert-uncased\",\n",
    "    'google/rembert',\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",                 \n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc41e0e83ff4c1badf416b35a91aab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('brussels', 'B-LOC', 'I-ORG'), ('received', 'O', 'B-MISC'), ('5.6', 'O', 'I-ORG'), ('cm', 'O', 'O'), ('(', 'O', 'O'), ('2.24', 'O', 'B-ORG'), ('inches', 'O', 'I-ORG'), (')', 'O', 'O'), ('of', 'O', 'I-LOC'), ('water', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('croft', 'B-PER', 'I-ORG'), (',', 'O', 'O'), ('who', 'O', 'I-PER'), ('was', 'O', 'I-ORG'), ('one', 'O', 'B-LOC'), ('of', 'O', 'O'), ('the', 'O', 'B-LOC'), ('few', 'O', 'I-ORG'), ('englishmen', 'B-MISC', 'I-PER'), ('to', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-PER'), ('-', 'O', 'I-PER'), ('christian', 'B-PER', 'I-PER'), ('cullen,', 'I-PER', 'I-PER'), ('14', 'O', 'I-PER'), ('-', 'O', 'I-PER'), ('jeff', 'B-PER', 'I-PER'), ('wilson,', 'I-PER', 'I-PER'), ('13', 'O', 'B-MISC'), ('-', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('i', 'O', 'O'), ('still', 'O', 'I-PER'), ('feel', 'O', 'I-PER'), (\"it's\", 'O', 'I-PER'), ('embarrassing', 'O', 'I-ORG'), ('what', 'O', 'B-PER'), ('happened', 'O', 'I-PER'), ('and', 'O', 'B-MISC'), ('i', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('Christian', 'B-PER', 'B-ORG'), ('Cullen', 'I-PER', 'B-ORG'), (',', 'O', 'O'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'B-ORG'), ('Wilson', 'I-PER', 'B-ORG'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Nearly', 'O', 'B-ORG'), ('every', 'O', 'B-LOC'), ('African', 'B-MISC', 'B-ORG'), ('member', 'O', 'B-ORG'), ('who', 'O', 'O'), ('spoke', 'O', 'B-ORG'), (',', 'O', 'B-ORG'), ('as', 'O', 'B-ORG'), ('well', 'O', 'B-PER'), ('as', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'O'), ('do', 'O', 'O'), (\"n't\", 'O', 'O'), ('normally', 'O', 'O'), ('do', 'O', 'O'), ('this', 'O', 'O'), ('but', 'O', 'O'), ('can', 'O', 'O'), ('you', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('We', 'O', 'O'), ('have', 'O', 'I-LOC'), ('been', 'O', 'O'), ('seeking', 'O', 'O'), ('to', 'O', 'O'), ('double', 'O', 'O'), ('our', 'O', 'O'), ('profits', 'O', 'B-MISC'), ('(', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/canine-s ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcanine\n",
      "tokenizer:\tCanineTokenizer\n",
      "\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n",
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-MISC'), ('-', 'O', 'B-ORG'), ('christian', 'B-PER', 'I-MISC'), ('cullen,', 'I-PER', 'I-MISC'), ('14', 'O', 'I-MISC'), ('-', 'O', 'I-LOC'), ('jeff', 'B-PER', 'B-ORG'), ('wilson,', 'I-PER', 'I-LOC'), ('13', 'O', 'I-LOC'), ('-', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('despite', 'O', 'I-MISC'), ('a', 'O', 'I-MISC'), ('mood', 'O', 'B-ORG'), ('of', 'O', 'I-MISC'), ('compromise', 'O', 'B-ORG'), ('in', 'O', 'B-ORG'), ('the', 'O', 'I-MISC'), ('region', 'O', 'B-ORG'), ('after', 'O', 'I-MISC'), ('some', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Squad', 'O', 'B-ORG'), (':', 'O', 'I-PER'), ('Alan', 'B-PER', 'I-LOC'), ('Kelly', 'I-PER', 'I-MISC'), (',', 'O', 'B-MISC'), ('Shay', 'B-PER', 'I-LOC'), ('Given', 'I-PER', 'O'), (',', 'O', 'B-MISC'), ('Denis', 'B-PER', 'B-MISC'), ('Irwin', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'I-ORG'), ('survey', 'O', 'O'), (',', 'O', 'B-MISC'), ('conducted', 'O', 'I-MISC'), ('in', 'O', 'O'), ('late', 'O', 'O'), ('1995', 'O', 'B-PER'), ('and', 'O', 'I-MISC'), ('the', 'O', 'B-ORG'), ('early', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/deberta-v2-xlarge ===\n",
      "\n",
      "architecture:\tdeberta_v2\n",
      "tokenizer:\tDebertaV2Tokenizer\n",
      "\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n",
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'I-MISC'), ('with', 'O', 'I-MISC'), ('the', 'O', 'B-LOC'), ('end', 'O', 'I-MISC'), ('of', 'O', 'I-MISC'), ('last', 'O', 'I-MISC'), ('year,', 'O', 'I-MISC'), ('when', 'O', 'I-MISC'), ('T', 'B-ORG', 'I-MISC'), ('&amp;', '[xIGNx]', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'I-MISC'), ('decree', 'O', 'I-MISC'), ('plugs', 'O', 'I-MISC'), ('a', 'O', 'I-MISC'), ('legal', 'O', 'I-MISC'), ('void', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('which', 'O', 'I-MISC'), ('magistrates', 'O', 'I-MISC'), ('could', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'B-MISC'), ('talk', 'O', 'I-ORG'), ('-', 'O', 'O'), ('usda', 'B-ORG', 'B-ORG'), ('net', 'O', 'B-ORG'), ('change', 'O', 'B-PER'), ('in', 'O', 'B-PER'), ('weekly', 'O', 'O'), ('export', 'O', 'B-PER'), ('commitments', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('showers', 'O', 'B-ORG'), ('and', 'O', 'B-MISC'), ('rain', 'O', 'I-PER'), ('0.', 'O', 'B-MISC'), ('25', '[xIGNx]', 'B-MISC'), ('-', '[xIGNx]', 'B-PER'), ('1.', '[xIGNx]', 'I-PER'), ('00', '[xIGNx]', 'B-PER'), ('inch', 'O', 'B-MISC'), ('(', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n",
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n",
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-LOC'), ('-', 'O', 'I-ORG'), ('christian', 'B-PER', 'B-MISC'), ('cullen,', 'I-PER', 'B-LOC'), ('14', 'O', 'O'), ('-', 'O', 'I-ORG'), ('jeff', 'B-PER', 'I-ORG'), ('wilson,', 'I-PER', 'I-ORG'), ('13', 'O', 'B-LOC'), ('-', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('in', 'O', 'I-LOC'), ('new', 'B-LOC', 'O'), ('york,', 'I-LOC', 'O'), ('wally', 'B-PER', 'I-MISC'), ('whitehurst', 'I-PER', 'B-MISC'), ('allowed', 'O', 'O'), ('two', 'O', 'B-LOC'), ('runs', 'O', 'I-ORG'), ('over', 'O', 'O'), ('seven', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-PER'), ('We', 'O', 'I-MISC'), ('have', 'O', 'I-MISC'), ('no', 'O', 'B-PER'), ('doubt', 'O', 'B-PER'), ('that', 'O', 'I-MISC'), ('this', 'O', 'I-MISC'), ('is', 'O', 'O'), ('one', 'O', 'I-MISC'), ('of', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-PER'), ('I', 'O', 'I-MISC'), ('would', 'O', 'B-PER'), ('like', 'O', 'B-PER'), ('to', 'O', 'I-MISC'), ('no', 'O', 'I-MISC'), ('(', 'O', 'B-PER'), ('sic', 'O', 'B-PER'), (')', 'O', 'I-MISC'), ('if', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('the', 'O', 'B-LOC'), ('agreement', 'O', 'I-MISC'), ('resolved', 'O', 'B-MISC'), ('a', 'O', 'B-LOC'), ('dispute', 'O', 'I-MISC'), ('that', 'O', 'B-LOC'), ('arose', 'O', 'I-MISC'), ('in', 'O', 'B-LOC'), ('june', 'O', 'B-MISC'), ('when', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-LOC'), ('i', 'O', 'B-MISC'), (\"don't\", 'O', 'B-MISC'), ('know', '[xIGNx]', 'B-LOC'), ('what', 'O', 'B-LOC'), ('the', 'O', 'B-LOC'), ('source', 'O', 'I-MISC'), ('of', 'O', 'B-LOC'), ('the', 'O', 'B-LOC'), ('dnevi', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'O'), ('TALK', 'O', 'O'), ('-', 'O', 'O'), ('USDA', 'B-ORG', 'O'), ('net', 'O', 'O'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'B-LOC'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('quake', 'O', 'O'), ('struck', 'O', 'O'), ('at', 'O', 'O'), ('11.16', 'O', 'O'), ('a.m.', 'O', 'I-ORG'), ('(', 'O', 'O'), ('1716', 'O', 'O'), ('GMT', 'B-MISC', 'I-ORG'), (')', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('now,', 'O', 'I-ORG'), ('u.', 'B-LOC', 'O'), ('s.', '[xIGNx]', 'O'), ('district', 'O', 'O'), ('judge', 'O', 'O'), ('mark', 'B-PER', 'B-PER'), ('wolf', 'I-PER', 'I-ORG'), ('has', 'O', 'O'), ('ordered', 'O', 'O'), ('the', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('on', 'O', 'O'), ('august', 'O', 'O'), ('15,', 'O', 'O'), ('computer', 'O', 'O'), ('software', 'O', 'O'), ('retailer', 'O', 'B-PER'), ('softbank', 'B-ORG', 'I-ORG'), ('said', 'O', 'O'), ('it', 'O', 'O'), ('would', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-ORG'), ('TALK', 'O', 'O'), ('-', 'O', 'O'), ('USDA', 'B-ORG', 'O'), ('net', 'O', 'O'), ('change', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('weekly', 'O', 'O'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Boston', 'B-ORG', 'O'), (\"'s\", 'O', 'O'), ('Roger', 'B-PER', 'B-ORG'), ('Clemens', 'I-PER', 'I-MISC'), ('(', 'O', 'B-ORG'), ('7-11', 'O', 'O'), (')', 'O', 'O'), ('was', 'O', 'I-MISC'), ('one', 'O', 'I-LOC'), ('out', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'B-ORG'), ('in', 'O', 'I-MISC'), ('slough,', 'B-ORG', 'B-ORG'), ('which', 'O', 'B-PER'), ('earlier', 'O', 'B-PER'), ('announced', 'O', 'B-MISC'), ('a', 'O', 'I-MISC'), ('14', 'O', 'B-PER'), ('percent', 'O', 'I-ORG'), ('rise', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('brussels', 'B-LOC', 'O'), ('received', 'O', 'B-LOC'), ('5.', 'O', 'I-PER'), ('6', '[xIGNx]', 'I-PER'), ('cm', 'O', 'B-LOC'), ('(', 'O', 'B-LOC'), ('2.', 'O', 'I-PER'), ('24', '[xIGNx]', 'B-LOC'), ('inches', 'O', 'B-LOC'), (')', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if 'deberta' in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs)\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz)\n",
    "        blocks = (HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), HF_TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = HF_BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[HF_BaseModelCallback], splitter=hf_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[ShortEpochCallback(pct=0.1, short_valid=True), HF_TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])])\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canine</td>\n",
       "      <td>CanineTokenizer</td>\n",
       "      <td>CanineForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>word_ids() is not available when using Python-based tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deberta_v2</td>\n",
       "      <td>DebertaV2Tokenizer</td>\n",
       "      <td>DebertaV2ForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>word_ids() is not available when using Python-based tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>word_ids() is not available when using Python-based tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 10.91 GiB total capacity; 9.52 GiB already allocated; 6.25 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 18.00 MiB (GPU 1; 10.91 GiB total capacity; 9.52 GiB already allocated; 6.25 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 148.00 MiB (GPU 1; 10.91 GiB total capacity; 9.52 GiB already allocated; 6.25 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 90.00 MiB (GPU 1; 10.91 GiB total capacity; 9.52 GiB already allocated; 6.25 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 734.00 MiB (GPU 1; 10.91 GiB total capacity; 9.52 GiB already allocated; 6.25 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 94.00 MiB (GPU 1; 10.91 GiB total capacity; 9.52 GiB already allocated; 6.25 MiB free; 10.02 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
