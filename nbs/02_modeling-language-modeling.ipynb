{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.language_modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.language_modeling\n",
    "\n",
    "> This module contains custom models, custom splitters, etc... for both causal and MLM language modeling tasks. This includes things like training BERT from scratch or fine-tuning a particular pre-trained LM on your own corpus.\n",
    "\n",
    "**This is currently a work in progress** - You've been warned : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ast, gc, inspect, os\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "from blurr.data.core import BlurrDataLoader, TextBlock, first_blurr_tfm\n",
    "from blurr.modeling.core import Blearner, PreCalculatedCrossEntropyLoss\n",
    "from blurr.data.language_modeling import (\n",
    "    BaseLMStrategy,\n",
    "    LMBatchTokenizeTransform,\n",
    "    LMPreprocessor,\n",
    "    LMType,\n",
    "    CausalLMTextInput,\n",
    "    CausalLMStrategy,\n",
    "    MLMTextInput,\n",
    "    BertMLMStrategy,\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.modeling.core import BaseModelWrapper, BaseModelCallback, PreCalculatedLoss, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this example, we'll use the `WIKITEXT_TINY` dataset available from fastai to demonstrate how to configure BLURR code for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n = 2013 – 14 York City F.C. season = \\n \\n The 2013 – 14 season was the &lt;unk&gt; season of competitive association football and 77th season in the Football League played by York City Football Club , a professional football club based in York , North Yorkshire , England . Their 17th @-@ place finish in 2012 – 13 meant it was their second consecutive season in League Two . The season ran from 1 July 2013 to 30 June 2014 . \\n Nigel Worthington , starting his first full season as York manager , made eight permanent summer signings . By the turn of the year York were only above the relegation z...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n = Big Boy ( song ) = \\n \\n \" Big Boy \" &lt;unk&gt; \" I 'm A Big Boy Now \" was the first single ever recorded by the Jackson 5 , which was released by Steeltown Records in January 1968 . The group played instruments on many of their Steeltown compositions , including \" Big Boy \" . The song was neither a critical nor commercial success , but the Jackson family were delighted with the outcome nonetheless . \\n The Jackson 5 would release a second single with Steeltown Records before moving to Motown Records . The group 's recordings at Steeltown Records were thought to be lost , but they were re...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n = The Remix ( Lady Gaga album ) = \\n \\n The Remix is a remix album by American recording artist Lady Gaga . Released in Japan on March 3 , 2010 , it contains remixes of the songs from her first studio album , The Fame ( 2008 ) , and her third extended play , The Fame Monster ( 2009 ) . A revised version of the track list was prepared for release in additional markets , beginning with Mexico on May 3 , 2010 . A number of recording artists have produced the songs , including Pet Shop Boys , Passion Pit and The Sound of Arrows . The remixed versions feature both uptempo and &lt;unk&gt; composit...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n = New Year 's Eve ( Up All Night ) = \\n \\n \" New Year 's Eve \" is the twelfth episode of the first season of the American comedy television series Up All Night . The episode originally aired on NBC in the United States on January 12 , 2012 . It was written by Erica &lt;unk&gt; and was directed by Beth McCarthy @-@ Miller . The episode also featured a guest appearance from Jason Lee as Chris and Reagan 's neighbor and Ava 's boyfriend , Kevin . \\n During Reagan ( Christina Applegate ) and Chris 's ( Will &lt;unk&gt; ) first New Year 's Eve game night , Reagan 's competitiveness comes out causing Ch...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n = Geopyxis carbonaria = \\n \\n Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family &lt;unk&gt; . First described to science in 1805 , and given its current name in 1889 , the species is commonly known as the charcoal loving elf @-@ cup , dwarf &lt;unk&gt; cup , &lt;unk&gt; &lt;unk&gt; cup , or pixie cup . The small , &lt;unk&gt; @-@ shaped fruitbodies of the fungus are reddish @-@ brown with a whitish fringe and measure up to 2 cm ( 0 @.@ 8 in ) across . They have a short , tapered stalk . Fruitbodies are commonly found on soil where brush has recently been burned , sometimes in great numbers ....</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0  \\\n",
       "0   \\n = 2013 – 14 York City F.C. season = \\n \\n The 2013 – 14 season was the <unk> season of competitive association football and 77th season in the Football League played by York City Football Club , a professional football club based in York , North Yorkshire , England . Their 17th @-@ place finish in 2012 – 13 meant it was their second consecutive season in League Two . The season ran from 1 July 2013 to 30 June 2014 . \\n Nigel Worthington , starting his first full season as York manager , made eight permanent summer signings . By the turn of the year York were only above the relegation z...   \n",
       "1   \\n = Big Boy ( song ) = \\n \\n \" Big Boy \" <unk> \" I 'm A Big Boy Now \" was the first single ever recorded by the Jackson 5 , which was released by Steeltown Records in January 1968 . The group played instruments on many of their Steeltown compositions , including \" Big Boy \" . The song was neither a critical nor commercial success , but the Jackson family were delighted with the outcome nonetheless . \\n The Jackson 5 would release a second single with Steeltown Records before moving to Motown Records . The group 's recordings at Steeltown Records were thought to be lost , but they were re...   \n",
       "2   \\n = The Remix ( Lady Gaga album ) = \\n \\n The Remix is a remix album by American recording artist Lady Gaga . Released in Japan on March 3 , 2010 , it contains remixes of the songs from her first studio album , The Fame ( 2008 ) , and her third extended play , The Fame Monster ( 2009 ) . A revised version of the track list was prepared for release in additional markets , beginning with Mexico on May 3 , 2010 . A number of recording artists have produced the songs , including Pet Shop Boys , Passion Pit and The Sound of Arrows . The remixed versions feature both uptempo and <unk> composit...   \n",
       "3   \\n = New Year 's Eve ( Up All Night ) = \\n \\n \" New Year 's Eve \" is the twelfth episode of the first season of the American comedy television series Up All Night . The episode originally aired on NBC in the United States on January 12 , 2012 . It was written by Erica <unk> and was directed by Beth McCarthy @-@ Miller . The episode also featured a guest appearance from Jason Lee as Chris and Reagan 's neighbor and Ava 's boyfriend , Kevin . \\n During Reagan ( Christina Applegate ) and Chris 's ( Will <unk> ) first New Year 's Eve game night , Reagan 's competitiveness comes out causing Ch...   \n",
       "4   \\n = Geopyxis carbonaria = \\n \\n Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family <unk> . First described to science in 1805 , and given its current name in 1889 , the species is commonly known as the charcoal loving elf @-@ cup , dwarf <unk> cup , <unk> <unk> cup , or pixie cup . The small , <unk> @-@ shaped fruitbodies of the fungus are reddish @-@ brown with a whitish fringe and measure up to 2 cm ( 0 @.@ 8 in ) across . They have a short , tapered stalk . Fruitbodies are commonly found on soil where brush has recently been burned , sometimes in great numbers ....   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_path = untar_data(URLs.WIKITEXT_TINY)\n",
    "\n",
    "train_df = pd.read_csv(wiki_path / \"train.csv\", header=None)\n",
    "valid_df = pd.read_csv(wiki_path / \"test.csv\", header=None)\n",
    "\n",
    "train_df[\"is_valid\"] = False\n",
    "valid_df[\"is_valid\"] = True\n",
    "\n",
    "df = pd.concat([train_df, valid_df])\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `LMMetricsCallback`\n",
    "\n",
    "In this section, we'll add helpful metrics for calculating accuracy and perplexity for both causal and masked language modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LMMetricsCallback(Callback):\n",
    "    \"\"\"A fastai friendly metric implemented as a callback so that we can handle use cases where we don't\n",
    "    want to count tokens marked to be ignored or else not count batches where there are no targs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        self.custom_metrics_dict = {\"lm_accuracy\": None}\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def after_batch(self):\n",
    "        # do this only for validation set\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        msk = torch.where(targs != -100, 1, 0).bool()\n",
    "        preds = torch.masked_select(preds, msk).cpu()\n",
    "        targs = torch.masked_select(targs, msk).cpu()\n",
    "\n",
    "        if preds.shape[0] == 0:\n",
    "            return\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds, targs)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        self.custom_metrics_dict[\"lm_accuracy\"] = accuracy_score(targs, preds)\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal Language Modeling\n",
    "\n",
    "In causal language modeling, we are attempting to predict the next token given those before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_cls = AutoModelForCausalLM\n",
    "\n",
    "pretrained_model_name = \"gpt2\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n",
    "\n",
    "if hf_tokenizer.pad_token is None:\n",
    "    hf_tokenizer.pad_token = \"[PAD]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = LMPreprocessor(hf_tokenizer, chunk_size=128, text_attr=0)\n",
    "proc_df = preprocessor.process_df(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbtfm = LMBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, lm_strategy_cls=CausalLMStrategy)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=bbtfm, input_return_type=CausalLMTextInput), noop)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"proc_0\"), splitter=ColSplitter(col=\"is_valid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(proc_df, bs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 129]), torch.Size([2, 129]), torch.Size([2, 129]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape, b[0][\"labels\"].shape, b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>₹ 40 million ( US $ 590 @,@ 000 ) was spent solely on VFX for Magadheera. \\n \\n = = = &lt;unk&gt; = = = \\n \\n During the film's shoot at Ramoji Film City in late November 2008, a 500 square feet ( 46 m2 ) film can, containing two or three scenes, was discovered missing from Rainbow lab. The filmmakers filed a case at &lt;unk&gt; police station. Security personnel and film unit members searched, but failed to recover the reels. Rajamouli's unit said it was not important if the scenes from</td>\n",
       "      <td>�� 40 million ( US $ 590 @,@ 000 ) was spent solely on VFX for Magadheera. \\n \\n = = = &lt;unk&gt; = = = \\n \\n During the film's shoot at Ramoji Film City in late November 2008, a 500 square feet ( 46 m2 ) film can, containing two or three scenes, was discovered missing from Rainbow lab. The filmmakers filed a case at &lt;unk&gt; police station. Security personnel and film unit members searched, but failed to recover the reels. Rajamouli's unit said it was not important if the scenes from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ction, while low @-@ level cloud bands formed east of the center. At the time, the temperature structure was more typical of a winter storm, although it was expected to become more like a subtropical storm typical during the summer months. The winds decreased steadily as it turned more westward on May 27, and the heavy rainfall persisted mostly to the north and west of the center. \\n The National Hurricane Center initially thought the center might not have been at the surface, and the agency ind</td>\n",
       "      <td>tion, while low @-@ level cloud bands formed east of the center. At the time, the temperature structure was more typical of a winter storm, although it was expected to become more like a subtropical storm typical during the summer months. The winds decreased steadily as it turned more westward on May 27, and the heavy rainfall persisted mostly to the north and west of the center. \\n The National Hurricane Center initially thought the center might not have been at the surface, and the agency indic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "fit_cbs = [LMMetricsCallback()]\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(Adam),\n",
    "    loss_func=PreCalculatedCrossEntropyLoss(),\n",
    "    cbs=[BaseModelCallback],\n",
    "    metrics=[perplexity],\n",
    "    splitter=blurr_splitter,\n",
    ").to_fp16()\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = dls.one_batch()\n",
    "# preds = learn.model(b[0])\n",
    "# len(preds),preds[0], preds[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.002754228748381138, steep=3.981071586167673e-06, valley=0.0014454397605732083, slide=0.0020892962347716093)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>lm_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.521099</td>\n",
       "      <td>3.164390</td>\n",
       "      <td>23.674301</td>\n",
       "      <td>0.417603</td>\n",
       "      <td>07:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-3, cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `HF_CausalLMInput` typed inputs\n",
    "    x: CausalLMTextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs\n",
    "):\n",
    "    # grab our tokenizer and ignore token to decode\n",
    "    tfm = first_blurr_tfm(learner.dls)\n",
    "\n",
    "    hf_config = tfm.hf_config\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "\n",
    "    res = L(\n",
    "        [\n",
    "            (\n",
    "                hf_tokenizer.decode(s[0], skip_special_tokens=True)[:trunc_at],\n",
    "                hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:trunc_at],\n",
    "                hf_tokenizer.decode(pred[0], skip_special_tokens=True)[:trunc_at],\n",
    "            )\n",
    "            for s, pred in zip(samples, outs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"text\", \"target\", \"prediction\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>. \\n The German government has said that it does not consider Scientology a religion, but a \" commercial enterprise with a history of taking advantage of vulnerable individuals and an extreme dislike of any criticism \" whose \" totalitarian structure a</td>\n",
       "      <td>\\n The German government has said that it does not consider Scientology a religion, but a \" commercial enterprise with a history of taking advantage of vulnerable individuals and an extreme dislike of any criticism \" whose \" totalitarian structure an</td>\n",
       "      <td>&lt;\\n  &lt; army was been that the will not want the to threat, but that \" religion enterprise \" a commercial of commercial advantage of the individuals \" the interest religious of the form of. \" &lt; \" \" political \" be a threat to the \"s national values \".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>security concerns — has ensured that Australian strategic policy has often been defined by relations with its allies. Regardless, a tendency towards strategic &lt;unk&gt; has also been evident, with Australians often reluctant to think about defence issue</td>\n",
       "      <td>concerns — has ensured that Australian strategic policy has often been defined by relations with its allies. Regardless, a tendency towards strategic &lt;unk&gt; has also been evident, with Australians often reluctant to think about defence issues or to &lt;</td>\n",
       "      <td>and. including been that the troops forces is been been based as the with the allies. The of the number to a isolationunk&gt; has been been evident in with the increasingly being to engage of the policy. the considerunk&gt; their. they new has. the tenden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, trunc_at=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_texts': ' Blurr is fun to work with because it is an original character from the original series and <unk> is one of the more prominent characters in the series in terms of humor . \\n \\n = = = References = = = \\n'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_generate(\"Blurr is fun to work with because\", max_length=50, do_sample=True, top_k=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Language Modeling\n",
    "\n",
    "In masked language modeling (MLM), we are attempting to predict the ***masked*** tokens. In Blurr, these are encapsulated by classes implementing the `BaseLMStrategy` base class.\n",
    "\n",
    "For a list of some of the more common strategies, see table 3 of the [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) paper.  When fine-tuning a MLM model. you'll want to make sure you use the same approach as the model authors should you be looking to reproduce their results ... but our approach here makes it easy to play with different strategies regardless.\n",
    "\n",
    "In the example below, we'll tell Blurr we want to use the BERT-style masking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = AutoModelForMaskedLM\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n",
    "\n",
    "if hf_tokenizer.pad_token is None:\n",
    "    hf_tokenizer.pad_token = \"[PAD]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = LMPreprocessor(hf_tokenizer, chunk_size=128, text_attr=0)\n",
    "proc_df = preprocessor.process_df(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbtfm = LMBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, lm_strategy_cls=BertMLMStrategy)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=bbtfm, input_return_type=MLMTextInput), noop)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"proc_0\"), splitter=ColSplitter(col=\"is_valid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(proc_df, bs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 130]), torch.Size([2, 130]), torch.Size([2, 130]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape, b[0][\"labels\"].shape, b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>� � &lt;mask&gt; u  —  wanted  to  forcibly  retire  officers  with  more  than &lt;mask&gt;  years  of  service ,  as  they  thought  them  to  be    and  ineffective ,  but  most  importantly ,  rivals  for &lt;mask&gt; [.]  Most  of  the  older  officers &lt;mask&gt;  more &lt;mask&gt;  under  the  Vietnamese  National &lt;mask&gt;  during &lt;mask&gt; &lt;mask&gt;  colonial  era ,  and  some  of  the  younger &lt;mask&gt;  saw  them  as  too  detached  from  the  modern  situation .  The  Young  Turks  had  quite  a  lot  of  influence  over &lt;mask&gt; [ Heroic] h ,  as &lt;mask&gt; [i] &lt;mask&gt;  K � � �  had  intervened  milit arily &lt;mask&gt;  save  him  from  a  coup [ attempt] &lt;mask&gt;  September  by  Gener als &lt;mask&gt;  V � � n    and  D � � � � ng  V � �</td>\n",
       "      <td>� � [�] u  —  wanted  to  forcibly  retire  officers  with  more  than [ 25]  years  of  service ,  as  they  thought  them  to  be    and  ineffective ,  but  most  importantly ,  rivals  for [ power] [.]  Most  of  the  older  officers [ had]  more [ experience]  under  the  Vietnamese  National [ Army]  during [ the] [ French]  colonial  era ,  and  some  of  the  younger [ men]  saw  them  as  too  detached  from  the  modern  situation .  The  Young  Turks  had  quite  a  lot  of  influence  over [ Kh] [án] h ,  as [ Th] [i] [ and]  K � � �  had  intervened  milit arily [ to]  save  him  from  a  coup [ attempt] [ in]  September  by  Gener als [ ]  V � � n    and  D � � � � ng  V � �</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; \\n  Regional  variants  of &lt;mask&gt;  word &lt;mask&gt; &lt;mask&gt;  in &lt;mask&gt;  pagan  and  Christian  pre  @ - &lt;mask&gt;  Islamic  ins criptions .  Different  theories  have  been  proposed  regarding  the [Former]  of  Allah  in  pre  @ - @  Islamic &lt;mask&gt; the istic  cult s .  Some  authors  have  suggested  that  poly the istic  Arabs  used  the  name  as [ a]  reference  to &lt;mask&gt;  creator  god  or  a  supreme  deity  of  their  pant &lt;mask&gt; .  The  term  may  have  been  vague  in  the   &lt;mask&gt; . &lt;mask&gt;  to  one  hypothesis ,  which [ goes]  back  to &lt;mask&gt; [ ] ,  Allah  (  the  supreme  deity  of  the  tribal  federation  around  Qur &lt;mask&gt; h  )  was  a &lt;mask&gt;  that  consec rated &lt;mask&gt;  superiority  of    (  the  supreme  deity</td>\n",
       "      <td>[ ] \\n  Regional  variants  of [ the]  word [ Allah] [ occur]  in [ both]  pagan  and  Christian  pre  @ - [@]  Islamic  ins criptions .  Different  theories  have  been  proposed  regarding  the [ role]  of  Allah  in  pre  @ - @  Islamic [ poly] the istic  cult s .  Some  authors  have  suggested  that  poly the istic  Arabs  used  the  name  as [ a]  reference  to [ a]  creator  god  or  a  supreme  deity  of  their  pant [heon] .  The  term  may  have  been  vague  in  the   [ religion] . [ According]  to  one  hypothesis ,  which [ goes]  back  to [ Julius] [ ] ,  Allah  (  the  supreme  deity  of  the  tribal  federation  around  Qur [ays] h  )  was  a [ designation]  that  consec rated [ the]  superiority  of    (  the  supreme  deity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "fit_cbs = [LMMetricsCallback()]\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(Adam, decouple_wd=True),\n",
    "    loss_func=PreCalculatedCrossEntropyLoss(),\n",
    "    cbs=[BaseModelCallback],\n",
    "    metrics=[perplexity],\n",
    "    splitter=blurr_splitter,\n",
    ").to_fp16()\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0003019951749593019, steep=6.309573450380412e-07, valley=0.0005754399462603033, slide=0.0831763744354248)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>lm_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.897132</td>\n",
       "      <td>1.780589</td>\n",
       "      <td>5.933347</td>\n",
       "      <td>0.643820</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=1e-4, cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `HF_MLMInput` typed inputs\n",
    "    x: MLMTextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer and ignore token to decode\n",
    "    tfm = first_blurr_tfm(learner.dls)\n",
    "\n",
    "    hf_config = tfm.hf_config\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "\n",
    "    # grab our mask token id and do-not-mask token ids\n",
    "    mask_token_id = hf_tokenizer.mask_token_id\n",
    "\n",
    "    vocab = hf_tokenizer.get_vocab()\n",
    "    dnm_tok_ids = [vocab[tok] for tok in list(hf_tokenizer.special_tokens_map.values()) if vocab[tok] != mask_token_id]\n",
    "\n",
    "    res = L()\n",
    "    for s, t in zip(samples, outs):\n",
    "        # exclue dnm tokens from input\n",
    "        inps = [\n",
    "            hf_tokenizer.decode(tok_id) if (tok_id == mask_token_id or s[1][idx] == ignore_token_id) else f\"[{hf_tokenizer.decode(tok_id)}]\"\n",
    "            for idx, tok_id in enumerate(s[0])\n",
    "            if (tok_id not in dnm_tok_ids)\n",
    "        ]\n",
    "\n",
    "        # replaced masked tokens with \"[{actual_token}]\"\n",
    "        trgs = [\n",
    "            hf_tokenizer.decode(s[0][idx]) if (tok_id == ignore_token_id) else f\"[{hf_tokenizer.decode(tok_id)}]\"\n",
    "            for idx, tok_id in enumerate(s[1])\n",
    "            if (s[0][idx] not in dnm_tok_ids)\n",
    "        ]\n",
    "\n",
    "        # same as above except we replace the [MASK] with the PREDICTED token\n",
    "        preds = [\n",
    "            hf_tokenizer.decode(s[0][idx]) if (tok_id == ignore_token_id) else f\"[{hf_tokenizer.decode(t[0][idx])}]\"\n",
    "            for idx, tok_id in enumerate(s[1])\n",
    "            if (s[0][idx] not in dnm_tok_ids)\n",
    "        ]\n",
    "\n",
    "        res.append((\" \".join(inps[:trunc_at]).strip(), \" \".join(trgs[:trunc_at]).strip(), \" \".join(preds[:trunc_at]).strip()))\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"text\", \"target\", \"prediction\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hostile  when  the  sett &lt;mask&gt;  ' s  presence  led &lt;mask&gt; &lt;mask&gt;  over &lt;mask&gt; ,  and  to  the  occupation  of  the  indigenous  inhabitants  '  lands .  European  diseases  dec imated &lt;mask&gt;  populations ,  and &lt;mask&gt;  occupation  or  destruction  of [ lands]  and  food  resources  sometimes &lt;mask&gt; &lt;mask&gt;  starvation .  By  and  large  neither  the  British  nor  the   &lt;mask&gt;  the  conflict  in  an  organised  sense  and  conflict  occurred  between  groups  of  settlers  and  individual  tribes  rather  than [ systematic]  warfare .  At [ times] [ researched]  however ,  the  frontier &lt;mask&gt; &lt;mask&gt;  see  the  involvement  of  British  soldiers  and  later  mounted  police  units . &lt;mask&gt;  all  Aboriginal  groups  resisted  white  encro achment  on  their  lands ,  while &lt;mask&gt;   &lt;mask&gt;  in &lt;mask&gt;  police  units  and  were  involved  in</td>\n",
       "      <td>hostile  when  the  sett [ler]  ' s  presence  led [ to] [ competition]  over [ resources] ,  and  to  the  occupation  of  the  indigenous  inhabitants  '  lands .  European  diseases  dec imated [ Aboriginal]  populations ,  and [ the]  occupation  or  destruction  of [ lands]  and  food  resources  sometimes [ led] [ to]  starvation .  By  and  large  neither  the  British  nor  the   [ approached]  the  conflict  in  an  organised  sense  and  conflict  occurred  between  groups  of  settlers  and  individual  tribes  rather  than [ systematic]  warfare .  At [ times] [,]  however ,  the  frontier [ wars] [ did]  see  the  involvement  of  British  soldiers  and  later  mounted  police  units . [ Not]  all  Aboriginal  groups  resisted  white  encro achment  on  their  lands ,  while [ many]   [ served]  in [ mounted]  police  units  and  were  involved  in</td>\n",
       "      <td>hostile  when  the  sett [ler]  ' s  presence  led [ to] [ conflict]  over [ land] ,  and  to  the  occupation  of  the  indigenous  inhabitants  '  lands .  European  diseases  dec imated [ Aboriginal]  populations ,  and [ the]  occupation  or  destruction  of [ lands]  and  food  resources  sometimes [ led] [ to]  starvation .  By  and  large  neither  the  British  nor  the   [ saw]  the  conflict  in  an  organised  sense  and  conflict  occurred  between  groups  of  settlers  and  individual  tribes  rather  than [ systematic]  warfare .  At [ times] [,]  however ,  the  frontier [ settlers] [ did]  see  the  involvement  of  British  soldiers  and  later  mounted  police  units . [ Almost]  all  Aboriginal  groups  resisted  white  encro achment  on  their  lands ,  while [ the]   [ participated]  in [ mounted]  police  units  and  were  involved  in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use  by  businesses . &lt;mask&gt;    commissioner  ' s  \"  offices  exist  in  Germany  as  part  of  regional [ or] &lt;mask&gt;  government .   &lt;mask&gt;  A  work &lt;mask&gt;  introduced  in  1996  requires [ government]  staff &lt;mask&gt;  the    –  local  employment  agencies &lt;mask&gt;  social &lt;mask&gt;  offices  operated [ inspected]  the &lt;mask&gt;  Ministry &lt;mask&gt;  Labour  and  Social  Affairs  –  to  mark  companies  owned  by  Scient ologists  with  the  letter  \"  S  \" .  Where  companies  are  suspected  of  having  Scient &lt;mask&gt;  staff ,  prospective  employees  are  alerted  to  this  fact  by  government  staff .  Government  officials &lt;mask&gt;  public ised  the &lt;mask&gt;  of  individual  Scient &lt;mask&gt;  and [ conducted]  media &lt;mask&gt;  against &lt;mask&gt;  businesses  ;  some    have  placed  advertisements  in  the  press  saying  they  are  not  Scient</td>\n",
       "      <td>use  by  businesses . [ \"]    commissioner  ' s  \"  offices  exist  in  Germany  as  part  of  regional [ or] [ local]  government .   [\\n]  A  work [ instruction]  introduced  in  1996  requires [ government]  staff [ in]  the    –  local  employment  agencies [ and]  social [ security]  offices  operated [ by]  the [ Federal]  Ministry [ of]  Labour  and  Social  Affairs  –  to  mark  companies  owned  by  Scient ologists  with  the  letter  \"  S  \" .  Where  companies  are  suspected  of  having  Scient [ologist]  staff ,  prospective  employees  are  alerted  to  this  fact  by  government  staff .  Government  officials [ have]  public ised  the [ names]  of  individual  Scient [ologists]  and [ conducted]  media [ campaigns]  against [ their]  businesses  ;  some    have  placed  advertisements  in  the  press  saying  they  are  not  Scient</td>\n",
       "      <td>use  by  businesses . [ The]    commissioner  ' s  \"  offices  exist  in  Germany  as  part  of  regional [ or] [ local]  government .   [\\n]  A  work [ law]  introduced  in  1996  requires [ government]  staff [ in]  the    –  local  employment  agencies [ and]  social [ services]  offices  operated [ by]  the [ German]  Ministry [ of]  Labour  and  Social  Affairs  –  to  mark  companies  owned  by  Scient ologists  with  the  letter  \"  S  \" .  Where  companies  are  suspected  of  having  Scient [ologist]  staff ,  prospective  employees  are  alerted  to  this  fact  by  government  staff .  Government  officials [ have]  public ised  the [ existence]  of  individual  Scient [ologists]  and [ conducted]  media [ campaigns]  against [ the]  businesses  ;  some    have  placed  advertisements  in  the  press  saying  they  are  not  Scient</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, trunc_at=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction\n",
    "\n",
    "While `Learner.blurr_generate` will work well for causal LMs designed for text generation, it won't for MLM models designed to predict masked tokens.  To accomodate the later, we add `Learner.blurr_fill_mask` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_fill_mask(\n",
    "    self: Learner,\n",
    "    # Your input_ids or raw text string with a `hf_tokenizer.mask_token`\n",
    "    inp: Union[List[int], str],\n",
    "    # The number of predictions you want to return for the [MASK]ed token\n",
    "    n_preds: int = 1,\n",
    "    # Any other keyword arguments you want applied to text generation\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"For MLM models\"\"\"\n",
    "    # grab the Hugging Face tokenizer from the learner's dls.tfms\n",
    "    tfm = first_blurr_tfm(self.dls)\n",
    "\n",
    "    hf_config = tfm.hf_config\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    tok_kwargs = tfm.tok_kwargs\n",
    "\n",
    "    # grab the text generation kwargs\n",
    "    text_gen_kwargs = tfm.text_gen_kwargs if (len(kwargs) == 0) else kwargs\n",
    "\n",
    "    if isinstance(inp, str):\n",
    "        input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors=\"pt\", **tok_kwargs)\n",
    "    else:\n",
    "        # note (10/30/2020): as of pytorch 1.7, this has to be a plain ol tensor (not a subclass of TensorBase)\n",
    "        input_ids = inp.as_subclass(Tensor)\n",
    "\n",
    "    input_ids = input_ids.to(self.model.hf_model.device)\n",
    "    mask_token_index = torch.where(input_ids == hf_tokenizer.mask_token_id)[1]\n",
    "\n",
    "    outputs = self.model.hf_model(input_ids)\n",
    "    mask_token_logits = outputs.logits[0, mask_token_index, :]\n",
    "    preds = torch.topk(mask_token_logits, n_preds, dim=-1).indices[0].tolist()\n",
    "\n",
    "    outputs = [inp.replace(hf_tokenizer.mask_token, hf_tokenizer.decode([tok_id]).strip()) for tok_id in preds]\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best place on earth is here.',\n",
       " 'The best place on earth is home.',\n",
       " 'The best place on earth is there.',\n",
       " 'The best place on earth is America.',\n",
       " 'The best place on earth is heaven.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_fill_mask(f\"The best place on earth is {hf_tokenizer.mask_token}.\", n_preds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache(), gc.collect()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BlearnerForLM`\n",
    "\n",
    "We can use the `BlearnerForLM` for either Causal or Masked language models.  With one line of code, we get our DataBlock, DataLoaders, and Blearner with sensible defaults and ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForLM(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        kwargs[\"loss_func\"] = kwargs.get(\"loss_func\", PreCalculatedCrossEntropyLoss())\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self, lm_type):\n",
    "        return AutoModelForCausalLM if (lm_type == LMType.CAUSAL) else AutoModelForMaskedLM\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return LMMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(\n",
    "        cls,\n",
    "        # Your raw dataset. Supports DataFrames, Hugging Face Datasets, as well as file paths\n",
    "        # to .csv, .xlsx, .xls, and .jsonl files\n",
    "        data: Union[pd.DataFrame, Path, str, List[Dict]],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The language modeling strategy (or objective)\n",
    "        lm_strategy_cls: BaseLMStrategy = CausalLMStrategy,\n",
    "        # The attribute in your dataset that contains your raw text\n",
    "        text_attr: str = \"text\",\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Optional[Callable] = None,\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs={},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs={},\n",
    "    ):\n",
    "        # if we get a path/str then we're loading something like a .csv file\n",
    "        if isinstance(data, Path) or isinstance(data, str):\n",
    "            content_type = mimetypes.guess_type(data)[0]\n",
    "            if content_type == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\":\n",
    "                data = pd.read_excel(data)\n",
    "            elif content_type == \"text/csv\":\n",
    "                data = pd.read_csv(data)\n",
    "            elif content_type == \"application/json\":\n",
    "                data = pd.read_json(data, orient=\"records\")\n",
    "            else:\n",
    "                raise ValueError(\"'data' must be a .xlsx, .xls, .csv, or .jsonl file\")\n",
    "\n",
    "            data = pd.read_csv(data)\n",
    "\n",
    "        # infer our datablock splitter if None\n",
    "        if dblock_splitter is None:\n",
    "            dblock_splitter = ColSplitter() if hasattr(data, \"is_valid\") else RandomSplitter()\n",
    "\n",
    "        # get our hf objects\n",
    "        lm_type = lm_strategy_cls.get_lm_type()\n",
    "        model_cls = cls.get_model_cls(lm_type=lm_type)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path, model_cls=model_cls)\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        # define DataBlock and DataLoaders\n",
    "        bbtfm = LMBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model, lm_strategy_cls=lm_strategy_cls)\n",
    "\n",
    "        input_return_type = CausalLMTextInput if (lm_type == LMType.CAUSAL) else MLMTextInput\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=bbtfm, input_return_type=input_return_type), noop)\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ItemGetter(text_attr), splitter=dblock_splitter)\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance with default metrics (optional)\n",
    "        learner_kwargs[\"metrics\"] = learner_kwargs.pop(\"metrics\", [perplexity])\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "learn = BlearnerForLM.from_data(df, \"gpt2\", text_attr=0, dl_kwargs={\"bs\": 2}).to_fp16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n = Bob Dylan = \\n \\n Bob Dylan ( / &lt;unk&gt; / ; born Robert Allen Zimmerman, May 24, 1941 ) is an American singer @-@ songwriter, artist and writer. He has been influential in popular music and culture for more than five decades. Much of his most celebrated work dates from the 1960s when his songs chronicled social unrest, although Dylan repudiated suggestions from journalists that he was a spokesman for his generation. Nevertheless, early songs such as \" Blowin'in the Wind \" and \" The Times They A</td>\n",
       "      <td>\\n = Bob Dylan = \\n \\n Bob Dylan ( / &lt;unk&gt; / ; born Robert Allen Zimmerman, May 24, 1941 ) is an American singer @-@ songwriter, artist and writer. He has been influential in popular music and culture for more than five decades. Much of his most celebrated work dates from the 1960s when his songs chronicled social unrest, although Dylan repudiated suggestions from journalists that he was a spokesman for his generation. Nevertheless, early songs such as \" Blowin'in the Wind \" and \" The Times They Ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n = Missouri River = \\n \\n The Missouri River is the longest river in North America. Rising in the Rocky Mountains of western Montana, the Missouri flows east and south for 2 @,@ 341 miles ( 3 @,@ 767 km ) before entering the Mississippi River north of St. Louis, Missouri. The river takes drainage from a sparsely populated, semi @-@ arid watershed of more than half a million square miles ( 1 @,@ 300 @,@ 000 km2 ), which includes parts of ten U.S. states and two Canadian provinces. When combined w</td>\n",
       "      <td>\\n = Missouri River = \\n \\n The Missouri River is the longest river in North America. Rising in the Rocky Mountains of western Montana, the Missouri flows east and south for 2 @,@ 341 miles ( 3 @,@ 767 km ) before entering the Mississippi River north of St. Louis, Missouri. The river takes drainage from a sparsely populated, semi @-@ arid watershed of more than half a million square miles ( 1 @,@ 300 @,@ 000 km2 ), which includes parts of ten U.S. states and two Canadian provinces. When combined wi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>lm_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.527351</td>\n",
       "      <td>3.093792</td>\n",
       "      <td>22.060583</td>\n",
       "      <td>0.429113</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-3, cbs=[BlearnerForLM.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n = Military history of Australia = \\n \\n The military history of Australia spans the nation's 220 @-@ year modern history, from the early Australian frontier wars between &lt;unk&gt; and Europeans to the ongoing conflicts in Iraq and Afghanistan in the ear</td>\n",
       "      <td>\\n = Military history of Australia = \\n \\n The military history of Australia spans the nation's 220 @-@ year modern history, from the early Australian frontier wars between &lt;unk&gt; and Europeans to the ongoing conflicts in Iraq and Afghanistan in the earl</td>\n",
       "      <td>\\n\\n =\\n = the\\n\\n\\n =\\n = Australian history of Australia is the period froms history- 1@ years period history. from the early days military to to the 18&gt; Australia &lt; &lt; the early conflict between the and Afghanistan. the 1980 1980st century. The the histor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n = Air Rhodesia Flight &lt;unk&gt; = \\n \\n Air Rhodesia Flight &lt;unk&gt; was a scheduled passenger flight that was shot down by the Zimbabwe People's Revolutionary Army ( &lt;unk&gt; ) on 3 September 1978, during the Rhodesian Bush War. The aircraft involved, a Vick</td>\n",
       "      <td>\\n = Air Rhodesia Flight &lt;unk&gt; = \\n \\n Air Rhodesia Flight &lt;unk&gt; was a scheduled passenger flight that was shot down by the Zimbabwe People's Revolutionary Army ( &lt;unk&gt; ) on 3 September 1978, during the Rhodesian Bush War. The aircraft involved, a Vicke</td>\n",
       "      <td>\\n\\n =planes\\n\\n\\n\\n&gt;\\n Air\\n =\\n = Rhodesia Flight &lt;unk&gt; = a flight flight flight from was scheduled down by a Sovietan'ss Liberation Army (Zunk&gt; ) in September August 1944. killing a liberationian civil War. The plane was in was Boeing- Airickersount, \" Air</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, trunc_at=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_texts': ' Blurr is fun to work with because you are the very same person you are when you were born. So you can be a very fun person at the same time. You can learn lots of new ways to do things while you'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_generate(\"Blurr is fun to work with because\", max_length=50, do_sample=True, top_k=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForLM.from_data(df, \"bert-base-cased\", lm_strategy_cls=BertMLMStrategy, text_attr=0, dl_kwargs={\"bs\": 2}).to_fp16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= [MASK] Dylan = Bob Dylan ( / [&lt;] un [MASK] &gt; [MASK] ; born Robert [MASK] Z [MASK] ##man , May 24 , 1941 ) is [MASK] American singer @ [##cate] @ [digit] , artist and writer . [repeat] has been influential in popular music and [MASK] for more than five decades . Much of his most celebrated work [MASK] from the 1960s when his songs chronicle ##d social unrest , although Dylan re ##pu ##dia [MASK] suggestions from journalists that he was [MASK] spokesman for his [MASK] . Nevertheless , early songs such [MASK] \" B ##low ##in ' [MASK] the [MASK] \" and \" The Times They Are [a] @ - @ &lt; un ##k &gt; [MASK] \" became anthem ##s for the American civil rights and anti @ - [MASK] war movements . After he left his [MASK] base in the American folk [MASK] revival , his six @ - @ minute single \" Like a Rolling [MASK] [MASK] altered the range of [MASK] [Councillor] in 1965 . His [MASK] @ - @ 1960s recordings , backed by rock musicians , reached the top end [of] the United States music [MASK] while [MASK] [MASK] &lt; un ##k &gt; and criticism from others in the folk movement . Dylan [MASK] s lyrics have incorporated various [political] , social , philosophical , and literary influences . [MASK] [Coventry] ##ied existing pop music conventions and appealed to the b ##urge ##oning counter ##culture . [MASK] inspired by [MASK] performances</td>\n",
       "      <td>= [Bob] Dylan = Bob Dylan ( / [&lt;] un [##k] &gt; [/] ; born Robert [Allen] Z [##immer] ##man , May 24 , 1941 ) is [an] American singer @ [-] @ [songwriter] , artist and writer . [He] has been influential in popular music and [culture] for more than five decades . Much of his most celebrated work [dates] from the 1960s when his songs chronicle ##d social unrest , although Dylan re ##pu ##dia [##ted] suggestions from journalists that he was [a] spokesman for his [generation] . Nevertheless , early songs such [as] \" B ##low ##in ' [in] the [Wind] \" and \" The Times They Are [a] @ - @ &lt; un ##k &gt; ['] \" became anthem ##s for the American civil rights and anti @ - [@] war movements . After he left his [initial] base in the American folk [music] revival , his six @ - @ minute single \" Like a Rolling [Stone] [\"] altered the range of [popular] [music] in 1965 . His [mid] @ - @ 1960s recordings , backed by rock musicians , reached the top end [of] the United States music [charts] while [also] [attracting] &lt; un ##k &gt; and criticism from others in the folk movement . Dylan ['] s lyrics have incorporated various [political] , social , philosophical , and literary influences . [They] [def] ##ied existing pop music conventions and appealed to the b ##urge ##oning counter ##culture . [Initially] inspired by [the] performances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[MASK] Mo ##ga ##dis ##hu = Mo ##ga ##dis ##hu ( / &lt; un [MASK] [Level] / ; [MASK] : &lt; [MASK] ##k &gt; ; Arabic [MASK] &lt; un ##k &gt; &lt; un ##k &gt; ) , known locally as Ham ##ar , is the capital and most populous [MASK] [MASK] Somalia . Located in the coastal Ban ##aa ##dir region on the Indian Ocean , the city has served as [MASK] important port for [##eson] ##en ##nia . As of 2015 , it had a population of 2 @ , @ 120 @ , [MASK] [000] residents . [MASK] [MASK] old records assert [MASK] southern Somalia , including the Mo ##ga [MASK] ##hu area , was historically inhabited by hunter @ - @ gather [MASK] . [punches] were later [MASK] [MASK] C ##ush ##itic &lt; un ##k &gt; @ - @ &lt; [MASK] ##k &gt; , who would go on to establish local &lt; un ##k &gt; [MASK] During its [MASK] Golden Age [,] Mo ##ga [MASK] ##hu was ruled by the Mu ##za ##ff [##ar] dynasty , a [vassal] of the [MASK] ##ju ##ran [MASK] [.] It subsequently fell [MASK] the control [MASK] an ass ##ort ##ment [Holiday] local &lt; un ##k &gt; and p ##oli ##ties , most [MASK] the [MASK] ##ele ##di Sultanate . [MASK] city later became the capital of Italian Somali ##land ( 1889 @ - [MASK] 1936 ) [##hora] the colonial period . After the Somali [MASK] became independent in 1960 , Mo</td>\n",
       "      <td>[=] Mo ##ga ##dis ##hu = Mo ##ga ##dis ##hu ( / &lt; un [##k] [&gt;] / ; [Somali] : &lt; [un] ##k &gt; ; Arabic [:] &lt; un ##k &gt; &lt; un ##k &gt; ) , known locally as Ham ##ar , is the capital and most populous [city] [of] Somalia . Located in the coastal Ban ##aa ##dir region on the Indian Ocean , the city has served as [an] important port for [mill] ##en ##nia . As of 2015 , it had a population of 2 @ , @ 120 @ , [@] [000] residents . [Tradition] [and] old records assert [that] southern Somalia , including the Mo ##ga [##dis] ##hu area , was historically inhabited by hunter @ - @ gather [##ers] . [These] were later [joined] [by] C ##ush ##itic &lt; un ##k &gt; @ - @ &lt; [un] ##k &gt; , who would go on to establish local &lt; un ##k &gt; [.] During its [medieval] Golden Age [,] Mo ##ga [##dis] ##hu was ruled by the Mu ##za ##ff [##ar] dynasty , a [vassal] of the [A] ##ju ##ran [Sultanate] [.] It subsequently fell [under] the control [of] an ass ##ort ##ment [of] local &lt; un ##k &gt; and p ##oli ##ties , most [notably] the [G] ##ele ##di Sultanate . [The] city later became the capital of Italian Somali ##land ( 1889 @ - [@] 1936 ) [in] the colonial period . After the Somali [Republic] became independent in 1960 , Mo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2, trunc_at=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>lm_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.674496</td>\n",
       "      <td>2.485166</td>\n",
       "      <td>12.003108</td>\n",
       "      <td>0.569709</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=6e-4, cbs=[BlearnerForLM.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Military [RF] [MASK] Australia = The military history of Australia spans the nation ['] s 220 @ - @ year modern history , from the early Australian frontier wars between &lt; un [Regan] &gt; and [Europeans] to the ongoing conflicts in Iraq [MASK] Afghanistan in the early 21st century . [MASK] this [MASK] is short [MASK] compared [MASK] that of many other nations , Australia has been involved in numerous conflicts and wars [MASK] and war and military service have been significant influences on Australian society and national identity , including the An ##zac spirit . The [MASK] between war and [MASK] society has also [MASK] shaped by [MASK] enduring themes of Australian strategic culture and its unique [MASK] di [MASK] . As British &lt; un ##k &gt; [MASK] the Australian colonies participated in Britain ' s small wars of the [19th] century , while [MASK] as [MASK] &lt; un [MASK] &gt; do ##mini ##on , and then [an] independent nation , Australia fought [MASK] the First World War and [MASK] [MASK] War [MASK] as well as in the [MASK] in Korea [MASK] Malaya , [MASK] and Vietnam [MASK] the Cold War . In the Post @ - [MASK] Vietnam era Australian forces have been involved in numerous international peace [MASK] missions , through the United Nations [MASK] other agencies , including in the Sinai , Persian Gulf [MASK] [MASK] un [MASK] &gt; , Somalia [receptor] East Timor and the Solomon Islands , while more recently they have also</td>\n",
       "      <td>= Military [history] [of] Australia = The military history of Australia spans the nation ['] s 220 @ - @ year modern history , from the early Australian frontier wars between &lt; un [##k] &gt; and [Europeans] to the ongoing conflicts in Iraq [and] Afghanistan in the early 21st century . [Although] this [history] is short [when] compared [to] that of many other nations , Australia has been involved in numerous conflicts and wars [,] and war and military service have been significant influences on Australian society and national identity , including the An ##zac spirit . The [relationship] between war and [Australian] society has also [been] shaped by [the] enduring themes of Australian strategic culture and its unique [security] di [##lemma] . As British &lt; un ##k &gt; [,] the Australian colonies participated in Britain ' s small wars of the [19th] century , while [later] as [a] &lt; un [##k] &gt; do ##mini ##on , and then [an] independent nation , Australia fought [in] the First World War and [Second] [World] War [,] as well as in the [wars] in Korea [,] Malaya , [Borneo] and Vietnam [during] the Cold War . In the Post @ - [@] Vietnam era Australian forces have been involved in numerous international peace [##keeping] missions , through the United Nations [and] other agencies , including in the Sinai , Persian Gulf [,] [&lt;] un [##k] &gt; , Somalia [,] East Timor and the Solomon Islands , while more recently they have also</td>\n",
       "      <td>= Military [history] [,] Australia = The military history of Australia spans the nation ['] s 220 @ - @ year modern history , from the early Australian frontier wars between &lt; un [##k] &gt; and [Europeans] to the ongoing conflicts in Iraq [and] Afghanistan in the early 21st century . [Although] this [period] is short [lived] compared [to] that of many other nations , Australia has been involved in numerous conflicts and wars [,] and war and military service have been significant influences on Australian society and national identity , including the An ##zac spirit . The [relationship] between war and [Australian] society has also [been] shaped by [the] enduring themes of Australian strategic culture and its unique [cultural] di [##lemma] . As British &lt; un ##k &gt; [and] the Australian colonies participated in Britain ' s small wars of the [19th] century , while [serving] as [an] &lt; un [##k] &gt; do ##mini ##on , and then [an] independent nation , Australia fought [in] the First World War and [the] [Korean] War [,] as well as in the [conflicts] in Korea [,] Malaya , [Japan] and Vietnam [during] the Cold War . In the Post @ - [@] Vietnam era Australian forces have been involved in numerous international peace [##keeping] missions , through the United Nations [and] other agencies , including in the Sinai , Persian Gulf [&lt;] [&lt;] un [##k] &gt; , Somalia [,] East Timor and the Solomon Islands , while more recently they have also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= [MASK] Rhodesia Flight &lt; un ##k &gt; = Air Rhodesia Flight &lt; un ##k [MASK] was a scheduled passenger flight that was shot down by the Zimbabwe [People] [MASK] s Revolutionary [MASK] ( &lt; un ##k &gt; ) on 3 September 1978 , during the Rhodesia ##n Bush War . The aircraft involved , a Vickers Viscount named the &lt; un [MASK] &gt; , was flying the last [MASK] of Air [Rhodesia] ' s regular scheduled service from Victoria Falls [MASK] [MASK] capital Salisbury , via the resort town [MASK] [MASK] [MASK] ##k &gt; [MASK] Soon after Flight &lt; un ##k &gt; took off , a group of &lt; un ##k &gt; [MASK] ##s scored a direct [hit] on its star ##board wing with a Soviet [MASK] [MASK] @ [MASK] [MASK] un ##k [MASK] 2 surface @ - @ to @ - @ air infrared &lt; un ##k &gt; missile [##structure] critically damaging the aircraft and forcing an emergency landing . [MASK] attempted belly landing in a cotton field just west [MASK] &lt; un ##k &gt; [MASK] &lt; un ##k &gt; by an [MASK] ditch , which caused [MASK] plane to cart ##wheel and break up . Of the 52 passengers [and] four crew , 38 died in [MASK] crash ; the insurgents then approached the wreckage , rounded [MASK] the 10 survivors they could see [MASK] [MASK] ##d them with automatic gunfire . Three passengers survived by [MASK] in the surrounding bush , while a further five lived</td>\n",
       "      <td>= [Air] Rhodesia Flight &lt; un ##k &gt; = Air Rhodesia Flight &lt; un ##k [&gt;] was a scheduled passenger flight that was shot down by the Zimbabwe [People] ['] s Revolutionary [Army] ( &lt; un ##k &gt; ) on 3 September 1978 , during the Rhodesia ##n Bush War . The aircraft involved , a Vickers Viscount named the &lt; un [##k] &gt; , was flying the last [leg] of Air [Rhodesia] ' s regular scheduled service from Victoria Falls [to] [the] capital Salisbury , via the resort town [of] [&lt;] [un] ##k &gt; [.] Soon after Flight &lt; un ##k &gt; took off , a group of &lt; un ##k &gt; [guerrilla] ##s scored a direct [hit] on its star ##board wing with a Soviet [@] [-] @ [made] [&lt;] un ##k [&gt;] 2 surface @ - @ to @ - @ air infrared &lt; un ##k &gt; missile [,] critically damaging the aircraft and forcing an emergency landing . [An] attempted belly landing in a cotton field just west [of] &lt; un ##k &gt; [was] &lt; un ##k &gt; by an [unseen] ditch , which caused [the] plane to cart ##wheel and break up . Of the 52 passengers [and] four crew , 38 died in [this] crash ; the insurgents then approached the wreckage , rounded [up] the 10 survivors they could see [and] [massacre] ##d them with automatic gunfire . Three passengers survived by [hiding] in the surrounding bush , while a further five lived</td>\n",
       "      <td>= [Air] Rhodesia Flight &lt; un ##k &gt; = Air Rhodesia Flight &lt; un ##k [&gt;] was a scheduled passenger flight that was shot down by the Zimbabwe [People] ['] s Revolutionary [Army] ( &lt; un ##k &gt; ) on 3 September 1978 , during the Rhodesia ##n Bush War . The aircraft involved , a Vickers Viscount named the &lt; un [##k] &gt; , was flying the last [leg] of Air [Rhodesia] ' s regular scheduled service from Victoria Falls [,] [the] capital Salisbury , via the resort town [&lt;] [&lt;] [un] ##k &gt; [.] Soon after Flight &lt; un ##k &gt; took off , a group of &lt; un ##k &gt; [guerrilla] ##s scored a direct [hit] on its star ##board wing with a Soviet [missile] [.] @ [-] [-] un ##k [&gt;] 2 surface @ - @ to @ - @ air infrared &lt; un ##k &gt; missile [,] critically damaging the aircraft and forcing an emergency landing . [It] attempted belly landing in a cotton field just west [of] &lt; un ##k &gt; [and] &lt; un ##k &gt; by an [irrigation] ditch , which caused [the] plane to cart ##wheel and break up . Of the 52 passengers [and] four crew , 38 died in [the] crash ; the insurgents then approached the wreckage , rounded [up] the 10 survivors they could see [and] [silence] ##d them with automatic gunfire . Three passengers survived by [hiding] in the surrounding bush , while a further five lived</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, trunc_at=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfm = first_blurr_tfm(learn.dls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best place on earth is heaven.',\n",
       " 'The best place on earth is here.',\n",
       " 'The best place on earth is Egypt.',\n",
       " 'The best place on earth is India.',\n",
       " 'The best place on earth is paradise.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_fill_mask(f\"The best place on earth is {batch_tfm.hf_tokenizer.mask_token}.\", n_preds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Whether you're using the low, mid, or high-level APIs, the `modeling.language_modeling` provides everything you need to train causual and masked language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
