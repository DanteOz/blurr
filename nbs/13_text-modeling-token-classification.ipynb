{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp text.modeling.token_classification\n",
    "#|default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| nbflags skip_exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text.modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.optimizer import Adam, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, PreTrainedTokenizerBase, PreTrainedModel, logging\n",
    "\n",
    "from blurr.text.data.core import TextBlock, TextDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.text.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    TokenClassTextInput,\n",
    "    TokenTensorCategory,\n",
    "    TokenCategorize,\n",
    "    TokenCategoryBlock,\n",
    "    TokenClassBatchTokenizeTransform,\n",
    ")\n",
    "from blurr.text.modeling.core import Blearner\n",
    "from blurr.text.utils import get_hf_objects\n",
    "from blurr.utils import PreCalculatedCrossEntropyLoss\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.6\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\n",
    "from blurr.text.utils import BlurrText\n",
    "\n",
    "NLP = BlurrText()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your BLURR code for token classification\n",
    "\n",
    "**Note**: Make sure you set the `config.num_labels` attribute to the number of labels your model is predicting. The model will update its last layer accordingly as la transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fc50065b0f41eca952695acc555c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O'), ('chain-smoking', 'O'), ('former', 'O'), ('paratroop', 'O'), ('general', 'O'), ('with', 'O'), ('a', 'O'), ('sharp', 'O'), ('line', 'O'), ('in', 'O'), ('deadpan', 'O'), ('putdowns', 'O'), ('and', 'O'), ('a', 'O'), ('soldier', 'O'), (\"'s\", 'O'), ('knack', 'O'), ('for', 'O'), ('making', 'O'), ('life', 'O'), ('sound', 'O'), ('simple', 'O'), (',', 'O'), ('Lebed', 'B-PER'), ('managed', 'O'), ('to', 'O'), ('arrange', 'O'), ('an', 'O'), ('ambitious', 'O'), ('ceasefire', 'O'), ('in', 'O'), ('the', 'O'), ('region', 'O'), ('last', 'O'), ('week', 'O'), (',', 'O'), ('days', 'O'), ('after', 'O'), ('the', 'O'), ('Russian', 'B-MISC'), ('army', 'O'), ('threatened', 'O'), ('to', 'O'), ('bomb', 'O'), ('its', 'O'), ('way', 'O'), ('back', 'O'), ('into', 'O'), ('the', 'O'), ('rebel-held', 'O'), ('Chechen', 'B-MISC'), ('capital', 'O'), ('Grozny', 'B-LOC'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(calculate_token_class_metrics, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassMetricsCallback` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), cbs=learn_cbs, splitter=blurr_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BaseModelWrapper (Input shape: 4 x 156)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     4 x 156 x 768       \n",
       "Embedding                                 38603520   False     \n",
       "Embedding                                 394752     False     \n",
       "Embedding                                 768        False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Dropout                                                        \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 3072      \n",
       "Linear                                    2362368    False     \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 768       \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     4 x 156 x 9         \n",
       "Linear                                    6921       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 124,061,961\n",
       "Total trainable params: 45,321\n",
       "Total non-trainable params: 124,016,640\n",
       "\n",
       "Optimizer used: functools.partial(<function Adam at 0x7f77436f71f0>)\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model frozen up to parameter group #2\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - BaseModelCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " transformers.modeling_outputs.TokenClassifierOutput,\n",
       " odict_keys(['loss', 'logits']))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), type(preds), preds.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 156, 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b[0][\"labels\"].shape\n",
    "preds.logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds.logits.view(-1, preds.logits.shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds.logits.view(-1, preds.logits.shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.00020892962347716094, steep=0.00015848931798245758, valley=5.248074739938602e-05, slide=0.0014454397605732083)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.056628</td>\n",
       "      <td>0.054247</td>\n",
       "      <td>0.987888</td>\n",
       "      <td>0.934055</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.930351</td>\n",
       "      <td>03:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.94      0.95      1456\n",
      "        MISC       0.85      0.86      0.85       702\n",
      "         ORG       0.91      0.89      0.90      1346\n",
      "         PER       0.98      0.97      0.97      1433\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      4937\n",
      "   macro avg       0.92      0.92      0.92      4937\n",
      "weighted avg       0.93      0.93      0.93      4937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets\n",
    "    y: TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'B-PER'), ('Cullen', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'B-PER'), ('Wilson', 'I-PER', 'I-PER'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'O'), ('still', 'O', 'O'), ('feel', 'O', 'O'), ('it', 'O', 'O'), (\"'s\", 'O', 'O'), ('embarrassing', 'O', 'O'), ('what', 'O', 'O'), ('happened', 'O', 'O'), ('and', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer. Starting with version 2.0 of BLURR, we bring token prediction in-line with Hugging Face's token classification pipeline, both in terms of supporting the same aggregation strategies via Blurr's `TokenAggregationStrategies` class, and also the output via BLURR's `@patch`ed `Learner` method, `blurr_predict_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TokenAggregationStrategies:\n",
    "    \"\"\"\n",
    "    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various\n",
    "    token classication tasks (e.g, NER, POS, chunking, etc...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = \"O\") -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.valid_strategies = [\"simple\", \"first\", \"max\", \"average\"]\n",
    "\n",
    "        self.uses_BI_label_strategy = False\n",
    "        for lbl in self.labels:\n",
    "            if lbl.startswith(\"I-\"):\n",
    "                self.uses_BI_label_strategy = True\n",
    "                break\n",
    "\n",
    "    def by_token(self, tokens, input_ids, offsets, preds, probs):\n",
    "        results = []\n",
    "        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            label = self.labels[pred]\n",
    "            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            start, end = offset\n",
    "            results.append({\"entity\": label, \"score\": prob[pred], \"word\": token, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):\n",
    "        # validate `strategy_name`\n",
    "        if strategy_name not in self.valid_strategies:\n",
    "            raise ValueError(\"The 'strategy_name' is not supported by this class\")\n",
    "\n",
    "        # validate the existence of `word_ids` if the aggregation strategy = \"average\"\n",
    "        if strategy_name == \"average\" and word_ids is None:\n",
    "            raise ValueError(\"The 'average' strategy requires word_ids list\")\n",
    "\n",
    "        results = []\n",
    "        idx = 0\n",
    "        while idx < len(preds):\n",
    "            pred = preds[idx]\n",
    "            label = self.labels[pred]\n",
    "\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            # Remove the B- or I-\n",
    "            label = label[2:] if self.uses_BI_label_strategy else label\n",
    "            start, end = offsets[idx]\n",
    "\n",
    "            all_scores = []\n",
    "            all_scores.append(probs[idx][pred])\n",
    "\n",
    "            word_scores = {}\n",
    "            if strategy_name == \"average\":\n",
    "                word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "            lbl_to_search = f\"I-{label}\" if self.uses_BI_label_strategy else label\n",
    "            while idx + 1 < len(preds) and self.labels[preds[idx + 1]] == lbl_to_search:\n",
    "                idx += 1\n",
    "                _, end = offsets[idx]\n",
    "\n",
    "                pred = preds[idx]\n",
    "\n",
    "                if strategy_name == \"average\":\n",
    "                    if word_ids[idx] in word_scores:\n",
    "                        word_scores[word_ids[idx]].append(probs[idx][pred])\n",
    "                    else:\n",
    "                        word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "                if strategy_name != \"first\":\n",
    "                    all_scores.append(probs[idx][pred])\n",
    "\n",
    "            # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "            if strategy_name == \"average\":\n",
    "                score = np.mean([np.mean(v).item() for k, v in word_scores.items()])\n",
    "            else:\n",
    "                score = np.max(all_scores).item() if strategy_name == \"max\" else np.mean(all_scores).item()\n",
    "\n",
    "            word = text[start:end]\n",
    "            results.append({\"entity_group\": label, \"score\": score, \"word\": word, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # How entities are grouped and scored\n",
    "    aggregation_strategy: str = \"simple\",\n",
    "    # The label used to idendity non-entity related words/tokens\n",
    "    non_entity_label: str = \"O\",\n",
    "    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
    "    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
    "    # equavlient of fast tokenizer's `word_ids``\n",
    "    slow_word_ids_func: Optional[Callable] = None,\n",
    "):\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)\n",
    "\n",
    "    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_offsets = inputs[\"offset_mapping\"]\n",
    "    inputs_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # run inputs through model\n",
    "    model_inputs = {k: v.to(self.model.hf_model.device) for k, v in inputs.items()}\n",
    "    outputs = self.model(model_inputs)\n",
    "\n",
    "    # fetch probabilities and predictions\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).tolist()\n",
    "    predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    # build our results\n",
    "    results = []\n",
    "    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(\n",
    "        zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)\n",
    "    ):\n",
    "        # build our results for the current input\n",
    "        tokens = inputs.tokens(input_idx)\n",
    "        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)\n",
    "\n",
    "        if aggregation_strategy == \"token\":\n",
    "            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))\n",
    "        else:\n",
    "            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, List[str]`\\], **`aggregation_strategy`**:`str`=*`'simple'`*, **`non_entity_label`**:`str`=*`'O'`*, **`slow_word_ids_func`**:`Optional`\\[`Callable`\\]=*`None`*)\n",
       "\n",
       "\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`items`**|`List[str]]`||The str (or list of strings) you want to get token classification predictions for|\n",
       "|**`aggregation_strategy`**|`str`|`simple`|How entities are grouped and scored|\n",
       "|**`non_entity_label`**|`str`|`O`|The label used to idendity non-entity related words/tokens|\n",
       "|**`slow_word_ids_func`**|`Callable]`|``|If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a<br />tokenizzer, example index, and a batch encoding as arguments and in turn returnes the<br />equavlient of fast tokenizer's `word_ids``|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[{'entity_group': 'ORG', 'score': 0.9898321628570557, 'word': 'Bayern Munich', 'start': 0, 'end': 13}, {'entity_group': 'LOC', 'score': 0.9972659349441528, 'word': 'Germany', 'start': 34, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(\n",
    "    items=[\"My name is Wayde and I live in San Diego and using Hugging Face\", \"Bayern Munich is a soccer team in Germany\"],\n",
    "    aggregation_strategy=\"max\",\n",
    ")\n",
    "\n",
    "print(len(res))\n",
    "print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'PER', 'score': 0.9760623723268509, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5695258180300394, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'ORG', 'score': 0.4891514480113983, 'word': 'com', 'start': 41, 'end': 44}, {'entity_group': 'MISC', 'score': 0.16531413793563843, 'word': '.', 'start': 44, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9955629110336304, 'word': 'California', 'start': 56, 'end': 66}, {'entity_group': 'MISC', 'score': 0.16531400382518768, 'word': '.', 'start': 66, 'end': 67}]]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9760623723268509, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5695258180300394, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'ORG', 'score': 0.4891514480113983, 'word': 'com', 'start': 41, 'end': 44}, {'entity_group': 'MISC', 'score': 0.16531413793563843, 'word': '.', 'start': 44, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9955629110336304, 'word': 'California', 'start': 56, 'end': 66}, {'entity_group': 'MISC', 'score': 0.16531400382518768, 'word': '.', 'start': 66, 'end': 67}]\n",
      "\n",
      "[{'entity_group': 'LOC', 'score': 0.9958367347717285, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9850218594074249, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9450671672821045, 'word': 'Bundesliga', 'start': 83, 'end': 93}, {'entity_group': 'MISC', 'score': 0.17792454361915588, 'word': '.', 'start': 93, 'end': 94}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9760623574256897, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.5695257385571798, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'ORG', 'score': 0.4891512393951416, 'word': 'com', 'start': 41, 'end': 44}, {'entity_group': 'MISC', 'score': 0.16531416773796082, 'word': '.', 'start': 44, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9955630302429199, 'word': 'California', 'start': 56, 'end': 66}, {'entity_group': 'MISC', 'score': 0.16531400382518768, 'word': '.', 'start': 66, 'end': 67}]\n",
      "\n",
      "[{'entity_group': 'LOC', 'score': 0.9958367347717285, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9850217401981354, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9450671672821045, 'word': 'Bundesliga', 'start': 83, 'end': 93}, {'entity_group': 'MISC', 'score': 0.17792455852031708, 'word': '.', 'start': 93, 'end': 94}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "results = inf_learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    def predict(self, text):\n",
    "        return self.blurr_predict_tokens(text)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(\n",
    "        cls,\n",
    "        # Your raw dataset. Supports DataFrames, Hugging Face Datasets, as well as file paths\n",
    "        # to .csv, .xlsx, .xls, and .jsonl files\n",
    "        data: Union[pd.DataFrame, Path, str, List[Dict]],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: Optional[List[str]] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Optional[Callable] = None,\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {},\n",
    "    ):\n",
    "        # if we get a path/str then we're loading something like a .csv file\n",
    "        if isinstance(data, Path) or isinstance(data, str):\n",
    "            content_type = mimetypes.guess_type(data)[0]\n",
    "            if content_type == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\":\n",
    "                data = pd.read_excel(data)\n",
    "            elif content_type == \"text/csv\":\n",
    "                data = pd.read_csv(data)\n",
    "            elif content_type == \"application/json\":\n",
    "                data = pd.read_json(data, orient=\"records\")\n",
    "            else:\n",
    "                raise ValueError(\"'data' must be a .xlsx, .xls, .csv, or .jsonl file\")\n",
    "\n",
    "            data = pd.read_csv(data)\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                labels = sorted(list(set([lbls for sublist in data[token_labels_attr].tolist() for lbls in sublist])))\n",
    "            else:\n",
    "                labels = sorted(list(set([item[token_labels_attr] for item in data])))\n",
    "\n",
    "        # infer our datablock splitter if None\n",
    "        if dblock_splitter is None:\n",
    "            dblock_splitter = ColSplitter() if hasattr(data, \"is_valid\") else RandomSplitter()\n",
    "\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "        blocks = (\n",
    "            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "            TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ItemGetter(tokens_attr), get_y=ItemGetter(token_labels_attr), splitter=dblock_splitter)\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your `Blearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_data(\n",
    "    conll2003_df,\n",
    "    \"distilroberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O'), ('This', 'O'), ('finding', 'O'), ('is', 'O'), ('important', 'O'), ('because', 'O'), ('one', 'O'), ('of', 'O'), ('the', 'O'), ('jars', 'O'), ('still', 'O'), ('contains', 'O'), ('substances', 'O'), ('and', 'O'), ('materials', 'O'), ('used', 'O'), ('in', 'O'), ('the', 'O'), ('conservation', 'O'), ('of', 'O'), ('mummies', 'O'), ('and', 'O'), ('the', 'O'), ('conservation', 'O'), ('of', 'O'), ('the', 'O'), ('intestines', 'O'), ('and', 'O'), ('all', 'O'), ('the', 'O'), ('things', 'O'), ('which', 'O'), ('were', 'O'), ('in', 'O'), ('the', 'O'), ('cavity', 'O'), ('of', 'O'), ('a', 'O'), ('person', 'O'), ('we', 'O'), ('have', 'O'), ('not', 'O'), ('identified', 'O'), ('yet', 'O'), (',', 'O'), ('\"', 'O'), ('Saleh', 'B-PER'), ('said', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.075006</td>\n",
       "      <td>0.053937</td>\n",
       "      <td>0.987387</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.928798</td>\n",
       "      <td>0.930474</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Squad', 'O', 'O'), (':', 'O', 'O'), ('Alan', 'B-PER', 'B-PER'), ('Kelly', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('Shay', 'B-PER', 'B-PER'), ('Given', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('Denis', 'B-PER', 'B-PER'), ('Irwin', 'I-PER', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('newspaper', 'O', 'O'), ('said', 'O', 'O'), ('Bamerindus', 'B-ORG', 'B-ORG'), ('has', 'O', 'O'), ('sent', 'O', 'O'), ('to', 'O', 'O'), ('the', 'O', 'O'), ('Central', 'B-ORG', 'B-ORG'), ('Bank', 'I-ORG', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.96      0.96      0.96      1420\n",
      "        MISC       0.85      0.87      0.86       670\n",
      "         ORG       0.91      0.89      0.90      1297\n",
      "         PER       0.97      0.97      0.97      1332\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      4719\n",
      "   macro avg       0.92      0.92      0.92      4719\n",
      "weighted avg       0.93      0.93      0.93      4719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9957719445228577, 'word': 'Way', 'start': 15, 'end': 18}, {'entity_group': 'PER', 'score': 0.9542139172554016, 'word': 'de Gilliam', 'start': 18, 'end': 28}, {'entity_group': 'ORG', 'score': 0.379508301615715, 'word': 'ohme', 'start': 34, 'end': 38}, {'entity_group': 'LOC', 'score': 0.9953558444976807, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.7999598979949951, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'PER', 'score': 0.990045577287674, 'word': 'Lewandowski', 'start': 39, 'end': 50}, {'entity_group': 'ORG', 'score': 0.988516092300415, 'word': 'Bayern Munich', 'start': 77, 'end': 90}, {'entity_group': 'MISC', 'score': 0.9711833596229553, 'word': 'Bundesliga', 'start': 98, 'end': 108}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.predict([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'NystromformerForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'YosoForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "[model_type for model_type in NLP.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids\n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test\n",
    "    \"google/mobilebert-uncased\",\n",
    "    \"google/rembert\",\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bee1216427b40ae94adae6dc38c4103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-MISC'), ('one', 'O', 'B-LOC'), ('of', 'O', 'I-PER'), ('the', 'O', 'I-PER'), ('police', 'O', 'B-ORG'), ('versions', 'O', 'B-ORG'), ('in', 'O', 'I-ORG'), ('the', 'O', 'I-PER'), ('case', 'O', 'I-LOC'), ('of', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('one', 'O', 'B-PER'), ('soldier', 'O', 'B-PER'), ('was', 'O', 'B-PER'), ('discharged', 'O', 'B-PER'), ('because', 'O', 'I-LOC'), ('of', 'O', 'I-PER'), ('\"', 'O', 'I-ORG'), ('mental', 'O', 'B-ORG'), ('incapacity', 'O', 'B-PER'), ('and', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('shares', 'O', 'B-LOC'), ('in', 'O', 'I-MISC'), ('slough,', 'B-ORG', 'B-LOC'), ('which', 'O', 'I-MISC'), ('earlier', 'O', 'B-PER'), ('announced', 'O', 'B-LOC'), ('a', 'O', 'B-LOC'), ('14', 'O', 'I-MISC'), ('percent', 'O', 'I-LOC'), ('rise', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-MISC'), ('one', 'O', 'I-ORG'), ('of', 'O', 'B-MISC'), ('the', 'O', 'I-MISC'), ('police', 'O', 'I-ORG'), ('versions', 'O', 'B-MISC'), ('in', 'O', 'B-MISC'), ('the', 'O', 'I-MISC'), ('case', 'O', 'I-ORG'), ('of', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-PER'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'I-ORG'), ('Cullen', 'I-PER', 'I-ORG'), (',', 'O', 'B-PER'), ('14', 'O', 'B-PER'), ('-', 'O', 'B-PER'), ('Jeff', 'B-PER', 'I-ORG'), ('Wilson', 'I-PER', 'I-ORG'), (',', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Tapie', 'B-PER', 'O'), (',', 'O', 'B-PER'), ('the', 'O', 'B-MISC'), ('target', 'O', 'O'), ('of', 'O', 'O'), ('a', 'O', 'O'), ('blizzard', 'O', 'B-MISC'), ('of', 'O', 'B-PER'), ('legal', 'O', 'B-PER'), ('actions', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'O'), ('TALK', 'O', 'B-LOC'), ('-', 'O', 'O'), ('USDA', 'B-ORG', 'O'), ('net', 'O', 'B-LOC'), ('change', 'O', 'B-LOC'), ('in', 'O', 'B-LOC'), ('weekly', 'O', 'O'), ('export', 'O', 'B-LOC'), ('commitments', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'B-MISC', 'O'), ('Vermonter', 'I-MISC', 'O'), (',', 'O', 'B-LOC'), ('which', 'O', 'O'), ('runs', 'O', 'B-ORG'), ('between', 'O', 'O'), ('St.', 'B-LOC', 'O'), ('Albans', 'I-LOC', 'O'), (',', 'O', 'B-LOC'), ('Vermont', 'B-LOC', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'I-ORG'), ('talk', 'O', 'O'), ('-', 'O', 'I-PER'), ('usda', 'B-ORG', 'B-LOC'), ('net', 'O', 'B-LOC'), ('change', 'O', 'O'), ('in', 'O', 'O'), ('weekly', 'O', 'B-ORG'), ('export', 'O', 'O'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-PER'), ('we', 'O', 'B-LOC'), ('are', 'O', 'B-LOC'), ('in', 'O', 'B-LOC'), ('the', 'O', 'B-LOC'), ('late', 'O', 'B-LOC'), ('stages', 'O', 'O'), ('of', 'O', 'B-LOC'), ('the', 'O', 'B-LOC'), ('weaker', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'I-MISC'), ('I', 'O', 'O'), ('do', 'O', 'I-PER'), (\"n't\", 'O', 'O'), ('normally', 'O', 'B-LOC'), ('do', 'O', 'I-PER'), ('this', 'O', 'I-MISC'), ('but', 'O', 'I-LOC'), ('can', 'O', 'I-LOC'), ('you', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('People', 'O', 'B-PER'), ('close', 'O', 'B-LOC'), ('to', 'O', 'O'), ('her', 'O', 'B-MISC'), ('said', 'O', 'B-PER'), ('she', 'O', 'B-PER'), ('then', 'O', 'B-ORG'), ('eased', 'O', 'B-LOC'), ('up', 'O', 'I-ORG'), ('on', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Hurte', 'B-PER', 'I-MISC'), ('Sierd', 'I-PER', 'B-LOC'), ('Zylstra', 'I-PER', 'I-MISC'), ('and', 'O', 'I-MISC'), ('his', 'O', 'I-MISC'), ('wife,', 'O', 'I-MISC'), ('Jetsi', 'B-PER', 'I-MISC'), ('Hendrika', 'I-PER', 'B-LOC'), ('Coers,', 'I-PER', 'B-LOC'), ('both', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('-', 'O', 'I-MISC'), ('President', 'O', 'I-MISC'), ('Dos', 'B-PER', 'I-MISC'), ('Santos', 'I-PER', 'I-MISC'), ('proposes', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), ('establishment', 'O', 'I-MISC'), ('by', 'O', 'I-MISC'), ('UN', 'B-ORG', 'I-MISC'), ('Security', 'I-ORG', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('squad', 'O', 'I-ORG'), (':', 'O', 'O'), ('alan', 'B-PER', 'B-LOC'), ('kelly,', 'I-PER', 'O'), ('shay', 'B-PER', 'I-PER'), ('given,', 'I-PER', 'I-ORG'), ('denis', 'B-PER', 'O'), ('irwin,', 'I-PER', 'O'), ('phil', 'B-PER', 'O'), ('babb,', 'I-PER', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-ORG'), ('i', 'O', 'B-ORG'), ('still', 'O', 'B-LOC'), ('feel', 'O', 'O'), (\"it's\", 'O', 'O'), ('embarrassing', 'O', 'O'), ('what', 'O', 'I-MISC'), ('happened', 'O', 'O'), ('and', 'O', 'B-PER'), ('i', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('christian', 'B-PER', 'B-ORG'), ('cullen,', 'I-PER', 'O'), ('14', 'O', 'I-MISC'), ('-', 'O', 'I-ORG'), ('jeff', 'B-PER', 'B-ORG'), ('wilson,', 'I-PER', 'B-ORG'), ('13', 'O', 'I-ORG'), ('-', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('interior', 'O', 'B-LOC'), ('minister', 'O', 'B-MISC'), ('jean', 'B-PER', 'B-LOC'), ('-', '[xIGNx]', 'B-MISC'), ('louis', '[xIGNx]', 'B-LOC'), ('debre,', 'I-PER', 'B-ORG'), ('under', 'O', 'B-LOC'), ('fire', 'O', 'I-PER'), ('for', 'O', 'O'), ('staging', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Shares', 'O', 'B-LOC'), ('in', 'O', 'B-LOC'), ('Slough', 'B-ORG', 'O'), (',', 'O', 'O'), ('which', 'O', 'B-LOC'), ('earlier', 'O', 'B-LOC'), ('announced', 'O', 'O'), ('a', 'O', 'B-LOC'), ('14', 'O', 'B-LOC'), ('percent', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('presence', 'O', 'B-LOC'), ('of', 'O', 'O'), ('Takemura', 'B-PER', 'B-LOC'), (',', 'O', 'O'), ('whose', 'O', 'B-LOC'), ('role', 'O', 'B-LOC'), ('as', 'O', 'O'), ('finance', 'O', 'B-LOC'), ('minister', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('the', 'O', 'B-ORG'), ('agreement', 'O', 'I-PER'), ('resolved', 'O', 'I-LOC'), ('a', 'O', 'B-ORG'), ('dispute', 'O', 'I-PER'), ('that', 'O', 'I-LOC'), ('arose', 'O', 'O'), ('in', 'O', 'I-PER'), ('june', 'O', 'B-PER'), ('when', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-ORG'), ('cofinec', 'B-ORG', 'I-LOC'), ('is', 'O', 'I-LOC'), ('a', 'O', 'I-MISC'), ('very', 'O', 'B-ORG'), ('good', 'O', 'I-LOC'), ('story', 'O', 'B-ORG'), ('in', 'O', 'B-PER'), ('the', 'O', 'B-ORG'), ('long', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-MISC'), ('TALK', 'O', 'I-MISC'), ('-', 'O', 'B-LOC'), ('USDA', 'B-ORG', 'I-MISC'), ('net', 'O', 'B-LOC'), ('change', 'O', 'B-LOC'), ('in', 'O', 'I-MISC'), ('weekly', 'O', 'B-LOC'), ('export', 'O', 'B-LOC'), ('commitments', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Sanchez', 'B-PER', 'B-LOC'), ('Vicario', 'I-PER', 'B-LOC'), (',', 'O', 'B-LOC'), ('runner-up', 'O', 'I-MISC'), ('to', 'O', 'I-MISC'), ('Graf', 'B-PER', 'I-MISC'), ('at', 'O', 'I-MISC'), ('the', 'O', 'I-MISC'), ('French', 'B-MISC', 'B-LOC'), ('Open', 'I-MISC', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('in', 'O', 'I-ORG'), ('his', 'O', 'I-ORG'), ('opinion', 'O', 'I-ORG'), ('the', 'O', 'I-ORG'), ('quartering', 'O', 'O'), ('of', 'O', 'I-MISC'), ('unita', 'B-ORG', 'O'), ('forces', 'O', 'B-PER'), ('must', 'O', 'I-ORG'), ('be', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'I-ORG'), ('we', 'O', 'I-ORG'), ('are', 'O', 'I-ORG'), ('in', 'O', 'I-ORG'), ('the', 'O', 'I-ORG'), ('late', 'O', 'O'), ('stages', 'O', 'I-ORG'), ('of', 'O', 'I-ORG'), ('the', 'O', 'I-ORG'), ('weaker', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'I-PER'), ('with', 'O', 'I-LOC'), ('the', 'O', 'B-LOC'), ('end', 'O', 'I-LOC'), ('of', 'O', 'I-LOC'), ('last', 'O', 'I-LOC'), ('year', 'O', 'B-ORG'), (',', 'O', 'B-ORG'), ('when', 'O', 'B-ORG'), ('T&amp;N', 'B-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('For', 'O', 'I-PER'), ('the', 'O', 'B-ORG'), ('foreign', 'O', 'I-LOC'), ('powers', 'O', 'B-ORG'), ('which', 'O', 'B-ORG'), ('back', 'O', 'I-LOC'), ('last', 'O', 'B-LOC'), ('year', 'O', 'B-ORG'), (\"'s\", 'O', 'B-ORG'), ('Dayton', 'B-LOC', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-PER'), ('-', 'O', 'I-PER'), ('christian', 'B-PER', 'I-PER'), ('cullen,', 'I-PER', 'B-ORG'), ('14', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('jeff', 'B-PER', 'I-PER'), ('wilson,', 'I-PER', 'I-LOC'), ('13', 'O', 'I-PER'), ('-', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('in', 'O', 'I-PER'), ('this', 'O', 'I-PER'), ('model,', 'O', 'I-PER'), ('individual', 'O', 'B-LOC'), ('computer', 'O', 'I-ORG'), ('users', 'O', 'I-PER'), ('can', 'O', 'B-LOC'), ('always', 'O', 'B-LOC'), ('gain', 'O', 'B-LOC'), ('access', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'B-LOC'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'O'), ('Cullen', 'I-PER', 'O'), (',', 'O', 'O'), ('14', 'O', 'I-ORG'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'O'), ('Wilson', 'I-PER', 'O'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('One', 'O', 'O'), ('soldier', 'O', 'O'), ('was', 'O', 'I-PER'), ('discharged', 'O', 'O'), ('because', 'O', 'O'), ('of', 'O', 'O'), ('\"', 'O', 'O'), ('mental', 'O', 'I-PER'), ('incapacity', 'O', 'I-LOC'), ('and', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('brussels', 'B-LOC', 'O'), ('received', 'O', 'B-PER'), ('5.', 'O', 'O'), ('6', '[xIGNx]', 'O'), ('cm', 'O', 'O'), ('(', 'O', 'O'), ('2.', 'O', 'O'), ('24', '[xIGNx]', 'O'), ('inches', 'O', 'O'), (')', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'O'), ('salang', 'B-LOC', 'O'), ('highway,', 'O', 'O'), (\"afghanistan's\", 'B-LOC', 'O'), ('main', 'O', 'O'), ('route', 'O', 'O'), ('to', 'O', 'O'), ('central', 'B-LOC', 'B-PER'), ('asia,', 'I-LOC', 'O'), ('has', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('On', 'O', 'O'), ('the', 'O', 'O'), ('women', 'O', 'O'), (\"'s\", 'O', 'O'), ('side', 'O', 'O'), (',', 'O', 'O'), ('second', 'O', 'O'), ('seed', 'O', 'O'), ('Monica', 'B-PER', 'O'), ('Seles', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O', 'O'), ('U.S.', 'B-LOC', 'O'), ('embassy', 'O', 'O'), ('spokesman', 'O', 'O'), ('in', 'O', 'O'), ('Riyadh', 'B-LOC', 'O'), ('said', 'O', 'O'), ('Specter', 'B-PER', 'O'), (',', 'O', 'O'), ('who', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('squad', 'O', 'I-MISC'), (':', 'O', 'O'), ('alan', 'B-PER', 'O'), ('kelly,', 'I-PER', 'O'), ('shay', 'B-PER', 'O'), ('given,', 'I-PER', 'O'), ('denis', 'B-PER', 'O'), ('irwin,', 'I-PER', 'O'), ('phil', 'B-PER', 'O'), ('babb,', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('punr', 'B-ORG', 'I-LOC'), ('leader', 'O', 'O'), ('gheorghe', 'B-PER', 'B-LOC'), ('funar', 'I-PER', 'B-LOC'), ('said', 'O', 'O'), ('in', 'O', 'O'), ('a', 'O', 'O'), ('statement', 'O', 'O'), ('iliescu,', 'B-PER', 'B-LOC'), ('in', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Hurte', 'B-PER', 'O'), ('Sierd', 'I-PER', 'O'), ('Zylstra', 'I-PER', 'O'), ('and', 'O', 'O'), ('his', 'O', 'O'), ('wife', 'O', 'O'), (',', 'O', 'O'), ('Jetsi', 'B-PER', 'O'), ('Hendrika', 'I-PER', 'O'), ('Coers', 'I-PER', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('credibility', 'O', 'O'), ('of', 'O', 'O'), ('the', 'O', 'O'), ('Buenos', 'B-LOC', 'I-LOC'), ('Aires', 'I-LOC', 'I-LOC'), ('provincial', 'O', 'O'), ('police', 'O', 'O'), (',', 'O', 'O'), ('the', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-ORG'), ('I', 'O', 'B-ORG'), ('do', 'O', 'B-ORG'), (\"n't\", 'O', 'B-ORG'), ('normally', 'O', 'B-ORG'), ('do', 'O', 'B-ORG'), ('this', 'O', 'B-ORG'), ('but', 'O', 'B-ORG'), ('can', 'O', 'B-ORG'), ('you', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Blair', 'B-PER', 'I-PER'), ('Larsen', 'I-PER', 'I-PER'), ('or', 'O', 'B-ORG'), ('the', 'O', 'B-ORG'), ('uncapped', 'O', 'B-ORG'), ('Glenn', 'B-PER', 'B-ORG'), ('Taylor', 'I-PER', 'I-PER'), ('are', 'O', 'B-ORG'), ('on', 'O', 'B-ORG'), ('standby', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if \"deberta\" in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(\n",
    "            hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz\n",
    "        )\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[BaseModelCallback], splitter=blurr_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(\n",
    "            1,\n",
    "            lr_max=3e-5,\n",
    "            moms=(0.8, 0.7, 0.8),\n",
    "            cbs=[ShortEpochCallback(pct=0.1, short_valid=True), TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])],\n",
    "        )\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_callbacks.ipynb.\n",
      "Converted 00_utils.ipynb.\n",
      "Converted 01_text-callbacks.ipynb.\n",
      "Converted 01_text-utils.ipynb.\n",
      "Converted 11_text-data-core.ipynb.\n",
      "Converted 11_text-modeling-core.ipynb.\n",
      "Converted 12_text-data-language-modeling.ipynb.\n",
      "Converted 12_text-modeling-language-modeling.ipynb.\n",
      "Converted 13_text-data-token-classification.ipynb.\n",
      "Converted 13_text-modeling-token-classification.ipynb.\n",
      "Converted 14_text-data-question-answering.ipynb.\n",
      "Converted 14_text-modeling-question-answering.ipynb.\n",
      "Converted 20_text-data-seq2seq-core.ipynb.\n",
      "Converted 20_text-modeling-seq2seq-core.ipynb.\n",
      "Converted 21_text-data-seq2seq-summarization.ipynb.\n",
      "Converted 21_text-modeling-seq2seq-summarization.ipynb.\n",
      "Converted 22_text-data-seq2seq-translation.ipynb.\n",
      "Converted 22_text-modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_text-examples-high-level-api.ipynb.\n",
      "Converted 99b_text-examples-glue.ipynb.\n",
      "Converted 99c_text-examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_text-examples-multilabel.ipynb.\n",
      "Converted 99e_text-examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "\n",
    "nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
