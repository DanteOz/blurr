{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp text.modeling.token_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text.modeling.token_classification\n",
    "\n",
    "> This module contains custom models, loss functions, custom splitters, etc... for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...). The objective of token classification is to predict the correct label for each token provided in the input. In the computer vision world, this is akin to what we do in segmentation tasks whereby we attempt to predict the class/label for each pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, ast, inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.optimizer import Adam, params\n",
    "from fastai.metrics import perplexity\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from seqeval import metrics as seq_metrics\n",
    "from transformers import AutoModelForTokenClassification, PreTrainedTokenizerBase, PreTrainedModel, logging\n",
    "\n",
    "from blurr.text.data.core import TextBlock, TextDataLoader, get_blurr_tfm, first_blurr_tfm\n",
    "from blurr.text.data.token_classification import (\n",
    "    get_token_labels_from_input_ids,\n",
    "    get_word_labels_from_token_labels,\n",
    "    TokenClassTextInput,\n",
    "    TokenTensorCategory,\n",
    "    TokenCategorize,\n",
    "    TokenCategoryBlock,\n",
    "    TokenClassBatchTokenizeTransform,\n",
    ")\n",
    "from blurr.text.modeling.core import Blearner\n",
    "from blurr.text.utils import NLP\n",
    "from blurr.utils import PreCalculatedCrossEntropyLoss\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your BLURR code for token classification\n",
    "\n",
    "**Note**: Make sure you set the `config.num_labels` attribute to the number of labels your model is predicting. The model will update its last layer accordingly as la transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c9295a0ff14a5683a6bf53beeeb7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                                    [3, 0, 7, 0, 0, 0, 7, 0, 0]   \n",
       "1                                                                                         [1, 2]   \n",
       "2                                                                                         [5, 0]   \n",
       "3     [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]  \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]  \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]  \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]  \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "conll2003_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "config.num_labels = len(labels)\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = NLP.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(hf_config.num_labels, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Human', 'O'), ('rights', 'O'), ('lawyer', 'O'), ('Brian', 'B-PER'), ('Currin', 'I-PER'), ('said', 'O'), ('the', 'O'), ('hearings', 'O'), ('last', 'O'), ('week', 'O'), ('were', 'O'), ('not', 'O'), ('intended', 'O'), ('as', 'O'), ('a', 'O'), ('form', 'O'), ('of', 'O'), ('confessional', 'O'), ('and', 'O'), ('that', 'O'), ('those', 'O'), ('who', 'O'), ('personally', 'O'), ('committed', 'O'), ('crimes', 'O'), ('during', 'O'), ('apartheid', 'O'), ('would', 'O'), ('testify', 'O'), ('only', 'O'), ('before', 'O'), ('a', 'O'), ('separate', 'O'), ('arm', 'O'), ('of', 'O'), ('the', 'O'), ('commission', 'O'), (',', 'O'), ('the', 'O'), ('amnesty', 'O'), ('committee', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "In this section, we'll add helpful metrics for token classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calculate_token_class_metrics(pred_toks, targ_toks, metric_key):\n",
    "    if metric_key == \"accuracy\":\n",
    "        return seq_metrics.accuracy_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"precision\":\n",
    "        return seq_metrics.precision_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"recall\":\n",
    "        return seq_metrics.recall_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"f1\":\n",
    "        return seq_metrics.f1_score(targ_toks, pred_toks)\n",
    "\n",
    "    if metric_key == \"classification_report\":\n",
    "        return seq_metrics.classification_report(targ_toks, pred_toks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassMetricsCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenClassMetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    A fastai friendly callback that includes accuracy, precision, recall, and f1 metrics using the\n",
    "    `seqeval` library.  Additionally, this metric knows how to *not* include your 'ignore_token' in it's\n",
    "    calculations.\n",
    "\n",
    "    See [here](https://github.com/chakki-works/seqeval) for more information on `seqeval`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tok_metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"], **kwargs):\n",
    "        self.run_before = Recorder\n",
    "\n",
    "        store_attr(self=self, names=\"tok_metrics, kwargs\")\n",
    "        self.custom_metrics_dict = {k: None for k in tok_metrics}\n",
    "\n",
    "        self.do_setup = True\n",
    "\n",
    "    def setup(self):\n",
    "        # one time setup code here.\n",
    "        if not self.do_setup:\n",
    "            return\n",
    "\n",
    "        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform\n",
    "        tfm = first_blurr_tfm(self.learn.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "        hf_tok_categorize_tfm = get_blurr_tfm(self.learn.dls.tfms[1], tfm_class=TokenCategorize)\n",
    "\n",
    "        self.hf_tokenizer = tfm.hf_tokenizer\n",
    "        self.ignore_label_token_id = hf_tok_categorize_tfm.ignore_token_id\n",
    "        self.tok_special_symbols = list(self.hf_tokenizer.special_tokens_map.values())\n",
    "        self.tok_kwargs = tfm.kwargs\n",
    "\n",
    "        # add custom text generation specific metrics\n",
    "        custom_metric_keys = self.custom_metrics_dict.keys()\n",
    "        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys])\n",
    "        self.learn.metrics = self.learn.metrics + custom_metrics\n",
    "        self.learn.token_classification_report = None\n",
    "\n",
    "        self.do_setup = False\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.setup()\n",
    "\n",
    "    # --- batch begin/after phases ---\n",
    "    def before_batch(self):\n",
    "        pass\n",
    "\n",
    "    def after_batch(self):\n",
    "        if self.training or self.learn.y is None:\n",
    "            return\n",
    "\n",
    "        # do this only for validation set\n",
    "        preds = self.pred.argmax(dim=-1)\n",
    "        targs = self.yb[0]  # yb is TensorText tuple, item 0 is the data\n",
    "\n",
    "        preds_list, targets_list = [], []\n",
    "        for i in range(targs.shape[0]):\n",
    "            item_targs, item_preds = [], []\n",
    "\n",
    "            for j in range(targs.shape[1]):\n",
    "                if targs[i, j] != self.ignore_label_token_id:\n",
    "                    item_preds.append(self.dls.vocab[preds[i][j].item()])\n",
    "                    item_targs.append(self.dls.vocab[targs[i][j].item()])\n",
    "\n",
    "            preds_list.append(item_preds)\n",
    "            targets_list.append(item_targs)\n",
    "\n",
    "        self.results += [(res[0], res[1]) for res in zip(preds_list, targets_list)]\n",
    "\n",
    "    # --- validation begin/after phases ---\n",
    "    def before_validate(self):\n",
    "        self.results = []\n",
    "\n",
    "    def after_validate(self):\n",
    "        if len(self.results) < 1:\n",
    "            return\n",
    "\n",
    "        preds, targs = map(list, zip(*self.results))\n",
    "        for k in self.custom_metrics_dict.keys():\n",
    "            self.custom_metrics_dict[k] = calculate_token_class_metrics(targs, preds, metric_key=k)\n",
    "\n",
    "        try:\n",
    "            self.learn.token_classification_report = calculate_token_class_metrics(targs, preds, \"classification_report\")\n",
    "        except ZeroDivisionError as err:\n",
    "            print(f\"Couldn't calcualte classification report: {err}\")\n",
    "\n",
    "    # --- for ValueMetric metrics ---\n",
    "    def metric_value(self, metric_key):\n",
    "        return self.custom_metrics_dict[metric_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = [TokenClassMetricsCallback()]\n",
    "\n",
    "learn = Learner(dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), cbs=learn_cbs, splitter=blurr_splitter)\n",
    "\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " transformers.modeling_outputs.TokenClassifierOutput,\n",
       " odict_keys(['loss', 'logits']))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "preds = learn.model(b[0])\n",
    "len(preds), type(preds), preds.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, torch.Size([4, 156]), 4, torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 156, 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b[0][\"labels\"].shape\n",
    "preds.logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 9]) torch.Size([624])\n"
     ]
    }
   ],
   "source": [
    "print(preds.logits.view(-1, preds.logits.shape[-1]).shape, b[1].view(-1).shape)\n",
    "test_eq(preds.logits.view(-1, preds.logits.shape[-1]).shape[0], b[1].view(-1).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0005248074419796466, steep=3.630780702224001e-05, valley=0.00010964782268274575, slide=0.00010964782268274575)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.057961</td>\n",
       "      <td>0.063177</td>\n",
       "      <td>0.988249</td>\n",
       "      <td>0.935681</td>\n",
       "      <td>0.927079</td>\n",
       "      <td>0.931360</td>\n",
       "      <td>03:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.95      1450\n",
      "        MISC       0.88      0.86      0.87       687\n",
      "         ORG       0.91      0.89      0.90      1320\n",
      "         PER       0.97      0.97      0.97      1329\n",
      "\n",
      "   micro avg       0.94      0.93      0.93      4786\n",
      "   macro avg       0.93      0.92      0.92      4786\n",
      "weighted avg       0.94      0.93      0.93      4786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing results\n",
    "\n",
    "Below we'll add in additional functionality to more intuitively show the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # This typedispatched `show_results` will be called for `TokenTensorCategory` typed targets\n",
    "    y: TokenTensorCategory,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    tfm = first_blurr_tfm(learner.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "    vocab = learner.dls.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample, pred in zip(x, y, samples, outs):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # align \"words\" with \"predicted\" labels\n",
    "        word_pred_labels = [pred_lbl for lbl_id, pred_lbl in zip(trg, ast.literal_eval(pred[0])) if lbl_id != ignore_token_id]\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ (word_targ[0], word_targ[1], pred_targ) for idx, (word_targ, pred_targ) in enumerate(zip(word_labels, word_pred_labels)) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"token / target label / predicted label\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('Christian', 'B-PER', 'B-PER'), ('Cullen', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('Jeff', 'B-PER', 'B-PER'), ('Wilson', 'I-PER', 'I-PER'), (',', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'O'), ('I', 'O', 'O'), ('do', 'O', 'O'), (\"n't\", 'O', 'O'), ('normally', 'O', 'O'), ('do', 'O', 'O'), ('this', 'O', 'O'), ('but', 'O', 'O'), ('can', 'O', 'O'), ('you', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "The default `Learner.predict` method returns a prediction per subtoken, including the special tokens for each architecture's tokenizer. Starting with version 2.0 of BLURR, we bring token prediction in-line with Hugging Face's token classification pipeline, both in terms of supporting the same aggregation strategies via Blurr's `TokenAggregationStrategies` class, and also the output via BLURR's `@patch`ed `Learner` method, `blurr_predict_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenAggregationStrategies:\n",
    "    \"\"\"\n",
    "    Provides the equivalanet of Hugging Face's token classification pipeline's `aggregation_strategy` support across various\n",
    "    token classication tasks (e.g, NER, POS, chunking, etc...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_tokenizer: PreTrainedTokenizerBase, labels: List[str], non_entity_label: str = \"O\") -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.valid_strategies = [\"simple\", \"first\", \"max\", \"average\"]\n",
    "\n",
    "        self.uses_BI_label_strategy = False\n",
    "        for lbl in self.labels:\n",
    "            if lbl.startswith(\"I-\"):\n",
    "                self.uses_BI_label_strategy = True\n",
    "                break\n",
    "\n",
    "    def by_token(self, tokens, input_ids, offsets, preds, probs):\n",
    "        results = []\n",
    "        for tok_idx, (token, input_id, offset, pred, prob) in enumerate(zip(tokens, input_ids, offsets, preds, probs)):\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            label = self.labels[pred]\n",
    "            if label == self.non_entity_label or input_id.item() in self.hf_tokenizer.all_special_ids:\n",
    "                continue\n",
    "\n",
    "            start, end = offset\n",
    "            results.append({\"entity\": label, \"score\": prob[pred], \"word\": token, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def by_word_strategy(self, strategy_name, text, input_ids, offsets, preds, probs, word_ids=None):\n",
    "        # validate `strategy_name`\n",
    "        if strategy_name not in self.valid_strategies:\n",
    "            raise ValueError(\"The 'strategy_name' is not supported by this class\")\n",
    "\n",
    "        # validate the existence of `word_ids` if the aggregation strategy = \"average\"\n",
    "        if strategy_name == \"average\" and word_ids is None:\n",
    "            raise ValueError(\"The 'average' strategy requires word_ids list\")\n",
    "\n",
    "        results = []\n",
    "        idx = 0\n",
    "        while idx < len(preds):\n",
    "            pred = preds[idx]\n",
    "            label = self.labels[pred]\n",
    "\n",
    "            # pass over any non-entity labels and \"special\" tokens\n",
    "            if label == self.non_entity_label or input_ids[idx].item() in self.hf_tokenizer.all_special_ids:\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            # Remove the B- or I-\n",
    "            label = label[2:] if self.uses_BI_label_strategy else label\n",
    "            start, end = offsets[idx]\n",
    "\n",
    "            all_scores = []\n",
    "            all_scores.append(probs[idx][pred])\n",
    "\n",
    "            word_scores = {}\n",
    "            if strategy_name == \"average\":\n",
    "                word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "            lbl_to_search = f\"I-{label}\" if self.uses_BI_label_strategy else label\n",
    "            while idx + 1 < len(preds) and self.labels[preds[idx + 1]] == lbl_to_search:\n",
    "                idx += 1\n",
    "                _, end = offsets[idx]\n",
    "\n",
    "                pred = preds[idx]\n",
    "\n",
    "                if strategy_name == \"average\":\n",
    "                    if word_ids[idx] in word_scores:\n",
    "                        word_scores[word_ids[idx]].append(probs[idx][pred])\n",
    "                    else:\n",
    "                        word_scores[word_ids[idx]] = [probs[idx][pred]]\n",
    "\n",
    "                if strategy_name != \"first\":\n",
    "                    all_scores.append(probs[idx][pred])\n",
    "\n",
    "            # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "            if strategy_name == \"average\":\n",
    "                score = np.mean([np.mean(v).item() for k, v in word_scores.items()])\n",
    "            else:\n",
    "                score = np.max(all_scores).item() if strategy_name == \"max\" else np.mean(all_scores).item()\n",
    "\n",
    "            word = text[start:end]\n",
    "            results.append({\"entity_group\": label, \"score\": score, \"word\": word, \"start\": start.item(), \"end\": end.item()})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def blurr_predict_tokens(\n",
    "    self: Learner,\n",
    "    # The str (or list of strings) you want to get token classification predictions for\n",
    "    items: Union[str, List[str]],\n",
    "    # How entities are grouped and scored\n",
    "    aggregation_strategy: str = \"simple\",\n",
    "    # The label used to idendity non-entity related words/tokens\n",
    "    non_entity_label: str = \"O\",\n",
    "    # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
    "    # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
    "    # equavlient of fast tokenizer's `word_ids``\n",
    "    slow_word_ids_func: Optional[Callable] = None,\n",
    "):\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    tfm = first_blurr_tfm(self.dls, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    strategies = TokenAggregationStrategies(hf_tokenizer, self.dls.vocab, non_entity_label)\n",
    "\n",
    "    inputs = hf_tokenizer(items, return_offsets_mapping=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_offsets = inputs[\"offset_mapping\"]\n",
    "    inputs_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # run inputs through model\n",
    "    model_inputs = {k: v.to(self.model.hf_model.device) for k, v in inputs.items()}\n",
    "    outputs = self.model(model_inputs)\n",
    "\n",
    "    # fetch probabilities and predictions\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).tolist()\n",
    "    predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    # build our results\n",
    "    results = []\n",
    "    for input_idx, (text, input_ids, offsets, preds, probs) in enumerate(\n",
    "        zip(items, inputs_input_ids, inputs_offsets, predictions, probabilities)\n",
    "    ):\n",
    "        # build our results for the current input\n",
    "        tokens = inputs.tokens(input_idx)\n",
    "        word_ids = inputs.word_ids(input_idx) if hf_tokenizer.is_fast else slow_word_ids_func(hf_tokenizer, input_idx, inputs)\n",
    "\n",
    "        if aggregation_strategy == \"token\":\n",
    "            results.append(strategies.by_token(tokens, input_ids, offsets, preds, probs))\n",
    "        else:\n",
    "            results.append(strategies.by_word_strategy(aggregation_strategy, text, input_ids, offsets, preds, probs, word_ids))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.blurr_predict_tokens\" class=\"doc_header\"><code>Learner.blurr_predict_tokens</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.blurr_predict_tokens</code>(**`items`**:`Union`\\[`str`, List[str]`\\], **`aggregation_strategy`**:`str`=*`'simple'`*, **`non_entity_label`**:`str`=*`'O'`*, **`slow_word_ids_func`**:`Optional`\\[`Callable`\\]=*`None`*)\n",
       "\n",
       "\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`items`**|`List[str]]`||The str (or list of strings) you want to get token classification predictions for|\n",
       "|**`aggregation_strategy`**|`str`|`simple`|How entities are grouped and scored|\n",
       "|**`non_entity_label`**|`str`|`O`|The label used to idendity non-entity related words/tokens|\n",
       "|**`slow_word_ids_func`**|`Callable]`|``|If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a<br />tokenizzer, example index, and a batch encoding as arguments and in turn returnes the<br />equavlient of fast tokenizer's `word_ids``|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.blurr_predict_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[{'entity_group': 'ORG', 'score': 0.9911214113235474, 'word': 'Bayern Munich', 'start': 0, 'end': 13}, {'entity_group': 'LOC', 'score': 0.9978020787239075, 'word': 'Germany', 'start': 34, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(\n",
    "    items=[\"My name is Wayde and I live in San Diego and using Hugging Face\", \"Bayern Munich is a soccer team in Germany\"],\n",
    "    aggregation_strategy=\"max\",\n",
    ")\n",
    "\n",
    "print(len(res))\n",
    "print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could go to Germany and watch Bayern Munich play in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'PER', 'score': 0.9220105707645416, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.8570420543352762, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9968061447143555, 'word': 'California', 'start': 56, 'end': 66}]]\n"
     ]
    }
   ],
   "source": [
    "res = learn.blurr_predict_tokens(txt)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9220105707645416, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.8570420543352762, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9968061447143555, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.5545341968536377, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'LOC', 'score': 0.9980267882347107, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9870105087757111, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9737485647201538, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fname = \"tok_class_learn_export\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9220104068517685, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'ORG', 'score': 0.8570426106452942, 'word': 'ohmeow', 'start': 34, 'end': 40}, {'entity_group': 'LOC', 'score': 0.9968060255050659, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.5545328259468079, 'word': 'cov', 'start': 7, 'end': 10}, {'entity_group': 'LOC', 'score': 0.9980266690254211, 'word': 'Germany', 'start': 39, 'end': 46}, {'entity_group': 'ORG', 'score': 0.9870105087757111, 'word': 'Bayern Munich', 'start': 57, 'end': 70}, {'entity_group': 'MISC', 'score': 0.9737485647201538, 'word': 'Bundesliga', 'start': 83, 'end': 93}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.export(fname=f\"{export_fname}.pkl\")\n",
    "inf_learn = load_learner(fname=f\"{export_fname}.pkl\")\n",
    "\n",
    "results = inf_learn.blurr_predict_tokens([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    del inf_learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BLearnerForTokenClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Blearner.__init__)\n",
    "class BlearnerForTokenClassification(Blearner):\n",
    "    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):\n",
    "        super().__init__(dls, hf_model, **kwargs)\n",
    "\n",
    "    def predict(self, text):\n",
    "        return self.blurr_predict_tokens(text)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_cls(self):\n",
    "        return AutoModelForTokenClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics_cb(self):\n",
    "        return TokenClassMetricsCallback()\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(\n",
    "        cls,\n",
    "        # Your raw dataset. Supports DataFrames, Hugging Face Datasets, as well as file paths\n",
    "        # to .csv, .xlsx, .xls, and .jsonl files\n",
    "        data: Union[pd.DataFrame, Path, str, List[Dict]],\n",
    "        # The name or path of the pretrained model you want to fine-tune\n",
    "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
    "        # The attribute in your dataset that contains a list of your tokens\n",
    "        tokens_attr: List[str] = \"tokens\",\n",
    "        # The attribute in your dataset that contains the entity labels for each token in your raw text\n",
    "        token_labels_attr: List[str] = \"token_labels\",\n",
    "        # The unique entity labels (or vocab) available in your dataset\n",
    "        labels: Optional[List[str]] = None,\n",
    "        # A function that will split your Dataset into a training and validation set\n",
    "        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters\n",
    "        dblock_splitter: Optional[Callable] = None,\n",
    "        # Any kwargs to pass to your `DataLoaders`\n",
    "        dl_kwargs: dict = {},\n",
    "        # Any kwargs to pass to your task specific `Blearner`\n",
    "        learner_kwargs: dict = {},\n",
    "    ):\n",
    "        # if we get a path/str then we're loading something like a .csv file\n",
    "        if isinstance(data, Path) or isinstance(data, str):\n",
    "            content_type = mimetypes.guess_type(data)[0]\n",
    "            if content_type == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\":\n",
    "                data = pd.read_excel(data)\n",
    "            elif content_type == \"text/csv\":\n",
    "                data = pd.read_csv(data)\n",
    "            elif content_type == \"application/json\":\n",
    "                data = pd.read_json(data, orient=\"records\")\n",
    "            else:\n",
    "                raise ValueError(\"'data' must be a .xlsx, .xls, .csv, or .jsonl file\")\n",
    "\n",
    "            data = pd.read_csv(data)\n",
    "\n",
    "        # we need to tell transformer how many labels/classes to expect\n",
    "        if labels is None:\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                labels = sorted(list(set([lbls for sublist in data[token_labels_attr].tolist() for lbls in sublist])))\n",
    "            else:\n",
    "                labels = sorted(list(set([item[token_labels_attr] for item in data])))\n",
    "\n",
    "        # infer our datablock splitter if None\n",
    "        if dblock_splitter is None:\n",
    "            dblock_splitter = ColSplitter() if hasattr(data, \"is_valid\") else RandomSplitter()\n",
    "\n",
    "        # get our hf objects\n",
    "        n_labels = len(labels)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = NLP.get_hf_objects(\n",
    "            pretrained_model_name_or_path, model_cls=cls.get_model_cls(), config_kwargs={\"num_labels\": n_labels}\n",
    "        )\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "        blocks = (\n",
    "            TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "            TokenCategoryBlock(vocab=labels),\n",
    "        )\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ItemGetter(tokens_attr), get_y=ItemGetter(token_labels_attr), splitter=dblock_splitter)\n",
    "        dls = dblock.dataloaders(data, **dl_kwargs.copy())\n",
    "\n",
    "        # return BLearner instance\n",
    "        return cls(dls, hf_model, **learner_kwargs.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your `Blearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForTokenClassification.from_data(\n",
    "    conll2003_df,\n",
    "    \"distilroberta-base\",\n",
    "    tokens_attr=\"tokens\",\n",
    "    token_labels_attr=\"ner_tags\",\n",
    "    labels=labels,\n",
    "    dl_kwargs={\"bs\": 2},\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O'), ('new', 'O'), ('crop', 'O'), (',', 'O'), ('were', 'O'), (':', 'O'), ('wheat', 'O'), ('up', 'O'), ('595,400', 'O'), ('tonnes', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('corn', 'O'), ('up', 'O'), ('1,900', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('319,600', 'O'), ('new', 'O'), (';', 'O'), ('soybeans', 'O'), ('down', 'O'), ('12,300', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('300,800', 'O'), ('new', 'O'), (';', 'O'), ('upland', 'O'), ('cotton', 'O'), ('up', 'O'), ('50,400', 'O'), ('bales', 'O'), ('new', 'O'), (',', 'O'), ('nil', 'O'), ('old', 'O'), (';', 'O'), ('soymeal', 'O'), ('54,800', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('100,600', 'O'), ('new', 'O'), (',', 'O'), ('soyoil', 'O'), ('nil', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('75,000', 'O'), ('new', 'O'), (';', 'O'), ('barley', 'O'), ('up', 'O'), ('1,700', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('sorghum', 'O'), ('6,200', 'O'), ('old', 'O'), (',', 'O'), ('up', 'O'), ('156,700', 'O'), ('new', 'O'), (';', 'O'), ('pima', 'O'), ('cotton', 'O'), ('up', 'O'), ('4,000', 'O'), ('bales', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), (';', 'O'), ('rice', 'O'), ('up', 'O'), ('49,900', 'O'), ('old', 'O'), (',', 'O'), ('nil', 'O'), ('new', 'O'), ('...', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O'), ('quake', 'O'), ('struck', 'O'), ('at', 'O'), ('11.16', 'O'), ('a.m.', 'O'), ('(', 'O'), ('1716', 'O'), ('GMT', 'B-MISC'), (')', 'O'), ('and', 'O'), ('was', 'O'), ('centred', 'O'), ('10', 'O'), ('miles', 'O'), ('(', 'O'), ('16', 'O'), ('km', 'O'), (')', 'O'), ('south', 'O'), ('of', 'O'), ('the', 'O'), ('port', 'O'), ('of', 'O'), ('Quepos', 'B-LOC'), (',', 'O'), ('which', 'O'), ('is', 'O'), ('90', 'O'), ('miles', 'O'), ('(', 'O'), ('140', 'O'), ('km', 'O'), (')', 'O'), ('south', 'O'), ('of', 'O'), ('the', 'O'), ('capital', 'O'), ('San', 'B-LOC'), ('Jose', 'I-LOC'), (',', 'O'), ('the', 'O'), ('Costa', 'B-ORG'), ('Rican', 'I-ORG'), ('Volcanic', 'I-ORG'), ('and', 'I-ORG'), ('Seismologicial', 'I-ORG'), ('Observatory', 'I-ORG'), ('said', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.dls.show_batch(dataloaders=learn.dls, max_n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>0.058621</td>\n",
       "      <td>0.987674</td>\n",
       "      <td>0.935123</td>\n",
       "      <td>0.929817</td>\n",
       "      <td>0.932462</td>\n",
       "      <td>03:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "learn.fit_one_cycle(1, lr_max=3e-5, moms=(0.8, 0.7, 0.8), cbs=[BlearnerForTokenClassification.get_metrics_cb()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Squad', 'O', 'O'), (':', 'O', 'O'), ('Alan', 'B-PER', 'B-PER'), ('Kelly', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('Shay', 'B-PER', 'B-PER'), ('Given', 'I-PER', 'I-PER'), (',', 'O', 'O'), ('Denis', 'B-PER', 'B-PER'), ('Irwin', 'I-PER', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Rangarajan', 'B-PER', 'B-PER'), ('said', 'O', 'O'), ('a', 'O', 'O'), ('current', 'O', 'O'), ('account', 'O', 'O'), ('deficit', 'O', 'O'), ('of', 'O', 'O'), ('two', 'O', 'O'), ('percent', 'O', 'O'), ('brought', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2, trunc_at=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.96      1433\n",
      "        MISC       0.85      0.90      0.87       661\n",
      "         ORG       0.92      0.88      0.90      1331\n",
      "         PER       0.97      0.97      0.97      1334\n",
      "\n",
      "   micro avg       0.94      0.93      0.93      4759\n",
      "   macro avg       0.92      0.93      0.92      4759\n",
      "weighted avg       0.94      0.93      0.93      4759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "print(learn.token_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hi! My name is Wayde Gilliam from ohmeow.com. I live in California.\"\n",
    "txt2 = \"I wish covid was over so I could watch Lewandowski score some more goals for Bayern Munich in the Bundesliga.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.8709542080760002, 'word': 'Wayde Gilliam', 'start': 15, 'end': 28}, {'entity_group': 'LOC', 'score': 0.4269283413887024, 'word': 'oh', 'start': 34, 'end': 36}, {'entity_group': 'LOC', 'score': 0.9965140223503113, 'word': 'California', 'start': 56, 'end': 66}]\n",
      "\n",
      "[{'entity_group': 'PER', 'score': 0.9946655631065369, 'word': 'Lewandowski', 'start': 39, 'end': 50}, {'entity_group': 'ORG', 'score': 0.9803867638111115, 'word': 'Bayern Munich', 'start': 77, 'end': 90}, {'entity_group': 'MISC', 'score': 0.9789237380027771, 'word': 'Bundesliga', 'start': 98, 'end': 108}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = learn.predict([txt, txt2])\n",
    "for res in results:\n",
    "    print(f\"{res}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the token classification training code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained token classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'NystromformerForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'YosoForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in NLP.get_models(task=\"TokenClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids\n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test\n",
    "    \"google/mobilebert-uncased\",\n",
    "    \"google/rembert\",\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590b945373414f0788fa43510bda3c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'I-PER'), ('talk', 'O', 'I-ORG'), ('-', 'O', 'I-MISC'), ('usda', 'B-ORG', 'B-PER'), ('net', 'O', 'O'), ('change', 'O', 'I-LOC'), ('in', 'O', 'O'), ('weekly', 'O', 'I-LOC'), ('export', 'O', 'B-PER'), ('commitments', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('despite', 'O', 'I-MISC'), ('a', 'O', 'I-ORG'), ('mood', 'O', 'I-LOC'), ('of', 'O', 'I-LOC'), ('compromise', 'O', 'O'), ('in', 'O', 'I-MISC'), ('the', 'O', 'B-PER'), ('region', 'O', 'I-LOC'), ('after', 'O', 'I-LOC'), ('some', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('hurte', 'B-PER', 'B-LOC'), ('sierd', 'I-PER', 'I-LOC'), ('zylstra', 'I-PER', 'I-LOC'), ('and', 'O', 'B-LOC'), ('his', 'O', 'I-PER'), ('wife,', 'O', 'I-LOC'), ('jetsi', 'B-PER', 'I-PER'), ('hendrika', 'I-PER', 'B-LOC'), ('coers,', 'I-PER', 'B-LOC'), ('both', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'I-LOC'), ('presence', 'O', 'B-LOC'), ('of', 'O', 'B-PER'), ('takemura,', 'B-PER', 'I-LOC'), ('whose', 'O', 'I-LOC'), ('role', 'O', 'I-LOC'), ('as', 'O', 'I-LOC'), ('finance', 'O', 'I-LOC'), ('minister', 'O', 'I-LOC'), ('in', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Shares', 'O', 'O'), ('in', 'O', 'I-MISC'), ('Slough', 'B-ORG', 'I-ORG'), (',', 'O', 'I-MISC'), ('which', 'O', 'B-LOC'), ('earlier', 'O', 'O'), ('announced', 'O', 'I-PER'), ('a', 'O', 'B-ORG'), ('14', 'O', 'B-ORG'), ('percent', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'I-ORG'), ('plant', 'O', 'B-MISC'), (',', 'O', 'I-MISC'), ('requiring', 'O', 'I-MISC'), ('an', 'O', 'I-MISC'), ('investment', 'O', 'I-MISC'), ('of', 'O', 'O'), ('three', 'O', 'O'), ('billion', 'O', 'I-LOC'), ('baht', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'I-ORG'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'B-MISC'), ('of', 'O', 'O'), ('last', 'O', 'O'), ('year', 'O', 'O'), (',', 'O', 'O'), ('when', 'O', 'I-PER'), ('T&amp;N', 'B-ORG', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Police', 'O', 'B-PER'), ('said', 'O', 'O'), ('the', 'O', 'O'), ('111', 'O', 'B-PER'), ('passengers', 'O', 'I-PER'), ('and', 'O', 'O'), ('six', 'O', 'B-PER'), ('crew', 'O', 'I-ORG'), ('on', 'O', 'I-PER'), ('board', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O', 'I-MISC'), ('talk', 'O', 'I-MISC'), ('-', 'O', 'I-MISC'), ('usda', 'B-ORG', 'I-MISC'), ('net', 'O', 'I-MISC'), ('change', 'O', 'I-MISC'), ('in', 'O', 'I-LOC'), ('weekly', 'O', 'I-MISC'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('ian', 'B-PER', 'I-MISC'), ('rush,', 'I-PER', 'I-MISC'), ('the', 'O', 'I-MISC'), ('welsh', 'B-MISC', 'I-LOC'), ('striker', 'O', 'I-MISC'), ('signed', 'O', 'I-MISC'), ('from', 'O', 'I-MISC'), ('liverpool', 'B-ORG', 'I-MISC'), ('in', 'O', 'I-MISC'), ('the', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-LOC'), ('TALK', 'O', 'I-LOC'), ('-', 'O', 'B-ORG'), ('USDA', 'B-ORG', 'B-PER'), ('net', 'O', 'I-LOC'), ('change', 'O', 'O'), ('in', 'O', 'B-LOC'), ('weekly', 'O', 'B-PER'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('DBRS', 'B-ORG', 'B-PER'), ('said', 'O', 'I-MISC'), ('it', 'O', 'I-LOC'), ('also', 'O', 'I-MISC'), ('confirmed', 'O', 'B-MISC'), ('Power', 'B-ORG', 'B-LOC'), ('Financial', 'I-ORG', 'B-MISC'), ('Corp', 'I-ORG', 'I-MISC'), (\"'s\", 'O', 'B-PER'), ('senior', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'I-MISC'), ('TALK', 'O', 'I-MISC'), ('-', 'O', 'I-MISC'), ('USDA', 'B-ORG', 'I-MISC'), ('net', 'O', 'I-MISC'), ('change', 'O', 'I-MISC'), ('in', 'O', 'I-MISC'), ('weekly', 'O', 'I-MISC'), ('export', 'O', 'I-MISC'), ('commitments', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O', 'I-MISC'), ('few', 'O', 'B-LOC'), ('years', 'O', 'I-MISC'), ('ago,', 'O', 'I-MISC'), ('barter', 'O', 'I-MISC'), ('deals', 'O', 'B-LOC'), ('accounted', 'O', 'I-MISC'), ('for', 'O', 'B-LOC'), ('up', 'O', 'I-MISC'), ('to', 'O', 'I-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('innocent', 'B-PER', 'B-PER'), ('butare,', 'I-PER', 'I-PER'), ('executive', 'O', 'I-PER'), ('secretary', 'O', 'I-ORG'), ('of', 'O', 'I-ORG'), ('the', 'O', 'I-PER'), ('rally', 'B-ORG', 'B-PER'), ('for', 'I-ORG', 'I-ORG'), ('the', 'I-ORG', 'I-LOC'), ('return', 'I-ORG', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('it', 'O', 'B-ORG'), ('said', 'O', 'B-ORG'), ('that', 'O', 'I-ORG'), ('under', 'O', 'B-ORG'), ('the', 'O', 'B-PER'), ('sale', 'O', 'I-ORG'), ('agreement,', 'O', 'B-ORG'), ('full', 'O', 'I-PER'), ('financial', 'O', 'B-MISC'), ('details', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'O'), ('-', 'O', 'O'), ('christian', 'B-PER', 'O'), ('cullen,', 'I-PER', 'O'), ('14', 'O', 'O'), ('-', 'O', 'O'), ('jeff', 'B-PER', 'I-LOC'), ('wilson,', 'I-PER', 'O'), ('13', 'O', 'I-LOC'), ('-', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('now,', 'O', 'O'), ('u.', 'B-LOC', 'O'), ('s.', '[xIGNx]', 'O'), ('district', 'O', 'I-PER'), ('judge', 'O', 'O'), ('mark', 'B-PER', 'O'), ('wolf', 'I-PER', 'O'), ('has', 'O', 'O'), ('ordered', 'O', 'O'), ('the', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Even', 'O', 'B-PER'), ('though', 'O', 'B-PER'), ('more', 'O', 'O'), ('than', 'O', 'O'), ('40', 'O', 'O'), ('million', 'O', 'B-PER'), ('copies', 'O', 'O'), ('of', 'O', 'O'), ('Windows', 'B-MISC', 'B-PER'), ('95', 'I-MISC', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'O'), ('quake', 'O', 'B-PER'), ('struck', 'O', 'B-PER'), ('at', 'O', 'O'), ('11.16', 'O', 'B-PER'), ('a.m.', 'O', 'B-PER'), ('(', 'O', 'B-PER'), ('1716', 'O', 'B-PER'), ('GMT', 'B-MISC', 'O'), (')', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O', 'I-LOC'), ('-', 'O', 'B-MISC'), ('christian', 'B-PER', 'B-MISC'), ('cullen,', 'I-PER', 'B-MISC'), ('14', 'O', 'B-MISC'), ('-', 'O', 'B-MISC'), ('jeff', 'B-PER', 'B-MISC'), ('wilson,', 'I-PER', 'B-MISC'), ('13', 'O', 'B-MISC'), ('-', 'O', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('brunswijk', 'B-PER', 'B-MISC'), ('turned', 'O', 'B-MISC'), ('himself', 'O', 'I-LOC'), ('into', 'O', 'B-MISC'), ('police', 'O', 'B-MISC'), ('after', 'O', 'I-LOC'), ('freddy', 'B-PER', 'B-MISC'), ('pinas,', 'I-PER', 'B-MISC'), ('a', 'O', 'I-LOC'), ('surinamese', 'B-MISC', 'I-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>01:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O', 'B-ORG'), ('TALK', 'O', 'B-ORG'), ('-', 'O', 'B-ORG'), ('USDA', 'B-ORG', 'B-ORG'), ('net', 'O', 'B-ORG'), ('change', 'O', 'B-ORG'), ('in', 'O', 'B-ORG'), ('weekly', 'O', 'B-ORG'), ('export', 'O', 'B-ORG'), ('commitments', 'O', 'I-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Mohammed', 'B-PER', 'B-ORG'), ('Saleh', 'I-PER', 'I-PER'), (',', 'O', 'B-ORG'), ('director', 'O', 'B-ORG'), ('of', 'O', 'B-ORG'), ('the', 'O', 'B-ORG'), ('Egyptian', 'B-MISC', 'B-ORG'), ('Museum', 'O', 'B-ORG'), (',', 'O', 'B-ORG'), ('told', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('even', 'O', 'O'), ('though', 'O', 'O'), ('more', 'O', 'O'), ('than', 'O', 'O'), ('40', 'O', 'B-PER'), ('million', 'O', 'I-PER'), ('copies', 'O', 'O'), ('of', 'O', 'O'), ('windows', 'B-MISC', 'O'), ('95', 'I-MISC', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O', 'O'), ('survey,', 'O', 'O'), ('conducted', 'O', 'I-PER'), ('in', 'O', 'O'), ('late', 'O', 'B-PER'), ('1995', 'O', 'O'), ('and', 'O', 'O'), ('the', 'O', 'B-PER'), ('early', 'O', 'B-PER'), ('part', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Shares', 'O', 'B-LOC'), ('in', 'O', 'B-LOC'), ('Slough', 'B-ORG', 'B-LOC'), (',', 'O', 'B-LOC'), ('which', 'O', 'B-LOC'), ('earlier', 'O', 'B-LOC'), ('announced', 'O', 'B-LOC'), ('a', 'O', 'I-ORG'), ('14', 'O', 'B-ORG'), ('percent', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-LOC'), ('I', 'O', 'B-LOC'), ('do', 'O', 'I-MISC'), (\"n't\", 'O', 'I-MISC'), ('know', 'O', 'I-ORG'), ('what', 'O', 'I-MISC'), ('the', 'O', 'B-LOC'), ('source', 'O', 'B-LOC'), ('of', 'O', 'B-LOC'), ('the', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('brussels', 'B-LOC', 'B-PER'), ('received', 'O', 'B-PER'), ('5.', 'O', 'B-MISC'), ('6', '[xIGNx]', 'I-PER'), ('cm', 'O', 'I-PER'), ('(', 'O', 'I-PER'), ('2.', 'O', 'I-LOC'), ('24', '[xIGNx]', 'I-LOC'), ('inches', 'O', 'I-PER'), (')', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'B-MISC', 'O'), ('vermonter,', 'I-MISC', 'O'), ('which', 'O', 'O'), ('runs', 'O', 'O'), ('between', 'O', 'O'), ('st.', 'B-LOC', 'B-PER'), ('albans,', 'I-LOC', 'O'), ('vermont,', 'B-LOC', 'B-PER'), ('near', 'O', 'B-PER'), ('the', 'O', 'B-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'O'), ('We', 'O', 'B-PER'), ('have', 'O', 'O'), ('no', 'O', 'I-MISC'), ('doubt', 'O', 'I-ORG'), ('that', 'O', 'I-MISC'), ('this', 'O', 'I-MISC'), ('is', 'O', 'I-ORG'), ('one', 'O', 'I-MISC'), ('of', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Slough', 'B-ORG', 'I-LOC'), (\"'s\", 'O', 'B-LOC'), ('chairman', 'O', 'O'), ('Sir', 'O', 'I-PER'), ('Nigel', 'B-PER', 'O'), ('Mobbs', 'I-PER', 'B-PER'), ('added', 'O', 'I-LOC'), ('to', 'O', 'O'), ('the', 'O', 'O'), ('bullish', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('hurte', 'B-PER', 'B-LOC'), ('sierd', 'I-PER', 'B-LOC'), ('zylstra', 'I-PER', 'B-LOC'), ('and', 'O', 'B-LOC'), ('his', 'O', 'O'), ('wife,', 'O', 'O'), ('jetsi', 'B-PER', 'O'), ('hendrika', 'I-PER', 'O'), ('coers,', 'I-PER', 'O'), ('both', 'O', 'B-LOC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('it', 'O', 'O'), ('explained', 'O', 'B-ORG'), ('the', 'O', 'O'), ('\"', 'O', 'B-ORG'), ('addition', 'O', 'B-ORG'), ('of', 'O', 'B-ORG'), ('a', 'O', 'O'), ('new', 'B-LOC', 'B-ORG'), ('york', 'I-LOC', 'B-ORG'), ('license', 'O', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O', 'O'), ('with', 'O', 'O'), ('the', 'O', 'O'), ('end', 'O', 'O'), ('of', 'O', 'O'), ('last', 'O', 'B-ORG'), ('year', 'O', 'B-ORG'), (',', 'O', 'O'), ('when', 'O', 'O'), ('T&amp;N', 'B-ORG', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('For', 'O', 'O'), ('the', 'O', 'O'), ('foreign', 'O', 'O'), ('powers', 'O', 'O'), ('which', 'O', 'O'), ('back', 'O', 'O'), ('last', 'O', 'B-ORG'), ('year', 'O', 'B-ORG'), (\"'s\", 'O', 'O'), ('Dayton', 'B-LOC', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('even', 'O', 'O'), ('though', 'O', 'O'), ('more', 'O', 'B-LOC'), ('than', 'O', 'I-LOC'), ('40', 'O', 'I-PER'), ('million', 'O', 'I-ORG'), ('copies', 'O', 'O'), ('of', 'O', 'B-MISC'), ('windows', 'B-MISC', 'B-PER'), ('95', 'I-MISC', 'B-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('mccurry', 'B-PER', 'O'), ('said', 'O', 'I-PER'), ('that', 'O', 'I-PER'), ('when', 'O', 'O'), ('clinton', 'B-PER', 'B-PER'), ('delivers', 'O', 'B-LOC'), ('his', 'O', 'B-ORG'), ('acceptance', 'O', 'B-MISC'), ('address', 'O', 'B-ORG'), ('on', 'O', 'B-ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('\"', 'O', 'B-MISC'), ('I', 'O', 'B-MISC'), ('do', 'O', 'B-MISC'), (\"n't\", 'O', 'B-MISC'), ('normally', 'O', 'B-MISC'), ('do', 'O', 'B-MISC'), ('this', 'O', 'B-MISC'), ('but', 'O', 'B-MISC'), ('can', 'O', 'B-MISC'), ('you', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O', 'B-MISC'), ('We', 'O', 'B-MISC'), ('are', 'O', 'B-MISC'), ('in', 'O', 'B-MISC'), ('the', 'O', 'B-MISC'), ('late', 'O', 'B-MISC'), ('stages', 'O', 'I-MISC'), ('of', 'O', 'B-MISC'), ('the', 'O', 'B-MISC'), ('weaker', 'O', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token / target label / predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Shares', 'O', 'I-PER'), ('in', 'O', 'I-PER'), ('Slough', 'B-ORG', 'I-PER'), (',', 'O', 'I-PER'), ('which', 'O', 'I-PER'), ('earlier', 'O', 'I-PER'), ('announced', 'O', 'I-PER'), ('a', 'O', 'B-LOC'), ('14', 'O', 'I-PER'), ('percent', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O', 'I-PER'), ('two', 'O', 'I-PER'), ('were', 'O', 'I-PER'), ('wounded', 'O', 'I-PER'), ('in', 'O', 'I-PER'), ('the', 'O', 'I-PER'), ('early', 'O', 'I-PER'), ('hours', 'O', 'I-PER'), ('of', 'O', 'I-PER'), ('Sunday', 'O', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 4\n",
    "seq_sz = 64\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if \"deberta\" in model_name else {}\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = NLP.get_hf_objects(\n",
    "        model_name, model_cls=model_cls, config=config, tokenizer_kwargs=tok_kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    try:\n",
    "        learn = None\n",
    "\n",
    "        batch_tok_tfm = TokenClassBatchTokenizeTransform(\n",
    "            hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz\n",
    "        )\n",
    "        blocks = (TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=labels))\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"tokens\"), get_y=ColReader(\"ner_tags\"), splitter=RandomSplitter())\n",
    "\n",
    "        dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        learn = Learner(dls, model, opt_func=partial(Adam), cbs=[BaseModelCallback], splitter=blurr_splitter).to_fp16()\n",
    "\n",
    "        learn.create_opt()  # -> will create your layer groups based on your \"splitter\" function\n",
    "        learn.freeze()\n",
    "\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print(\"*** TESTING Training/Results ***\")\n",
    "        learn.fit_one_cycle(\n",
    "            1,\n",
    "            lr_max=3e-5,\n",
    "            moms=(0.8, 0.7, 0.8),\n",
    "            cbs=[ShortEpochCallback(pct=0.1, short_valid=True), TokenClassMetricsCallback(tok_metrics=[\"accuracy\"])],\n",
    "        )\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"PASSED\", \"\"))\n",
    "        learn.show_results(learner=learn, max_n=2, trunc_at=10)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, \"FAILED\", err))\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        if learn:\n",
    "            del learn\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>AlbertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>BigBirdForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>CamembertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>ConvBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>DebertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>BertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>ElectraForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>FunnelForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>GPT2ForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>LayoutLMForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>LongformerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>MPNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>IBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>MobileBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>RemBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>RoFormerForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>RobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>SqueezeBertForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>XLMRobertaForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>XLNetForTokenClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module includes all the low, mid, and high-level API bits for token classification tasks training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_callbacks.ipynb.\n",
      "Converted 00_utils.ipynb.\n",
      "Converted 01_text-callbacks.ipynb.\n",
      "Converted 01_text-utils.ipynb.\n",
      "Converted 11_text-data-core.ipynb.\n",
      "Converted 11_text-modeling-core.ipynb.\n",
      "Converted 12_text-data-language-modeling.ipynb.\n",
      "Converted 12_text-modeling-language-modeling.ipynb.\n",
      "Converted 13_text-data-token-classification.ipynb.\n",
      "Converted 13_text-modeling-token-classification.ipynb.\n",
      "Converted 14_text-data-question-answering.ipynb.\n",
      "Converted 14_text-modeling-question-answering.ipynb.\n",
      "Converted 20_text-data-seq2seq-core.ipynb.\n",
      "Converted 20_text-modeling-seq2seq-core.ipynb.\n",
      "Converted 21_text-data-seq2seq-summarization.ipynb.\n",
      "Converted 21_text-modeling-seq2seq-summarization.ipynb.\n",
      "Converted 22_text-data-seq2seq-translation.ipynb.\n",
      "Converted 22_text-modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_text-examples-high-level-api.ipynb.\n",
      "Converted 99b_text-examples-glue.ipynb.\n",
      "Converted 99c_text-examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_text-examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
