{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp examples.multilabel_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "> This is an example of how to use blurr for multilabel classification tasks using both the mid and high level Blurr API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.hook import _print_shapes\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *\n",
    "from blurr.modeling.core import *\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what we're running with ...\n",
      "\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"Here's what we're running with ...\\n\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "# hide\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building our `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset civil_comments (/home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86d99de888c47a985fe162e6a5648c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset(\"civil_comments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# --- Option 1: Experimental subset (using 10k training examples) ---\n",
    "raw_train_df = raw_datasets[\"train\"].select(range(10000)).to_pandas()\n",
    "raw_valid_df = raw_datasets[\"validation\"].select(range(2000)).to_pandas()\n",
    "\n",
    "# --- Option 2: Full dataset (using the predefined training and validation sets) ---\n",
    "# raw_train_df = pd.DataFrame(raw_datasets['train'], columns=list(raw_datasets['train'].features.keys()))\n",
    "# raw_valid_df = pd.DataFrame(raw_datasets['validation'], columns=list(raw_datasets['validation'].features.keys()))\n",
    "\n",
    "raw_train_df[\"is_valid\"] = False\n",
    "raw_valid_df[\"is_valid\"] = True\n",
    "\n",
    "toxic_df = pd.concat([raw_train_df, raw_valid_df])\n",
    "print(len(toxic_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>text</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identity_attack   insult  obscene  severe_toxicity  sexual_explicit  \\\n",
       "0         0.000000  0.00000      0.0         0.000000              0.0   \n",
       "1         0.000000  0.00000      0.0         0.000000              0.0   \n",
       "2         0.000000  0.00000      0.0         0.000000              0.0   \n",
       "3         0.000000  0.00000      0.0         0.000000              0.0   \n",
       "4         0.021277  0.87234      0.0         0.021277              0.0   \n",
       "\n",
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   threat  toxicity  is_valid  \n",
       "0     0.0  0.000000     False  \n",
       "1     0.0  0.000000     False  \n",
       "2     0.0  0.000000     False  \n",
       "3     0.0  0.000000     False  \n",
       "4     0.0  0.893617     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identity_attack',\n",
       " 'insult',\n",
       " 'obscene',\n",
       " 'toxicity',\n",
       " 'severe_toxicity',\n",
       " 'sexual_explicit',\n",
       " 'threat']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_cols = [\"identity_attack\", \"insult\", \"obscene\", \"toxicity\", \"severe_toxicity\", \"sexual_explicit\", \"threat\"]\n",
    "lbl_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>text</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identity_attack  insult  obscene  severe_toxicity  sexual_explicit  \\\n",
       "0                0       0        0                0                0   \n",
       "1                0       0        0                0                0   \n",
       "2                0       0        0                0                0   \n",
       "3                0       0        0                0                0   \n",
       "4                0       1        0                0                0   \n",
       "\n",
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   threat  toxicity  is_valid  \n",
       "0       0         0     False  \n",
       "1       0         0     False  \n",
       "2       0         0     False  \n",
       "3       0         0     False  \n",
       "4       0         1     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = toxic_df.round({col: 0 for col in lbl_cols})\n",
    "toxic_df = toxic_df.convert_dtypes()\n",
    "\n",
    "toxic_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build our Hugging Face objects\n",
    "\n",
    "For our huggingface model, let's used the distilled version of RoBERTa. This should allow us to train the model on bigger mini-batches without much performance loss.  Even on my 1080Ti, I should be able to train all the parameters (which isn't possible with the `roberta-base` model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n",
      "<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>\n",
      "<class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_model.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "print(hf_arch)\n",
    "print(type(hf_config))\n",
    "print(type(hf_tokenizer))\n",
    "print(type(hf_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have to configure the `num_labels` to the number of labels we are predicting. Given that our labels are already encoded, we use a `MultiCategoryBlock` with encoded=True and vocab equal to the columns with our 1's and 0's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build our `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), MultiCategoryBlock(encoded=True, vocab=lbl_cols))\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(lbl_cols), splitter=ColSplitter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(toxic_df, bs=4, val_bs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 512]), torch.Size([4, 7]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), b[0][\"input_ids\"].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build our `Learner`\n",
    "\n",
    "With our DataLoaders built, we can now build our `Learner` and train.  We'll use mixed precision so we can train with bigger batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(Adam),\n",
    "    loss_func=PreCalculatedBCELoss(), # BCEWithLogitsLossFlat(),\n",
    "    metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter,\n",
    ").to_fp16()\n",
    "\n",
    "learn.loss_func.thresh = 0.2\n",
    "learn.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7]),\n",
       " SequenceClassifierOutput(loss=TensorMultiCategory(0.7306, device='cuda:1', grad_fn=<AliasBackward0>), logits=tensor([[ 0.1651, -0.0486,  0.1606,  0.0890, -0.0652,  0.0174,  0.1820],\n",
       "         [ 0.1949, -0.0781,  0.1461,  0.0850, -0.0577,  0.0305,  0.1821],\n",
       "         [ 0.1811, -0.0787,  0.1451,  0.0795, -0.0290,  0.0319,  0.1615],\n",
       "         [ 0.1804, -0.0760,  0.1438,  0.0824, -0.0140,  0.0327,  0.1537]],\n",
       "        device='cuda:1', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(b[0])\n",
    "preds.logits.shape, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.005248074419796467)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.058627</td>\n",
       "      <td>0.065105</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.051616</td>\n",
       "      <td>0.065715</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=slice(1e-8, 1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Everyone tries to hack everyone else.   I have no doubt Russia would try to hack even canada.   However, the US has been doing the same, if we recall Snowden.\\n\\nEven Merkel's phone conversations were being tapped by the CIA. \\n\\nThe real purpose of this issue is political.  Trump is upset because people are trying to imply that he didn't deserve his victory, that the Russians helped him.  It's an ego thing.  Good CEOs sometimes have giant egos.  I have no problem with that as long as they produce results, I gladly buy shares in their company.\\n\\nOtoh, Russia did invade Crimea recently, and their missile brought down a commercial airliner and killed lots of innocent people.  The world has a right to be annoyed at the Russians.\\n\\nIf you want to find evidence of Russians hacking, you will find them.  But if you want to find China or some guy in a basement somewhere, I have no doubt you can find the same as well.  Whether they succeeded or not, that's hard to prove, but there's lots of blackhats</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And, just as expected, it's quite alright to slag Catholics on these pages.  Civil Comments, yeah sure.\\n\\n1 poster made a sneering reference is very poor taste 'bout Catholics on another thread, and as predicted, rather than apologizing for something made in poor taste just to slag me, that poster doubled down.  Who me?  No way.  It's the victim who can't take a joke, takes things the wrong way.  It's always that way, that's what bullies do, they blame others, their fault.\\n\\nEven had another poster rush to his defence.  Wasn't that just precious?  Strength in #s.\\n\\nCoulds done the proper thing, coulda taken the high road, but no, easier just to feign innocence.\\n\\nThat's fine, no problem, just hope it's OK to do likewise back at that person, some would say he's a bigot, but I won't go there.  A withdraw woulda sufficed, but apparently it's the Catholics fault.\\n\\nI guess it's OK to use somebody's religion to make a joke and their fault if they can't take the joke.\\n\\nPretty low, man, pretty low</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func.thresh = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((#2) ['insult','toxicity'],),\n",
       "  [[False, True, False, True, False, False, False]],\n",
       "  [[0.0007236572564579546,\n",
       "    0.028274601325392723,\n",
       "    0.0050211502239108086,\n",
       "    0.03941638767719269,\n",
       "    1.3184367162466515e-06,\n",
       "    0.0016613906482234597,\n",
       "    0.00014883848780300468]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = \"\"\"\n",
    "Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. \n",
    "No enchiladas for them!\n",
    "\"\"\"\n",
    "learn.blurr_predict(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 7]), torch.Size([2000, 7]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, targs = learn.get_preds(with_loss=False)\n",
    "preds.shape, targs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level API\n",
    "\n",
    "With the high-level API, we can create our DataBlock, DataLoaders, and Blearner in one line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "try:\n",
    "    del learn\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build our Hugging Face objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n",
      "<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>\n",
      "<class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_model.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "print(hf_arch)\n",
    "print(type(hf_config))\n",
    "print(type(hf_tokenizer))\n",
    "print(type(hf_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure our `BlearnerForSequenceClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = BlearnerForSequenceClassification.from_dataframe(\n",
    "    toxic_df, pretrained_model_name, text_attr=\"text\", label_attr=lbl_cols, dl_kwargs={\"bs\": 4}\n",
    ")\n",
    "\n",
    "learn.loss_func.thresh = 0.02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.041047</td>\n",
       "      <td>0.066081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>01:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Everyone tries to hack everyone else.   I have no doubt Russia would try to hack even canada.   However, the US has been doing the same, if we recall Snowden.\\n\\nEven Merkel's phone conversations were being tapped by the CIA. \\n\\nThe real purpose of this issue is political.  Trump is upset because people are trying to imply that he didn't deserve his victory, that the Russians helped him.  It's an ego thing.  Good CEOs sometimes have giant egos.  I have no problem with that as long as they produce results, I gladly buy shares in their company.\\n\\nOtoh, Russia did invade Crimea recently, and their missile brought down a commercial airliner and killed lots of innocent people.  The world has a right to be annoyed at the Russians.\\n\\nIf you want to find evidence of Russians hacking, you will find them.  But if you want to find China or some guy in a basement somewhere, I have no doubt you can find the same as well.  Whether they succeeded or not, that's hard to prove, but there's lots of blackhats</td>\n",
       "      <td>[]</td>\n",
       "      <td>[insult, toxicity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Continued...\\n\\nCat:  Once again it is truly difficult to understand what you are attempting to say by your incoherent comments / gibberish. If you posted in plain English and complete sentences, it might.....might but I cannot say for sure.....help get your points across. \\nI don't know why you are so concerned about a market downturn. As you've said, you are mostly and have been for years, in cash. A 30% downturn would not bother me whatsoever. And I doubt that it would bother many true income (dividend / distribution) investors. I can see though how such a decline might scare someone who needs a 10%+ return on their money to pay their bills and who relies solely on trading the market to make those returns. Especially when they won't go short. That wuld scare me too. But for those dividend investors who just cash their dividend cheques / checks or better yet don't need that money and reinvest it and who would benefit from a market decline, share prices don't mean a whole lot.\\nSFI</td>\n",
       "      <td>[]</td>\n",
       "      <td>[insult, toxicity]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((#2) ['insult','toxicity'],),\n",
       "  [[False, True, False, True, False, False, False]],\n",
       "  [[0.0002814286563079804,\n",
       "    0.034543476998806,\n",
       "    0.0029982218984514475,\n",
       "    0.042017482221126556,\n",
       "    5.5763802464525725e-08,\n",
       "    0.0013961137738078833,\n",
       "    1.595290632394608e-05]])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = \"\"\"\n",
    "Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. \n",
    "No enchiladas for them!\n",
    "\"\"\"\n",
    "learn.blurr_predict(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 7]), torch.Size([2000, 7]), torch.Size([2000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, targs, losses = learn.get_preds(with_loss=True)\n",
    "preds.shape, targs.shape, losses.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build our Hugging Face objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n",
      "<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>\n",
      "<class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls, config=config)\n",
    "hf_model.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "print(hf_arch)\n",
    "print(type(hf_config))\n",
    "print(type(hf_tokenizer))\n",
    "print(type(hf_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare your dataset\n",
    "\n",
    "We'll create a `labels` column that includes the OHE labels for each example. The raw values come in the form of probabilities ranging from 0. to 1., so we simply round those >= .51 to 1.0, else set it to 0. for our purposes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset civil_comments (/home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f981f7418b946db8b01e92464e25b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['identity_attack', 'insult', 'obscene', 'severe_toxicity', 'sexual_explicit', 'text', 'threat', 'toxicity'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['identity_attack', 'insult', 'obscene', 'severe_toxicity', 'sexual_explicit', 'text', 'threat', 'toxicity'],\n",
       "     num_rows: 500\n",
       " })]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset(\"civil_comments\", split=[\"train[:1000]\", \"validation[:500]\"])\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-9fafb36afdab0472.arrow\n",
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-764f62d98456def5.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    inputs = hf_tokenizer(example[\"text\"], truncation=True)\n",
    "    targets = [\n",
    "        float(round(example[lbl])) for lbl in [\"identity_attack\", \"insult\", \"obscene\", \"severe_toxicity\", \"sexual_explicit\", \"threat\", \"toxicity\"]\n",
    "    ]\n",
    "    return {**inputs, **{\"labels\": targets}}\n",
    "\n",
    "\n",
    "tokenized_datasets = [ds.map(tokenize_function, batched=False) for ds in raw_datasets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build our `DataLoaders`\n",
    "\n",
    "By assigning the aforementioned labels to the `label_names` argument of our `BlurrDataLoader`s, we get the friendly label printed when we run `show_batch` or `show_results` intead of the label's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"identity_attack\", \"insult\", \"obscene\", \"severe_toxicity\", \"sexual_explicit\", \"threat\", \"toxicity\"]\n",
    "\n",
    "trn_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[0],\n",
    "    hf_arch=hf_arch,\n",
    "    hf_config=hf_config,\n",
    "    hf_tokenizer=hf_tokenizer,\n",
    "    hf_model=hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[1],\n",
    "    hf_arch=hf_arch,\n",
    "    hf_config=hf_config,\n",
    "    hf_tokenizer=hf_tokenizer,\n",
    "    hf_model=hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 161]), torch.Size([8, 7]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These are terrorists.  They  are not people like me.  They are not Oregonians, but invaders from Arizona, Utah, Idaho, Nevada, Ohio, Tennesee, Alabama, and more.  They are used to being chased by the law in their own jurisdictions, not used to being the law.  They are criminals.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Relentless attempts by liberal pundits to prop-up the notion that we live in a nation deranged and distorted by \"White racism\" is driven, it seems, by their fear that this ideological (sacred) \"cash-cow\" is dying, and must be resuscitated at all costs. To use a movie review as an excuse to dredge up what amounts to a liberal conceit, with no basis in reality, is particularly offensive. \\n\\nSince the'reviewer' raised the issue, it's worth mentioning: while systemic \"White-racism\" went extinct deca</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(Adam),\n",
    "    loss_func=BCEWithLogitsLossFlat(),\n",
    "    metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter,\n",
    ").to_fp16()\n",
    "\n",
    "learn.loss_func.thresh = 0.2\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.068528</td>\n",
       "      <td>0.078455</td>\n",
       "      <td>0.983714</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's why I qualified my allegation with \"odds are very good\".</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If the law forbids him from owning or possessing firearms, why did the court order HPD to give back his weapons after he plead guilty and was convicted of the offense they now say disqualifies him from gun ownership??</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your sequence classification model isn't training, make sure you have set the `num_labels` correctly (95% of the time this is the culprit).  And with this example, you can see that Blurr can make both your multiclassification and multilabel classification tasks a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n",
      "  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
