# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data-core.ipynb (unless otherwise specified).

__all__ = ['HF_BaseInput', 'HF_BeforeBatchTransform', 'HF_AfterBatchTransform', 'blurr_sort_func', 'HF_TextBlock',
           'get_blurr_tfm', 'BlurrBatchCreator', 'BlurrDataLoader', 'preproc_hf_dataset']

# Cell
import inspect
from functools import reduce, partial
from dataclasses import dataclass

import numpy as np
import pandas as pd

import torch
from datasets import load_dataset
from transformers import logging, DataCollatorWithPadding
from fastcore.all import *
from fastai.data.block import *
from fastai.data.core import DataLoader, DataLoaders, TfmdDL
from fastai.data.transforms import *
from fastai.text.data import SortedDL
from fastai.torch_core import TensorBase, display_df, get_empty_df, show_title

from ..utils import BLURR

logging.set_verbosity_error()

# Cell
class HF_BaseInput(TensorBase):
    """The base represenation of your inputs; used by the various fastai `show` methods"""
    def show(
        self,
        hf_tokenizer,   # A Hugging Face tokenizer
        ctx=None,       # The "context" associated to the current `show_batch/results` call
        trunc_at=None,  # Any truncation you want to apply to the decoded tokenized inputs
        **kwargs
    ):
        input_ids = self.cpu().numpy()
        decoded_input = str(hf_tokenizer.decode(input_ids, skip_special_tokens=True))[:trunc_at]

        return show_title(decoded_input, ctx=ctx, label='text')

# Cell
class HF_BeforeBatchTransform(Transform):
    """Handles everything you need to assemble a mini-batch of inputs and targets, as well as
    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.
    """
    def __init__(
        self,
        hf_arch,       # The abbreviation/name of your Hugging Face transformer architecture
        hf_config,     # A Hugging Face configuration object
        hf_tokenizer,  # A Hugging Face tokenizer
        hf_model,      # A Hugging Face model
        # The `max_length` argument applied to your `hf_tokenizer` during tokenization
        max_length=None,
        # The `padding` argument applied to your `hf_tokenizer` during tokenization
        padding=True,
        # The `truncation` argument applied to your `hf_tokenizer` during tokenization
        truncation=True,
        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`
        # if your inputs are pre-tokenized (not numericalized)
        is_split_into_words=False,
        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs
        tok_kwargs={},
        # Keyword arguments to apply to `HF_BeforeBatchTransform`
        **kwargs
    ):
        store_attr(self=self, names='hf_arch, hf_config, hf_tokenizer, hf_model')
        store_attr(self=self, names='max_length, padding, truncation, is_split_into_words, tok_kwargs')
        store_attr(self=self, names='kwargs')

    def encodes(
        self,
        samples  # A subset of data to put into a mini-batch
    ):
        """This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs
        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full
        dataset ahead of time.
        """
        samples = L(samples)

        # grab inputs
        if (is_listy(samples[0][0]) and not self.is_split_into_words):
            inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))
        else:
            inps = samples.itemgot(0).items

        # tokenize
        tok_d = self.hf_tokenizer(inps,
                                  max_length=self.max_length,
                                  padding=self.padding,
                                  truncation=self.truncation,
                                  is_split_into_words=self.is_split_into_words,
                                  return_tensors='pt',
                                  **self.tok_kwargs)

        # update samples with tokenized inputs (e.g. input_ids, attention_mask, etc...)
        d_keys = tok_d.keys()
        updated_samples= [ (*[{k: tok_d[k][idx] for k in d_keys}], *sample[1:])
                          for idx, sample in enumerate(samples) ]

        return updated_samples

# Cell
class HF_AfterBatchTransform(Transform):
    """A class used to cast your inputs into something understandable in fastai `show` methods"""
    def __init__(
        self,
        hf_arch,       # The abbreviation/name of your Hugging Face transformer architecture
        hf_config,     # A Hugging Face configuration object
        hf_tokenizer,  # A Hugging Face tokenizer
        hf_model,      # A Hugging Face model
        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`
        # if your inputs are pre-tokenized (not numericalized)
        is_split_into_words=False,
        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs
        tok_kwargs={},
        # Any text generation keyword arguments
        text_gen_kwargs={},
        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)
        input_return_type=HF_BaseInput
    ):
        store_attr(self=self, names='hf_arch, hf_config, hf_tokenizer, hf_model, tok_kwargs, text_gen_kwargs')
        store_attr(self=self, names='is_split_into_words, input_return_type')

    def decodes(
        self,
        # The encoded samples for your batch. `input_ids` will be pulled out of your dictionary of Hugging Face
        # inputs, cast to `self.input_return_type` and returned for methods such as `show_batch`
        encoded_samples
    ):
        """Returns the proper object and data for show related fastai methods"""
        if (isinstance(encoded_samples, dict)):
            return self.input_return_type(encoded_samples['input_ids'], hf_tokenizer=self.hf_tokenizer)
        return encoded_samples

# Cell
def blurr_sort_func(
    example,
    hf_tokenizer, # A Hugging Face tokenizer
    tok_kwargs    # Any other keyword arguments you want to include during tokenization
):
    """This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization"""
    return len(hf_tokenizer.tokenize(example[0], **tok_kwargs))

# Cell
class HF_TextBlock(TransformBlock):
    """The core `TransformBlock` to prepare your data for training in Blurr with fastai's `DataBlock` API"""
    def __init__(
        self,
        hf_arch=None,                      # The abbreviation/name of your Hugging Face transformer architecture
        hf_config=None,                    # A Hugging Face configuration object
        hf_tokenizer=None,                 # A Hugging Face tokenizer
        hf_model=None,                     # A Hugging Face model
        # The before batch transform you want to use to tokenize your raw data on the fly
        # (defaults to # An instance of `HF_BeforeBatchTransform`)
        before_batch_tfm=None,
        # The batch_tfms to apply to the creation of your DataLoaders,
        # (defaults to HF_AfterBatchTransform)
        after_batch_tfm=None,
        # The `max_length` argument applied to your `hf_tokenizer` during tokenization
        max_length=None,
        # The `padding` argument applied to your `hf_tokenizer` during tokenization
        padding=True,
        # The `truncation` argument applied to your `hf_tokenizer` during tokenization
        truncation=True,
        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`
        # if your inputs are pre-tokenized (not numericalized)
        is_split_into_words=False,
        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)
        input_return_type=HF_BaseInput,
        # The type of `DataLoader` you want created (defaults to `SortedDL`)
        dl_type=None,
        # Any keyword arguments you want applied to your before batch tfm
        before_batch_kwargs={},
        # Any keyword arguments you want applied to your after batch tfm (or referred to in fastai as `batch_tfms`)
        after_batch_kwargs={},
        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization
        tok_kwargs={},
        # Any keyword arguments you want applied to `HF_TextBlock`
        **kwargs
    ):
        if((not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and before_batch_tfm is None):
            raise ValueError(
                """You must supply the Hugging Face architecture, config, tokenizer, and model
                - or - an instances of HF_BeforeBatchTransform"""
            )

        if (before_batch_tfm is None):
            before_batch_tfm = HF_BeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,
                                                       max_length=max_length,
                                                       padding=padding,
                                                       truncation=truncation,
                                                       is_split_into_words=is_split_into_words,
                                                       tok_kwargs=tok_kwargs.copy(),
                                                       **before_batch_kwargs.copy())

        # as we'll have our HF_BeforeBatchTransforme by now, we'll use it to set the TransformBlock's
        # Hugging Face face objects
        self.hf_arch = before_batch_tfm.hf_arch
        self.hf_config = before_batch_tfm.hf_config
        self.hf_tokenizer = before_batch_tfm.hf_tokenizer
        self.hf_model = before_batch_tfm.hf_model

        if (after_batch_tfm is None):
            after_batch_tfm = HF_AfterBatchTransform(hf_arch=self.hf_arch,
                                                     hf_config=self.hf_config,
                                                     hf_tokenizer=self.hf_tokenizer,
                                                     hf_model=self.hf_model,
                                                     is_split_into_words=is_split_into_words,
                                                     tok_kwargs=tok_kwargs.copy(),
                                                     text_gen_kwargs={},
                                                     input_return_type=input_return_type,
                                                     **after_batch_kwargs.copy())

        if (dl_type is None):
            dl_sort_func = partial(blurr_sort_func, hf_tokenizer=self.hf_tokenizer, tok_kwargs=tok_kwargs)
            dl_type = partial(SortedDL, sort_func=dl_sort_func)

        return super().__init__(dl_type=dl_type,
                                dls_kwargs={ 'before_batch': before_batch_tfm },
                                batch_tfms=after_batch_tfm)

# Cell
def get_blurr_tfm(
    tfms_list,                          # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)
    tfm_class=HF_AfterBatchTransform    # The transform to find
):
    """Given a fastai DataLoaders batch transforms, this method can be used to get at a transform
    instance used in your Blurr DataBlock
    """
    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)

# Cell
@typedispatch
def show_batch(
    x:HF_BaseInput, # This typedispatched `show_batch` will be called for `HF_BaseInput` typed inputs
    y,              # Your targets
    samples,        # Your raw inputs/targets
    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for
    # decoding them into something understandable
    dataloaders,
    # Your `show_batch` context
    ctxs=None,
    # The maximum number of items to show
    max_n=6,
    # Any truncation your want applied to your decoded inputs
    trunc_at=None,
    # Any other keyword arguments you want applied to `show_batch`
    **kwargs
):
    # grab our tokenizer
    batch_tfm = get_blurr_tfm(dataloaders.after_batch)
    hf_tokenizer = batch_tfm.hf_tokenizer

    res = L()
    n_inp = dataloaders.n_inp

    for idx, (input_ids, label, sample) in enumerate(zip(x, y, samples)):
        if (idx >= max_n): break

        rets = [ hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at] ]
        for item in sample[n_inp:]: rets.append(label.item() if (torch.is_tensor(item)) else item)
        res.append(tuplify(rets))

    cols = ['text'] + [ 'target' if (i == 0) else f'target_{i}' for i in range(len(res[0]) - n_inp) ]
    display_df(pd.DataFrame(res, columns=cols)[:max_n])
    return ctxs

# Cell
@dataclass
class BlurrBatchCreator():
    """A class that can be assigned to a `TfmdDL.create_batch` method; used to in Blurr's low-level API
    to create batches that can be used in the Blurr library
    """
    def __init__(
        self,
        hf_tokenizer,      # Your Hugging Face tokenizer
        data_collator=None # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)
    ):
        self.hf_tokenizer = hf_tokenizer
        self.data_collator = data_collator if (data_collator) else DataCollatorWithPadding(tokenizer=hf_tokenizer)

    def __call__(
        self,
        features # A mini-batch (list of examples to run through your model)
    ):
        """This method will collate your data using `self.data_collator` and add a target element to the
        returned tuples if `labels` are defined as is the case when most Hugging Face datasets
        """
        batch = self.data_collator(features)
        if (isinstance(features[0], dict)):
            return dict(batch), batch['labels'] if ('labels' in features[0]) else dict(batch)

        return batch

# Cell
@delegates()
class BlurrDataLoader(TfmdDL):
    """A class that makes creating a fast.ai `DataLoader` that works with Blurr"""
    def __init__(
        self,
        dataset,                  # A standard PyTorch Dataset
        hf_arch,                  # The abbreviation/name of your Hugging Face transformer architecture
        hf_config,                # A Hugging Face configuration object
        hf_tokenizer,             # A Hugging Face tokenizer
        hf_model,                 # A Hugging Face model
        batch_creator=None,       # An instance of `BlurrBatchCreator` or equivalent
        after_batch_tfm=None,     # The batch_tfm used to decode Blurr batches (default: HF_AfterBatchTransform)
        preproccesing_func=None,  # (optional) A preprocessing function that will be applied to your dataset
        **kwargs
    ):
        if(preproccesing_func): dataset = preproccesing_func(dataset, hf_tokenizer, hf_model)

        if ('create_batch' in kwargs): kwargs.pop('create_batch')
        if (not batch_creator):
            batch_creator = BlurrBatchCreator(hf_tokenizer=hf_tokenizer)

        if ('after_batch' in kwargs): kwargs.pop('after_batch')
        if (not after_batch_tfm):
            after_batch_tfm = HF_AfterBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model)

        super().__init__(dataset=dataset, create_batch=batch_creator, after_batch=after_batch_tfm, **kwargs)
        store_attr(self=self, names='hf_arch, hf_config, hf_tokenizer, hf_model')

    def new(
        self,
        dataset=None, # A standard PyTorch and fastai dataset
        cls=None,     # The class you want to create an instance of (will be "self" if None)
        **kwargs      # Any additional keyword arguments you want to pass to the __init__ method of `cls`
    ):
        """We have to override the new method in order to add back the Hugging Face objects in this factory
        method (called for example in places like `show_results`). With the exception of the additions to the kwargs
        dictionary, the code below is pulled from the `DataLoaders.new` method as is.
        """
        if dataset is None: dataset = self.dataset
        if cls is None: cls = type(self)

        cur_kwargs = dict(dataset=dataset, num_workers=self.fake_l.num_workers, pin_memory=self.pin_memory,
                          timeout=self.timeout, bs=self.bs, shuffle=self.shuffle, drop_last=self.drop_last,
                          indexed=self.indexed, device=self.device)

        for n in self._methods:
            o = getattr(self, n)
            if not isinstance(o, MethodType): cur_kwargs[n] = o

        # we need to add these arguments back in (these, after_batch, and create_batch will go in as kwargs)
        kwargs['hf_arch'] = self.hf_arch
        kwargs['hf_config'] = self.hf_config
        kwargs['hf_tokenizer'] = self.hf_tokenizer
        kwargs['hf_model'] = self.hf_model

        return cls(**merge(cur_kwargs, kwargs))

# Cell
def preproc_hf_dataset(
    dataset,       # A standard PyTorch Dataset
    hf_tokenizer,  # A Hugging Face tokenizer
    hf_model       # A Hugging Face model
):
    """This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training
    libraries
    """
    if ('label') in dataset.column_names: dataset = dataset.rename_column("label", "labels")

    hf_model_fwd_args = list(inspect.signature(hf_model.forward).parameters.keys())
    bad_cols = set(dataset.column_names).difference(hf_model_fwd_args)
    dataset = dataset.remove_columns(bad_cols)

    dataset.set_format("torch")
    return dataset