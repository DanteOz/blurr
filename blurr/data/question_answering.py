# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_data-question-answering.ipynb (unless otherwise specified).

__all__ = ['pre_process_squad', 'HF_QuestionAnswerInput', 'HF_QABeforeBatchTransform']

# Cell
import ast
from functools import reduce

from fastcore.all import *
from fastai.data.block import DataBlock, CategoryBlock, ColReader, ColSplitter
from fastai.imports import *
from fastai.losses import CrossEntropyLossFlat
from fastai.torch_core import *
from fastai.torch_imports import *
from transformers import AutoModelForQuestionAnswering, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel

from ..utils import BLURR
from .core import HF_BaseInput, HF_AfterBatchTransform, HF_BeforeBatchTransform, first_blurr_tfm

logging.set_verbosity_error()


# Cell
def pre_process_squad(
    # A row in your pd.DataFrame
    row,
    # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)
    hf_arch: str,
    # A Hugging Face tokenizer
    hf_tokenizer: PreTrainedTokenizerBase,
    # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')
    ctx_attr: str = "context",
    # The attribute in your dataset that contains the question being asked (default: 'question')
    qst_attr: str = "question",
    # The attribute in your dataset that contains the actual answer (default: 'answer_text')
    ans_attr: str = "answer_text",
):
    context, qst, ans = row[ctx_attr], row[qst_attr], row[ans_attr]

    tok_kwargs = {}

    if hf_tokenizer.padding_side == "right":
        tok_input = hf_tokenizer.convert_ids_to_tokens(hf_tokenizer.encode(qst.lstrip(), context, **tok_kwargs))
    else:
        tok_input = hf_tokenizer.convert_ids_to_tokens(hf_tokenizer.encode(context, qst.lstrip(), **tok_kwargs))

    tok_ans = hf_tokenizer.tokenize(str(row[ans_attr]), **tok_kwargs)

    start_idx, end_idx = 0, 0
    for idx, tok in enumerate(tok_input):
        try:
            if tok == tok_ans[0] and tok_input[idx : idx + len(tok_ans)] == tok_ans:
                start_idx, end_idx = idx, idx + len(tok_ans)
                break
        except:
            pass

    row["tokenized_input"] = tok_input
    row["tokenized_input_len"] = len(tok_input)
    row["tok_answer_start"] = start_idx
    row["tok_answer_end"] = end_idx

    return row


# Cell
class HF_QuestionAnswerInput(HF_BaseInput):
    pass


# Cell
class HF_QABeforeBatchTransform(HF_BeforeBatchTransform):

    def encodes(self, samples):
        samples, batch_encoding = super().encodes(samples, return_batch_encoding=True)

        updated_samples = []
        for idx, s in enumerate(samples):
            # update the targets: is_found (s[1]), answer start token index (s[2]), and answer end token index (s[3])
            qst_mask = [i != 1 for i in batch_encoding.sequence_ids(idx)]
            start, end, has_ans = self.find_start_end(s[1], s[0]["input_ids"], s[0]["offset_mapping"], qst_mask)
            start_t, end_t, has_ans_t  = TensorCategory(start), TensorCategory(end), TensorCategory(has_ans)

            # cls_index: location of CLS token (used by xlnet and xlm); is a list.index(value) for pytorch tensor's
            s[0]["cls_index"] = (s[0]["input_ids"] == self.hf_tokenizer.cls_token_id).nonzero()[0]
            # p_mask: mask with 1 for token than cannot be in the answer, else 0 (used by xlnet and xlm)
            s[0]["p_mask"] = s[0]["special_tokens_mask"]

            updated_samples.append((s[0], has_ans_t, start_t, end_t))

        return updated_samples

    def find_start_end(self, ans_data, input_ids, offset_mapping, qst_mask):
        # mask the question tokens so they aren't included in the search
        masked_offset_mapping = offset_mapping.clone()
        masked_offset_mapping[qst_mask] = tensor([-100, -100])

        # based on the character start/end index, see if we can find the span of tokens in the `offset_mapping`
        start = torch.where((masked_offset_mapping[:, 0] == ans_data[1]) | (masked_offset_mapping[:, 1] == ans_data[1]))[0]
        end = torch.where((masked_offset_mapping[:, 0] <= ans_data[2]) & (masked_offset_mapping[:, 1] >= ans_data[2]))[0]

        if len(start) > 0 and len(end) > 0:
            start = start[-1]
            end = end[-1]

            if end < len(masked_offset_mapping):
                return (start, end, tensor(1))

        # if neither star or end is found, or the end token is part of this chunk, consider the answer not found
        return (tensor(0), tensor(0), tensor(0))


# Cell
@typedispatch
def show_batch(
    # This typedispatched `show_batch` will be called for `HF_QuestionAnswerInput` typed inputs
    x: HF_QuestionAnswerInput,
    # Your targets
    y,
    # Your raw inputs/targets
    samples,
    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for
    # decoding them into something understandable
    dataloaders,
    # Your `show_batch` context
    ctxs=None,
    # The maximum number of items to show
    max_n=6,
    # Any truncation your want applied to your decoded inputs
    trunc_at=None,
    # Any other keyword arguments you want applied to `show_batch`
    **kwargs
):
    # grab our tokenizer
    tfm = first_blurr_tfm(dataloaders, HF_QABeforeBatchTransform)
    hf_tokenizer = tfm.hf_tokenizer

    res = L()
    for sample, input_ids, has_ans, start, end in zip(samples, x, *y):
        txt = hf_tokenizer.decode(sample[0], skip_special_tokens=True)[:trunc_at]
        found = has_ans.item() == 1
        ans_text = hf_tokenizer.decode(input_ids[start:end], skip_special_tokens=False)
        res.append((txt, found, (start.item(), end.item()), ans_text))

    display_df(pd.DataFrame(res, columns=["text", "found", "start/end", "answer"])[:max_n])
    return ctxs
