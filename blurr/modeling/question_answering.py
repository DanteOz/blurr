# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_modeling-question-answering.ipynb (unless otherwise specified).

__all__ = ['squad_metric', 'QAModelCallback', 'MultiTargetLoss', 'QAMetricsCallback', 'compute_qa_metrics',
           'BlearnerForQuestionAnswering']

# Cell
import os, ast, inspect
from typing import Any, Callable, Dict, List, Optional, Union, Type

from fastcore.all import *
from fastai.callback.all import *
from fastai.data.block import DataBlock, CategoryBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter
from fastai.data.core import DataLoader, DataLoaders, TfmdDL
from fastai.imports import *
from fastai.learner import *
from fastai.losses import CrossEntropyLossFlat
from fastai.optimizer import Adam, OptimWrapper, params
from fastai.torch_core import *
from fastai.torch_imports import *
from fastprogress.fastprogress import progress_bar, master_bar
from seqeval import metrics as seq_metrics
from transformers import AutoModelForQuestionAnswering, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel

from ..utils import BLURR
from ..data.core import TextBlock, BlurrDataLoader, first_blurr_tfm
from .core import BaseModelCallback, PreCalculatedLoss, Blearner
from ..data.question_answering import QAPreprocessor, QATextInput, QABatchTokenizeTransform

# metrics we'll use in extractive qa
from datasets import load_metric
squad_metric = load_metric("squad")

logging.set_verbosity_error()


# Cell
class QAModelCallback(BaseModelCallback):
    """The prediction is a combination start/end logits"""

    def after_pred(self):
        super().after_pred()
        self.learn.pred = (self.pred.start_logits, self.pred.end_logits)


# Cell
class MultiTargetLoss(Module):
    """Provides the ability to apply different loss functions to multi-modal targets/predictions"""

    def __init__(
        self,
        # The loss function for each target
        loss_classes: List[Callable] = [CrossEntropyLossFlat, CrossEntropyLossFlat],
        # Any kwargs you want to pass to the loss functions above
        loss_classes_kwargs: List[dict] = [{}, {}],
        # The weights you want to apply to each loss (default: [1,1])
        weights: Union[List[float], List[int]] = [1, 1],
        # The `reduction` parameter of the lass function (default: 'mean')
        reduction: str = "mean",
    ):
        loss_funcs = [cls(reduction=reduction, **kwargs) for cls, kwargs in zip(loss_classes, loss_classes_kwargs)]
        store_attr(self=self, names="loss_funcs, weights")
        self._reduction = reduction

    # custom loss function must have either a reduction attribute or a reduction argument (like all fastai and
    # PyTorch loss functions) so that the framework can change this as needed (e.g., when doing lear.get_preds
    # it will set = 'none'). see this forum topic for more info: https://bit.ly/3br2Syz
    @property
    def reduction(self):
        return self._reduction

    @reduction.setter
    def reduction(self, v):
        self._reduction = v
        for lf in self.loss_funcs:
            lf.reduction = v

    def forward(self, outputs, *targets):
        loss = 0.0
        for i, loss_func, weights, output, target in zip(range(len(outputs)), self.loss_funcs, self.weights, outputs, targets):
            loss += weights * loss_func(output, target)

        return loss

    def activation(self, outs):
        acts = [self.loss_funcs[i].activation(o) for i, o in enumerate(outs)]
        return acts

    def decodes(self, outs):
        decodes = [self.loss_funcs[i].decodes(o) for i, o in enumerate(outs)]
        return decodes


# Cell
class QAMetricsCallback(Callback):
    def __init__(self, compute_metrics_func, validation_ds, qa_metrics=["exact_match", "f1"], **kwargs):
        self.run_before = Recorder

        store_attr()
        self.custom_metrics_dict = {k: None for k in qa_metrics}
        self.do_setup = True

    def setup(self):
        # one time setup code here.
        if not self.do_setup:
            return

        # grab the hf_tokenizer from the TokenClassBatchTokenizeTransform
        tfm = first_blurr_tfm(self.learn.dls, tfms=[QABatchTokenizeTransform])
        self.hf_tokenizer = tfm.hf_tokenizer
        self.tok_kwargs = tfm.tok_kwargs

        # add custom question answering specific metrics
        custom_metrics = L([ValueMetric(partial(self.metric_value, metric_key=k), k) for k in self.qa_metrics])
        self.learn.metrics = self.learn.metrics + custom_metrics

        self.do_setup = False

    def before_fit(self):
        self.setup()

    # --- batch before/after phases ---
    def before_batch(self):
        if self.training or self.learn.y is None:
            return

        self.batch_inputs = {k: v.cpu().detach().numpy() if isinstance(v, Tensor) else v for k, v in self.x.items()}

    def after_batch(self):
        if self.training or self.learn.y is None:
            return

        for i in range(len(self.batch_inputs["input_ids"])):
            batch_inps = {k: self.batch_inputs[k][i] for k in self.batch_inputs.keys()}
            self.results.append(
                {**batch_inps, "start_logits": self.pred[0][i].cpu().detach().numpy(), "end_logits": self.pred[1][i].cpu().detach().numpy()}
            )

    # --- validation begin/after phases ---
    def before_validate(self):
        self.results = []

    def after_validate(self):
        if len(self.results) < 1:
            return

        metric_vals_d = self.compute_metrics_func(self.results, self.validation_ds, self.hf_tokenizer, self.tok_kwargs)
        for k, v in metric_vals_d.items():
            self.custom_metrics_dict[k] = v

    # --- for ValueMetric metrics ---
    def metric_value(self, metric_key):
        return self.custom_metrics_dict[metric_key]


# Cell
def compute_qa_metrics(results, dataset, hf_tokenizer, tok_kwargs, id_attr="id", n_best=20):
    # what is the max length for our inputs?
    max_length = tok_kwargs.get("max_length", hf_tokenizer.model_max_length)

    # map examples to chunks indicies that are part of the
    example_to_chunks = collections.defaultdict(list)
    for idx, chunk in enumerate(results):
        example_to_chunks[chunk[id_attr]].append(idx)

    predicted_answers = []
    for item_idx, item in enumerate(dataset):
        example_id = item[id_attr]

        answers = []
        for chunk_idx in example_to_chunks[example_id]:
            chunk = results[chunk_idx]
            input_ids = chunk["input_ids"]
            start_logits = chunk["start_logits"]
            end_logits = chunk["end_logits"]

            start_indexes = np.argsort(start_logits)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logits)[-1 : -n_best - 1 : -1].tolist()

            for s_idx, start_index in enumerate(start_indexes):
                for e_idx, end_index in enumerate(end_indexes):
                    # Skip answers that are not fully in the context
                    if start_index == 0 and end_index == 0:
                        continue

                    # Skip answers with a length that is either < 0 or > max_answer_length
                    if end_index < start_index or end_index - start_index + 1 > max_length:
                        continue

                    answer = {
                        "text": hf_tokenizer.decode(input_ids[start_index:end_index], skip_special_tokens=True),
                        "logit_score": start_logits[start_index] + end_logits[end_index],
                    }
                    answers.append(answer)

        # select the answer with the best score
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    ref_answers = [{"id": item["id"], "answers": item["answers"]} for item_idx, item in enumerate(dataset)]

    metric_vals_d = squad_metric.compute(predictions=predicted_answers, references=ref_answers)
    return metric_vals_d


# Cell
@typedispatch
def show_results(
    # This typedispatched `show_results` will be called for `QuestionAnswerTextInput` typed inputs
    x: QATextInput,
    # The targets
    y,
    # Your raw inputs/targets
    samples,
    # The model's predictions
    outs,
    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into
    # something understandable
    learner,
    # Whether you want to remove special tokens during decoding/showing the outputs
    skip_special_tokens=True,
    # Your `show_results` context
    ctxs=None,
    # The maximum number of items to show
    max_n=6,
    # Any truncation your want applied to your decoded inputs
    trunc_at=None,
    # Any other keyword arguments you want applied to `show_results`
    **kwargs
):
    tfm = first_blurr_tfm(learner.dls, tfms=[QABatchTokenizeTransform])
    hf_tokenizer = tfm.hf_tokenizer

    res = L()
    for sample, input_ids, start, end, pred in zip(samples, x, *y, outs):
        txt = hf_tokenizer.decode(sample[0], skip_special_tokens=True)[:trunc_at]
        found = start.item() != 0 and end.item() != 0
        ans_text = hf_tokenizer.decode(input_ids[start:end], skip_special_tokens=False)

        pred_ans_toks = hf_tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=False)[int(pred[0]) : int(pred[1])]
        pred_ans_txt = hf_tokenizer.convert_tokens_to_string(pred_ans_toks)

        res.append((txt, found, (start.item(), end.item()), ans_text, (int(pred[0]), int(pred[1])), pred_ans_txt))

    display_df(pd.DataFrame(res, columns=["text", "found", "start/end", "answer", "pred start/end", "pred answer"]))
    return ctxs


# Cell
@delegates(Blearner.__init__)
class BlearnerForQuestionAnswering(Blearner):
    def __init__(self, dls: DataLoaders, hf_model: PreTrainedModel, **kwargs):
        kwargs["loss_func"] = kwargs.get("loss_func", MultiTargetLoss())
        super().__init__(dls, hf_model, base_model_cb=QAModelCallback, **kwargs)

    @classmethod
    def get_model_cls(self):
        return AutoModelForQuestionAnswering

    @classmethod
    def _get_x(cls, x, qst, ctx, padding_side="right"):
        return (x[qst], x[ctx]) if (padding_side == "right") else (x[ctx], x[qst])

    @classmethod
    def _create_learner(
        cls,
        # Your raw dataset
        data,
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func: Callable = None,
        # The maximum sequence length to constrain our data
        max_seq_len: int = None,
        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')
        context_attr: str = "context",
        # The attribute in your dataset that contains the question being asked (default: 'question')
        question_attr: str = "question",
        # The attribute in your dataset that contains the actual answer (default: 'answer_text')
        answer_text_attr: str = "answer_text",
        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')
        tok_ans_start_attr: str = "tok_answer_start",
        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')
        tok_ans_end_attr: str = "tok_answer_end",
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter: Callable = RandomSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={},
    ):
        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path, model_cls=cls.get_model_cls())

        # potentially used by our preprocess_func, it is the basis for our CategoryBlock vocab
        if max_seq_len is None:
            max_seq_len = hf_config.get("max_position_embeddings", 128)

        # client can pass in a function that takes the raw data, hf objects, and max_seq_len ... and
        # returns a DataFrame with the expected format
        if preprocess_func:
            data = preprocess_func(
                data,
                hf_arch,
                hf_config,
                hf_tokenizer,
                hf_model,
                max_seq_len,
                context_attr,
                question_attr,
                answer_text_attr,
                tok_ans_start_attr,
                tok_ans_end_attr,
            )

        # bits required by our "before_batch_tfm" and DataBlock
        vocab = list(range(max_seq_len))
        padding_side = hf_tokenizer.padding_side
        trunc_strat = "only_second" if (padding_side == "right") else "only_first"

        before_batch_tfm = QABatchTokenizeTransform(
            hf_arch,
            hf_config,
            hf_tokenizer,
            hf_model,
            max_length=max_seq_len,
            truncation=trunc_strat,
            tok_kwargs={"return_special_tokens_mask": True},
        )

        # define getters
        if isinstance(data, pd.DataFrame):
            get_x = partial(cls._get_x, qst=question_attr, ctx=context_attr, padding_side=padding_side)
            get_y = [ColReader(tok_ans_start_attr), ColReader(tok_ans_end_attr)]
        else:
            get_x = partial(cls._get_x, qst=question_attr, ctx=context_attr, padding_side=padding_side)
            get_y = [ItemGetter(tok_ans_start_attr), ItemGetter(tok_ans_end_attr)]

        # define DataBlock and DataLoaders
        blocks = (
            TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=QATextInput),
            CategoryBlock(vocab=vocab),
            CategoryBlock(vocab=vocab),
        )

        dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=dblock_splitter, n_inp=1)

        dls = dblock.dataloaders(data, **dl_kwargs.copy())

        # return BLearner instance
        return cls(dls, hf_model, **learner_kwargs.copy())

    @classmethod
    def from_dataframe(
        cls,
        # Your pandas DataFrame
        df: pd.DataFrame,
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func: Callable = None,
        # The maximum sequence length to constrain our data
        max_seq_len: int = None,
        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')
        context_attr: str = "context",
        # The attribute in your dataset that contains the question being asked (default: 'question')
        question_attr: str = "question",
        # The attribute in your dataset that contains the actual answer (default: 'answer_text')
        answer_text_attr: str = "answer_text",
        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')
        tok_ans_start_attr: str = "tok_answer_start",
        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')
        tok_ans_end_attr: str = "tok_answer_end",
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter: Callable = ColSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={},
    ):
        return cls._create_learner(
            df,
            pretrained_model_name_or_path,
            preprocess_func,
            max_seq_len,
            context_attr,
            question_attr,
            answer_text_attr,
            tok_ans_start_attr,
            tok_ans_end_attr,
            dblock_splitter,
            dl_kwargs,
            learner_kwargs,
        )

    @classmethod
    def from_csv(
        cls,
        # The path to your csv file
        csv_file: Union[Path, str],
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func: Callable = None,
        # The maximum sequence length to constrain our data
        max_seq_len: int = None,
        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')
        context_attr: str = "context",
        # The attribute in your dataset that contains the question being asked (default: 'question')
        question_attr: str = "question",
        # The attribute in your dataset that contains the actual answer (default: 'answer_text')
        answer_text_attr: str = "answer_text",
        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')
        tok_ans_start_attr: str = "tok_answer_start",
        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')
        tok_ans_end_attr: str = "tok_answer_end",
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter: Callable = ColSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={},
    ):
        df = pd.read_csv(csv_file)

        return cls.from_dataframe(
            df,
            pretrained_model_name_or_path,
            preprocess_func,
            max_seq_len,
            context_attr,
            question_attr,
            answer_text_attr,
            tok_ans_start_attr,
            tok_ans_end_attr,
            dblock_splitter,
            dl_kwargs,
            learner_kwargs,
        )

    @classmethod
    def from_dictionaries(
        cls,
        # A list of dictionaries
        ds: List[Dict],
        # The name or path of the pretrained model you want to fine-tune
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        # A function to perform any preprocessing required for your Dataset
        preprocess_func: Callable = None,
        # The maximum sequence length to constrain our data
        max_seq_len: int = None,
        # The attribute in your dataset that contains the context (where the answer is included) (default: 'context')
        context_attr: str = "context",
        # The attribute in your dataset that contains the question being asked (default: 'question')
        question_attr: str = "question",
        # The attribute in your dataset that contains the actual answer (default: 'answer_text')
        answer_text_attr: str = "answer_text",
        # The attribute in your dataset that contains the tokenized answer start (default: 'tok_answer_start')
        tok_ans_start_attr: str = "tok_answer_start",
        # The attribute in your dataset that contains the tokenized answer end(default: 'tok_answer_end')
        tok_ans_end_attr: str = "tok_answer_end",
        # A function that will split your Dataset into a training and validation set
        # See [here](https://docs.fast.ai/data.transforms.html#Split) for a list of fast.ai splitters
        dblock_splitter: Callable = RandomSplitter(),
        # Any kwargs to pass to your `DataLoaders`
        dl_kwargs={},
        # Any kwargs to pass to your task specific `Blearner`
        learner_kwargs={},
    ):
        return cls._create_learner(
            ds,
            pretrained_model_name_or_path,
            preprocess_func,
            max_seq_len,
            context_attr,
            question_attr,
            answer_text_attr,
            tok_ans_start_attr,
            tok_ans_end_attr,
            dblock_splitter,
            dl_kwargs,
            learner_kwargs,
        )
