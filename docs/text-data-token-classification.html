---

title: text.data.token_classification


keywords: fastai
sidebar: home_sidebar

summary: "This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...)"
description: "This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...)"
nb_path: "nbs/13_text-data-token-classification.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/13_text-data-token-classification.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>What we&#39;re running with at the time this documentation was generated:
torch: 1.10.1+cu111
fastai: 2.5.6
transformers: 4.16.2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">Setup<a class="anchor-link" href="#Setup"> </a></h2><p>We'll use a subset of <code>conll2003</code> to demonstrate how to configure your blurr code for token classification</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;conll2003&quot;</span><span class="p">)</span>
<span class="n">raw_datasets</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: [&#39;chunk_tags&#39;, &#39;id&#39;, &#39;ner_tags&#39;, &#39;pos_tags&#39;, &#39;tokens&#39;],
        num_rows: 14041
    })
    validation: Dataset({
        features: [&#39;chunk_tags&#39;, &#39;id&#39;, &#39;ner_tags&#39;, &#39;pos_tags&#39;, &#39;tokens&#39;],
        num_rows: 3250
    })
    test: Dataset({
        features: [&#39;chunk_tags&#39;, &#39;id&#39;, &#39;ner_tags&#39;, &#39;pos_tags&#39;, &#39;tokens&#39;],
        num_rows: 3453
    })
})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to get a list of the distinct entities we want to predict. If they are represented as list in their raw/readable form in another attribute/column in our dataset, we could use something like this to build a sorted list of distinct values as such: <code>labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))</code>.</p>
<p>Fortunately, the <code>conll2003</code> dataset allows us to get at this list directly using the code below.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;chunk_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;pos_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;O&#39;, &#39;B-ADJP&#39;, &#39;I-ADJP&#39;, &#39;B-ADVP&#39;, &#39;I-ADVP&#39;, &#39;B-CONJP&#39;, &#39;I-CONJP&#39;, &#39;B-INTJ&#39;, &#39;I-INTJ&#39;, &#39;B-LST&#39;, &#39;I-LST&#39;, &#39;B-NP&#39;, &#39;I-NP&#39;, &#39;B-PP&#39;, &#39;I-PP&#39;, &#39;B-PRT&#39;, &#39;I-PRT&#39;, &#39;B-SBAR&#39;, &#39;I-SBAR&#39;, &#39;B-UCP&#39;]
[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
[&#39;&#34;&#39;, &#34;&#39;&#39;&#34;, &#39;#&#39;, &#39;$&#39;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;.&#39;, &#39;:&#39;, &#39;``&#39;, &#39;CC&#39;, &#39;CD&#39;, &#39;DT&#39;, &#39;EX&#39;, &#39;FW&#39;, &#39;IN&#39;, &#39;JJ&#39;, &#39;JJR&#39;, &#39;JJS&#39;, &#39;LS&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span>
<span class="n">labels</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">conll2003_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_cls</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span>

<span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;roberta-base&quot;</span>  <span class="c1"># &quot;bert-base-multilingual-cased&quot;</span>
<span class="n">n_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">get_hf_objects</span><span class="p">(</span>
    <span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">model_cls</span><span class="o">=</span><span class="n">model_cls</span><span class="p">,</span> <span class="n">config_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_labels&quot;</span><span class="p">:</span> <span class="n">n_labels</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_config</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;roberta&#39;,
 transformers.models.roberta.configuration_roberta.RobertaConfig,
 transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,
 transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preprocessing">Preprocessing<a class="anchor-link" href="#Preprocessing"> </a></h2><p>Starting with version 2.0, <code>BLURR</code> provides a token classification preprocessing class that can be used to preprocess DataFrames or Hugging Face Datasets. We also introduce a novel way of handling long documents for this task that ensures tokens associated to a word is not split up in "chunked" documents.  See below for an example.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TokenClassPreprocessor" class="doc_header"><code>class</code> <code>TokenClassPreprocessor</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L28" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TokenClassPreprocessor</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>chunk_examples</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>word_stride</code></strong>:<code>int</code>=<em><code>2</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>, <strong><code>label_names</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]]=<em><code>None</code></em>, <strong><code>batch_size</code></strong>:<code>int</code>=<em><code>1000</code></em>, <strong><code>id_attr</code></strong>:<code>Optional</code>[<code>str</code>]=<em><code>None</code></em>, <strong><code>word_list_attr</code></strong>:<code>str</code>=<em><code>'tokens'</code></em>, <strong><code>label_list_attr</code></strong>:<code>str</code>=<em><code>'labels'</code></em>, <strong><code>is_valid_attr</code></strong>:<code>Optional</code>[<code>str</code>]=<em><code>'is_valid'</code></em>, <strong><code>slow_word_ids_func</code></strong>:<code>Optional</code>[<code>typing.Callable</code>]=<em><code>None</code></em>, <strong><code>tok_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>) :: <a href="/blurr/text-data-core.html#Preprocessor"><code>Preprocessor</code></a></p>
</blockquote>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr>
<td><strong><code>chunk_examples</code></strong></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>Set to <code>True</code> if the preprocessor should chunk examples that exceed <code>max_length</code></td>
</tr>
<tr>
<td><strong><code>word_stride</code></strong></td>
<td><code>int</code></td>
<td><code>2</code></td>
<td>Like "stride" except for words (not tokens)</td>
</tr>
<tr>
<td><strong><code>ignore_token_id</code></strong></td>
<td><code>int</code></td>
<td><code>-100</code></td>
<td>The token ID that should be ignored when calculating the loss</td>
</tr>
<tr>
<td><strong><code>label_names</code></strong></td>
<td><code>typing.Optional[typing.List[str]]</code></td>
<td><code>None</code></td>
<td>The label names (if not specified, will build from DataFrame)</td>
</tr>
<tr>
<td><strong><code>batch_size</code></strong></td>
<td><code>int</code></td>
<td><code>1000</code></td>
<td>The number of examples to process at a time</td>
</tr>
<tr>
<td><strong><code>id_attr</code></strong></td>
<td><code>typing.Optional[str]</code></td>
<td><code>None</code></td>
<td>The unique identifier in the dataset</td>
</tr>
<tr>
<td><strong><code>word_list_attr</code></strong></td>
<td><code>str</code></td>
<td><code>tokens</code></td>
<td>The attribute holding the list of words</td>
</tr>
<tr>
<td><strong><code>label_list_attr</code></strong></td>
<td><code>str</code></td>
<td><code>labels</code></td>
<td>The attribute holding the list of labels (one for each word in <code>word_list_attr</code>)</td>
</tr>
<tr>
<td><strong><code>is_valid_attr</code></strong></td>
<td><code>typing.Optional[str]</code></td>
<td><code>is_valid</code></td>
<td>The attribute that should be created if your are processing individual training and validation<br />datasets into a single dataset, and will indicate to which each example is associated</td>
</tr>
<tr>
<td><strong><code>slow_word_ids_func</code></strong></td>
<td><code>typing.Optional[typing.Callable]</code></td>
<td><code>None</code></td>
<td>If using a slow tokenizer, users will need to prove a <code>slow_word_ids_func</code> that accepts a<br />tokenizzer, example index, and a batch encoding as arguments and in turn returnes the<br />equavlient of fast tokenizer's <code>word_ids</code></td>
</tr>
<tr>
<td><strong><code>tok_kwargs</code></strong></td>
<td><code>dict</code></td>
<td><code>None</code></td>
<td>Tokenization kwargs that will be applied with calling the tokenizer</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="labels-are-Ids">labels are Ids<a class="anchor-link" href="#labels-are-Ids"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">TokenClassPreprocessor</span><span class="p">(</span>
    <span class="n">hf_tokenizer</span><span class="p">,</span>
    <span class="n">chunk_examples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">word_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">label_names</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">id_attr</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span>
    <span class="n">word_list_attr</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
    <span class="n">label_list_attr</span><span class="o">=</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">,</span>
    <span class="n">tok_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">proc_df</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">process_df</span><span class="p">(</span><span class="n">conll2003_df</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">proc_df</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span>
<span class="n">proc_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>61298
[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>proc_tokens</th>
      <th>proc_ner_tags</th>
      <th>chunk_tags</th>
      <th>id</th>
      <th>ner_tags</th>
      <th>pos_tags</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[EU, rejects, German, call, to, boycott]</td>
      <td>[3, 0, 7, 0, 0, 0]</td>
      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>
      <td>0</td>
      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>
      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>
      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[to, boycott, British, lamb, .]</td>
      <td>[0, 0, 7, 0, 0]</td>
      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>
      <td>0</td>
      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>
      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>
      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[Peter, Blackburn]</td>
      <td>[1, 2]</td>
      <td>[11, 12]</td>
      <td>1</td>
      <td>[1, 2]</td>
      <td>[22, 22]</td>
      <td>[Peter, Blackburn]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[BRUSSELS, 1996-08-22]</td>
      <td>[5, 0]</td>
      <td>[11, 12]</td>
      <td>2</td>
      <td>[5, 0]</td>
      <td>[22, 11]</td>
      <td>[BRUSSELS, 1996-08-22]</td>
    </tr>
  </tbody>
</table>
</div></div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="labels-are-entity-names">labels are entity names<a class="anchor-link" href="#labels-are-entity-names"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">conll2003_labeled_df</span> <span class="o">=</span> <span class="n">conll2003_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">conll2003_labeled_df</span><span class="o">.</span><span class="n">ner_tags</span> <span class="o">=</span> <span class="n">conll2003_labeled_df</span><span class="o">.</span><span class="n">ner_tags</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">lbl_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">lbl_id</span> <span class="ow">in</span> <span class="n">v</span><span class="p">])</span>
<span class="n">conll2003_labeled_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chunk_tags</th>
      <th>id</th>
      <th>ner_tags</th>
      <th>pos_tags</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>
      <td>0</td>
      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>
      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>
      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[11, 12]</td>
      <td>1</td>
      <td>[B-PER, I-PER]</td>
      <td>[22, 22]</td>
      <td>[Peter, Blackburn]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[11, 12]</td>
      <td>2</td>
      <td>[B-LOC, O]</td>
      <td>[22, 11]</td>
      <td>[BRUSSELS, 1996-08-22]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>
      <td>3</td>
      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>
      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>
      <td>4</td>
      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O]</td>
      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>
      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>
    </tr>
  </tbody>
</table>
</div></div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">TokenClassPreprocessor</span><span class="p">(</span>
    <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">label_names</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">id_attr</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="n">word_list_attr</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span> <span class="n">label_list_attr</span><span class="o">=</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">,</span> <span class="n">tok_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">proc_df</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">process_df</span><span class="p">(</span><span class="n">conll2003_labeled_df</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">proc_df</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preprocessor</span><span class="o">.</span><span class="n">label_names</span><span class="p">)</span>
<span class="n">proc_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>14041
[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>proc_tokens</th>
      <th>proc_ner_tags</th>
      <th>chunk_tags</th>
      <th>id</th>
      <th>ner_tags</th>
      <th>pos_tags</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[EU, rejects, German, call, to, boycott]</td>
      <td>[B-ORG, O, B-MISC, O, O, O]</td>
      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>
      <td>0</td>
      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>
      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>
      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[Peter, Blackburn]</td>
      <td>[B-PER, I-PER]</td>
      <td>[11, 12]</td>
      <td>1</td>
      <td>[B-PER, I-PER]</td>
      <td>[22, 22]</td>
      <td>[Peter, Blackburn]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[BRUSSELS, 1996-08-22]</td>
      <td>[B-LOC, O]</td>
      <td>[11, 12]</td>
      <td>2</td>
      <td>[B-LOC, O]</td>
      <td>[22, 11]</td>
      <td>[BRUSSELS, 1996-08-22]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[The, European, Commission, said, on, Thursday]</td>
      <td>[O, B-ORG, I-ORG, O, O, O]</td>
      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>
      <td>3</td>
      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>
      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>
    </tr>
  </tbody>
</table>
</div></div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Labeling-strategies">Labeling strategies<a class="anchor-link" href="#Labeling-strategies"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="BaseLabelingStrategy" class="doc_header"><code>class</code> <code>BaseLabelingStrategy</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L186" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>BaseLabelingStrategy</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>label_names</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]], <strong><code>non_entity_label</code></strong>:<code>str</code>=<em><code>'O'</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we include a <a href="/blurr/text-data-token-classification.html#BaseLabelingStrategy"><code>BaseLabelingStrategy</code></a> abstract class and several different strategies for assigning labels to your tokenized inputs. The "only first token" and "B/I" labeling strategies are discussed in the <a href="https://huggingface.co/course/chapter7/2?fw=pt">"Token Classification"</a> section in part 7 of the Hugging Face's Transformers course.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="OnlyFirstTokenLabelingStrategy" class="doc_header"><code>class</code> <code>OnlyFirstTokenLabelingStrategy</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L204" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>OnlyFirstTokenLabelingStrategy</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>label_names</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]], <strong><code>non_entity_label</code></strong>:<code>str</code>=<em><code>'O'</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>) :: <a href="/blurr/text-data-token-classification.html#BaseLabelingStrategy"><code>BaseLabelingStrategy</code></a></p>
</blockquote>
<p>Only the first token of word is associated with the label (all other subtokens with the <code>ignore_index_id</code>). Works where labels
are Ids or strings (in the later case we'll use the <code>label_names</code> to look up it's Id)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="SameLabelLabelingStrategy" class="doc_header"><code>class</code> <code>SameLabelLabelingStrategy</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L226" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>SameLabelLabelingStrategy</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>label_names</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]], <strong><code>non_entity_label</code></strong>:<code>str</code>=<em><code>'O'</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>) :: <a href="/blurr/text-data-token-classification.html#BaseLabelingStrategy"><code>BaseLabelingStrategy</code></a></p>
</blockquote>
<p>Every token associated with a given word is associated with the word's label. Works where labels
are Ids or strings (in the later case we'll use the <code>label_names</code> to look up it's Id)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="BILabelingStrategy" class="doc_header"><code>class</code> <code>BILabelingStrategy</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L244" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>BILabelingStrategy</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>label_names</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]], <strong><code>non_entity_label</code></strong>:<code>str</code>=<em><code>'O'</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>) :: <a href="/blurr/text-data-token-classification.html#BaseLabelingStrategy"><code>BaseLabelingStrategy</code></a></p>
</blockquote>
<p>If using B/I labels, the first token assoicated to a given word gets the "B" label while all other tokens related
to that same word get "I" labels.  If "I" labels don't exist, this strategy behaves like the <a href="/blurr/text-data-token-classification.html#OnlyFirstTokenLabelingStrategy"><code>OnlyFirstTokenLabelingStrategy</code></a>.
Works where labels are Ids or strings (in the later case we'll use the <code>label_names</code> to look up it's Id)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Reconstructing-inputs/labels">Reconstructing inputs/labels<a class="anchor-link" href="#Reconstructing-inputs/labels"> </a></h3><p>The utility methods below allow blurr users to reconstruct the original word/label associations from the input_ids/label associations.  For example, these are used in our token classification <code>show_batch</code> method below.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">raw_word_list</span> <span class="o">=</span> <span class="n">conll2003_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>
    <span class="n">raw_label_list</span> <span class="o">=</span> <span class="n">conll2003_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span>

    <span class="n">be</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="p">(</span><span class="n">raw_word_list</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">be</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
    <span class="n">targ_ids</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">100</span> <span class="k">if</span> <span class="p">(</span><span class="n">word_id</span> <span class="o">==</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">raw_label_list</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="n">be</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()]</span>

    <span class="n">tok_labels</span> <span class="o">=</span> <span class="n">get_token_labels_from_input_ids</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">targ_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">tok_label</span><span class="p">,</span> <span class="n">targ_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tok_labels</span><span class="p">,</span> <span class="p">[</span><span class="n">label_id</span> <span class="k">for</span> <span class="n">label_id</span> <span class="ow">in</span> <span class="n">targ_ids</span> <span class="k">if</span> <span class="n">label_id</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">]):</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="n">tok_label</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">targ_id</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_token_labels_from_input_ids" class="doc_header"><code>get_token_labels_from_input_ids</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L278" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_token_labels_from_input_ids</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>input_ids</code></strong>:<code>List</code>[<code>int</code>], <strong><code>token_label_ids</code></strong>:<code>List</code>[<code>int</code>], <strong><code>vocab</code></strong>:<code>List</code>[<code>str</code>], <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>, <strong><code>ignore_token</code></strong>:<code>str</code>=<em><code>'[xIGNx]'</code></em>)</p>
</blockquote>
<p>Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby
each tuple defines the "token" and its label name. For example:
[('ĠWay', B-PER), ('de', B-PER), ('ĠGill', I-PER), ('iam', I-PER), ('Ġloves'), ('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG)]</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr>
<td><strong><code>input_ids</code></strong></td>
<td><code>typing.List[int]</code></td>
<td></td>
<td>List of input_ids for the tokens in a single piece of processed text</td>
</tr>
<tr>
<td><strong><code>token_label_ids</code></strong></td>
<td><code>typing.List[int]</code></td>
<td></td>
<td>List of label indexs for each token</td>
</tr>
<tr>
<td><strong><code>vocab</code></strong></td>
<td><code>typing.List[str]</code></td>
<td></td>
<td>List of label names from witch the <code>label</code> indicies can be used to find the name of the label</td>
</tr>
<tr>
<td><strong><code>ignore_token_id</code></strong></td>
<td><code>int</code></td>
<td><code>-100</code></td>
<td>The token ID that should be ignored when calculating the loss</td>
</tr>
<tr>
<td><strong><code>ignore_token</code></strong></td>
<td><code>str</code></td>
<td><code>[xIGNx]</code></td>
<td>The token used to identifiy ignored tokens (default: [xIGNx])</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">raw_word_list</span> <span class="o">=</span> <span class="n">conll2003_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>
    <span class="n">raw_label_list</span> <span class="o">=</span> <span class="n">conll2003_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span>

    <span class="n">be</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="p">(</span><span class="n">raw_word_list</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">be</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
    <span class="n">targ_ids</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">100</span> <span class="k">if</span> <span class="p">(</span><span class="n">word_id</span> <span class="o">==</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">raw_label_list</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="n">be</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()]</span>

    <span class="n">tok_labels</span> <span class="o">=</span> <span class="n">get_token_labels_from_input_ids</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">targ_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">word_labels</span> <span class="o">=</span> <span class="n">get_word_labels_from_token_labels</span><span class="p">(</span><span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">tok_labels</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">word_label</span><span class="p">,</span> <span class="n">raw_word</span><span class="p">,</span> <span class="n">raw_label_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">word_labels</span><span class="p">,</span> <span class="n">raw_word_list</span><span class="p">,</span> <span class="n">raw_label_list</span><span class="p">):</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="n">word_label</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">raw_word</span><span class="p">)</span>
        <span class="n">test_eq</span><span class="p">(</span><span class="n">word_label</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">raw_label_id</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_word_labels_from_token_labels" class="doc_header"><code>get_word_labels_from_token_labels</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L309" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_word_labels_from_token_labels</code>(<strong><code>hf_arch</code></strong>:<code>str</code>, <strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>tok_labels</code></strong>)</p>
</blockquote>
<p>Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the
"word" and its label. Method assumes that model inputs are a list of words, and in conjunction with the <code>align_labels_with_tokens</code> method,
allows the user to reconstruct the orginal raw inputs and labels.</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_arch</code></strong></td>
<td><code>str</code></td>
<td></td>
<td><em>No Content</em></td>
</tr>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr>
<td><strong><code>tok_labels</code></strong></td>
<td></td>
<td></td>
<td>A list of tuples, where each represents a token and its label (e.g., [('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG), ...])</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mid-level-API">Mid-level API<a class="anchor-link" href="#Mid-level-API"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TokenTensorCategory" class="doc_header"><code>class</code> <code>TokenTensorCategory</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L339" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TokenTensorCategory</code>(<strong><code>x</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>TensorBase</code></p>
</blockquote>
<p>A <code>Tensor</code> which support subclass pickling, and maintains metadata when casting or after methods</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TokenCategorize" class="doc_header"><code>class</code> <code>TokenCategorize</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L344" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TokenCategorize</code>(<strong><code>vocab</code></strong>:<code>List</code>[<code>str</code>]=<em><code>None</code></em>, <strong><code>ignore_token</code></strong>:<code>str</code>=<em><code>'[xIGNx]'</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>) :: <code>Transform</code></p>
</blockquote>
<p>Reversible transform of a list of category string to <code>vocab</code> id</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>vocab</code></strong></td>
<td><code>typing.List[str]</code></td>
<td><code>None</code></td>
<td>The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))</td>
</tr>
<tr>
<td><strong><code>ignore_token</code></strong></td>
<td><code>str</code></td>
<td><code>[xIGNx]</code></td>
<td>The token used to identifiy ignored tokens (default: xIGNx)</td>
</tr>
<tr>
<td><strong><code>ignore_token_id</code></strong></td>
<td><code>int</code></td>
<td><code>-100</code></td>
<td>The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/blurr/text-data-token-classification.html#TokenCategorize"><code>TokenCategorize</code></a> modifies the fastai <code>Categorize</code> transform in a couple of ways.</p>
<p>First, it allows your targets to consist of a <code>Category</code> <strong><em>per</em></strong> token, and second, it uses the idea of an <code>ignore_token_id</code> to mask subtokens that don't need a prediction.  For example, the target of special tokens (e.g., pad, cls, sep) are set to <code>ignore_token_id</code> as are subsequent sub-tokens of a given token should more than 1 sub-token make it up.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TokenCategoryBlock" class="doc_header"><code>TokenCategoryBlock</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L377" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TokenCategoryBlock</code>(<strong><code>vocab</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]]=<em><code>None</code></em>, <strong><code>ignore_token</code></strong>:<code>str</code>=<em><code>'[xIGNx]'</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>)</p>
</blockquote>
<p><code>TransformBlock</code> for per-token categorical targets</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>vocab</code></strong></td>
<td><code>typing.Optional[typing.List[str]]</code></td>
<td><code>None</code></td>
<td>The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))</td>
</tr>
<tr>
<td><strong><code>ignore_token</code></strong></td>
<td><code>str</code></td>
<td><code>[xIGNx]</code></td>
<td>The token used to identifiy ignored tokens (default: xIGNx)</td>
</tr>
<tr>
<td><strong><code>ignore_token_id</code></strong></td>
<td><code>int</code></td>
<td><code>-100</code></td>
<td>The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TokenClassTextInput" class="doc_header"><code>class</code> <code>TokenClassTextInput</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L390" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TokenClassTextInput</code>(<strong><code>x</code></strong>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/text-data-core.html#TextInput"><code>TextInput</code></a></p>
</blockquote>
<p>The base represenation of your inputs; used by the various fastai <code>show</code> methods</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, we define a custom class, <a href="/blurr/text-data-token-classification.html#TokenClassTextInput"><code>TokenClassTextInput</code></a>, for the <code>@typedispatch</code>ed methods to use so that we can override how token classification inputs/targets are assembled, as well as, how the data is shown via methods like <code>show_batch</code> and <code>show_results</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TokenClassBatchTokenizeTransform" class="doc_header"><code>class</code> <code>TokenClassBatchTokenizeTransform</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/text/data/token_classification.py#L395" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TokenClassBatchTokenizeTransform</code>(<strong><code>hf_arch</code></strong>:<code>str</code>, <strong><code>hf_config</code></strong>:<code>PretrainedConfig</code>, <strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>hf_model</code></strong>:<code>PreTrainedModel</code>, <strong><code>include_labels</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>, <strong><code>labeling_strategy_cls</code></strong>:<a href="/blurr/text-data-token-classification.html#BaseLabelingStrategy"><code>BaseLabelingStrategy</code></a>=<em><code>OnlyFirstTokenLabelingStrategy</code></em>, <strong><code>target_label_names</code></strong>:<code>Optional</code>[<code>List</code>[<code>str</code>]]=<em><code>None</code></em>, <strong><code>non_entity_label</code></strong>:<code>str</code>=<em><code>'O'</code></em>, <strong><code>max_length</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>padding</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>truncation</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>is_split_into_words</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>slow_word_ids_func</code></strong>:<code>Optional</code>[<code>typing.Callable</code>]=<em><code>None</code></em>, <strong><code>tok_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/text-data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a></p>
</blockquote>
<p>Handles everything you need to assemble a mini-batch of inputs and targets, as well as
decode the dictionary produced as a byproduct of the tokenization process in the <code>encodes</code> method.</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_arch</code></strong></td>
<td><code>str</code></td>
<td></td>
<td>The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)</td>
</tr>
<tr>
<td><strong><code>hf_config</code></strong></td>
<td><code>PretrainedConfig</code></td>
<td></td>
<td>A specific configuration instance you want to use</td>
</tr>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr>
<td><strong><code>hf_model</code></strong></td>
<td><code>PreTrainedModel</code></td>
<td></td>
<td>A Hugging Face model</td>
</tr>
<tr>
<td><strong><code>include_labels</code></strong></td>
<td><code>bool</code></td>
<td><code>True</code></td>
<td>To control whether the "labels" are included in your inputs. If they are, the loss will be calculated in<br />the model's forward function and you can simply use <a href="/blurr/utils.html#PreCalculatedLoss"><code>PreCalculatedLoss</code></a> as your <code>Learner</code>'s loss function to use it</td>
</tr>
<tr>
<td><strong><code>ignore_token_id</code></strong></td>
<td><code>int</code></td>
<td><code>-100</code></td>
<td>The token ID that should be ignored when calculating the loss</td>
</tr>
<tr>
<td><strong><code>labeling_strategy_cls</code></strong></td>
<td><a href="/blurr/text-data-token-classification.html#BaseLabelingStrategy"><code>BaseLabelingStrategy</code></a></td>
<td><a href="/blurr/text-data-token-classification.html#OnlyFirstTokenLabelingStrategy"><code>OnlyFirstTokenLabelingStrategy</code></a></td>
<td>The labeling strategy you want to apply when associating labels with word tokens</td>
</tr>
<tr>
<td><strong><code>target_label_names</code></strong></td>
<td><code>typing.Optional[typing.List[str]]</code></td>
<td><code>None</code></td>
<td>the target label names</td>
</tr>
<tr>
<td><strong><code>non_entity_label</code></strong></td>
<td><code>str</code></td>
<td><code>O</code></td>
<td>the label for non-entity</td>
</tr>
<tr>
<td><strong><code>max_length</code></strong></td>
<td><code>typing.Optional[int]</code></td>
<td><code>None</code></td>
<td>To control the length of the padding/truncation. It can be an integer or None,<br />in which case it will default to the maximum length the model can accept. If the model has no<br />specific maximum input length, truncation/padding to max_length is deactivated.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>padding</code></strong></td>
<td><code>typing.Union[bool, str]</code></td>
<td><code>True</code></td>
<td>To control the <code>padding</code> applied to your <code>hf_tokenizer</code> during tokenization. If None, will default to<br /><code>False</code> or `'do_not_pad'.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>truncation</code></strong></td>
<td><code>typing.Union[bool, str]</code></td>
<td><code>True</code></td>
<td>To control <code>truncation</code> applied to your <code>hf_tokenizer</code> during tokenization. If None, will default to<br /><code>False</code> or <code>do_not_truncate</code>.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>is_split_into_words</code></strong></td>
<td><code>bool</code></td>
<td><code>True</code></td>
<td>The <code>is_split_into_words</code> argument applied to your <code>hf_tokenizer</code> during tokenization. Set this to <code>True</code><br />if your inputs are pre-tokenized (not numericalized)</td>
</tr>
<tr>
<td><strong><code>slow_word_ids_func</code></strong></td>
<td><code>typing.Optional[typing.Callable]</code></td>
<td><code>None</code></td>
<td>If using a slow tokenizer, users will need to prove a <code>slow_word_ids_func</code> that accepts a<br />tokenizzer, example index, and a batch encoding as arguments and in turn returnes the<br />equavlient of fast tokenizer's `word_ids``</td>
</tr>
<tr>
<td><strong><code>tok_kwargs</code></strong></td>
<td><code>dict</code></td>
<td><code>None</code></td>
<td>Any other keyword arguments you want included when using your <code>hf_tokenizer</code> to tokenize your inputs</td>
</tr>
<tr>
<td><strong><code>kwargs</code></strong></td>
<td></td>
<td></td>
<td><em>No Content</em></td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/blurr/text-data-token-classification.html#TokenClassBatchTokenizeTransform"><code>TokenClassBatchTokenizeTransform</code></a> is used to exclude any of the target's tokens we don't want to include in the loss calcuation (e.g. padding, cls, sep, etc...).</p>
<p>Note also that we default <code>is_split_into_words = True</code> since token classification tasks expect a list of words and labels for each word.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Examples">Examples<a class="anchor-link" href="#Examples"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Using-the-mid-level-API">Using the mid-level API<a class="anchor-link" href="#Using-the-mid-level-API"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Batch-Time-Tokenization">Batch-Time Tokenization<a class="anchor-link" href="#Batch-Time-Tokenization"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-1:-Get-your-Hugging-Face-objects.">Step 1: Get your Hugging Face objects.<a class="anchor-link" href="#Step-1:-Get-your-Hugging-Face-objects."> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;distilroberta-base&quot;</span>
<span class="n">n_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">get_hf_objects</span><span class="p">(</span>
    <span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">model_cls</span><span class="o">=</span><span class="n">AutoModelForTokenClassification</span><span class="p">,</span> <span class="n">config_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_labels&quot;</span><span class="p">:</span> <span class="n">n_labels</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_config</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;roberta&#39;,
 transformers.models.roberta.configuration_roberta.RobertaConfig,
 transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,
 transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-2:-Create-your-DataBlock">Step 2: Create your <code>DataBlock</code><a class="anchor-link" href="#Step-2:-Create-your-DataBlock"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_tok_tfm</span> <span class="o">=</span> <span class="n">TokenClassBatchTokenizeTransform</span><span class="p">(</span>
    <span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span><span class="p">,</span> <span class="n">labeling_strategy_cls</span><span class="o">=</span><span class="n">BILabelingStrategy</span><span class="p">,</span> <span class="n">target_label_names</span><span class="o">=</span><span class="n">labels</span>
<span class="p">)</span>
<span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextBlock</span><span class="p">(</span><span class="n">batch_tokenize_tfm</span><span class="o">=</span><span class="n">batch_tok_tfm</span><span class="p">,</span> <span class="n">input_return_type</span><span class="o">=</span><span class="n">TokenClassTextInput</span><span class="p">),</span> <span class="n">TokenCategoryBlock</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">labels</span><span class="p">))</span>

<span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">&quot;tokens&quot;</span><span class="p">),</span> <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">),</span> <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-3:-Build-your-DataLoaders">Step 3: Build your <code>DataLoaders</code><a class="anchor-link" href="#Step-3:-Build-your-DataLoaders"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">conll2003_df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(2, torch.Size([4, 88]), torch.Size([4, 88]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">dls</span><span class="p">,</span> <span class="n">max_n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">trunc_at</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea "><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word / target label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[('15', 'O'), ('-', 'O'), ('Christian', 'B-PER'), ('Cullen', 'I-PER'), (',', 'O'), ('14', 'O'), ('-', 'O'), ('Jeff', 'B-PER'), ('Wilson', 'I-PER'), (',', 'O'), ('13', 'O'), ('-', 'O'), ('Walter', 'B-PER'), ('Little', 'I-PER'), (',', 'O'), ('12', 'O'), ('-', 'O'), ('Frank', 'B-PER'), ('Bunce', 'I-PER'), (',', 'O')]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[('In', 'O'), ('New', 'B-LOC'), ('York', 'I-LOC'), (',', 'O'), ('Garret', 'B-PER'), ('Anderson', 'I-PER'), ('and', 'O'), ('Gary', 'B-PER'), ('DiSarcina', 'I-PER'), ('drove', 'O'), ('in', 'O'), ('two', 'O'), ('runs', 'O'), ('apiece', 'O'), ('in', 'O'), ('a', 'O'), ('five-run', 'O'), ('first', 'O'), ('inning', 'O'), ('and', 'O')]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[('But', 'O'), ('the', 'O'), ('official', 'O'), (',', 'O'), ('Aryeh', 'B-PER'), ('Shumer', 'I-PER'), (',', 'O'), ('said', 'O'), ('it', 'O'), ('was', 'O'), ('only', 'O'), ('fitting', 'O'), ('that', 'O'), ('Weizman', 'B-PER'), ('and', 'O'), ('Arafat', 'B-PER'), ('should', 'O'), ('talk', 'O'), ('after', 'O'), ('the', 'O')]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[('Serbian', 'B-MISC'), ('officials', 'O'), ('have', 'O'), ('denied', 'O'), ('any', 'O'), ('abuses', 'O'), ('occurred', 'O'), ('during', 'O'), ('a', 'O'), ('10-day', 'O'), ('registration', 'O'), ('period', 'O'), ('and', 'O'), ('the', 'O'), ('Bosnian', 'B-MISC'), ('Serbs', 'I-MISC'), (',', 'O'), ('angry', 'O'), ('at', 'O'), ('the', 'O')]</td>
    </tr>
  </tbody>
</table></div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Passing-extra-infromation">Passing extra infromation<a class="anchor-link" href="#Passing-extra-infromation"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-1b:-Get-your-Hugging-Face-objects.">Step 1b: Get your Hugging Face objects.<a class="anchor-link" href="#Step-1b:-Get-your-Hugging-Face-objects."> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;distilroberta-base&quot;</span>
<span class="n">n_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">get_hf_objects</span><span class="p">(</span>
    <span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">model_cls</span><span class="o">=</span><span class="n">AutoModelForTokenClassification</span><span class="p">,</span> <span class="n">config_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_labels&quot;</span><span class="p">:</span> <span class="n">n_labels</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_config</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;roberta&#39;,
 transformers.models.roberta.configuration_roberta.RobertaConfig,
 transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,
 transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-1b.-Preprocess-dataset">Step 1b. Preprocess dataset<a class="anchor-link" href="#Step-1b.-Preprocess-dataset"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">TokenClassPreprocessor</span><span class="p">(</span>
    <span class="n">hf_tokenizer</span><span class="p">,</span>
    <span class="n">label_names</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">id_attr</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">,</span>
    <span class="n">word_list_attr</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
    <span class="n">label_list_attr</span><span class="o">=</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">,</span>
    <span class="n">tok_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">proc_df</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">process_df</span><span class="p">(</span><span class="n">conll2003_df</span><span class="p">)</span>
<span class="n">proc_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>proc_tokens</th>
      <th>proc_ner_tags</th>
      <th>chunk_tags</th>
      <th>id</th>
      <th>ner_tags</th>
      <th>pos_tags</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>
      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>
      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>
      <td>0</td>
      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>
      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>
      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[Peter, Blackburn]</td>
      <td>[1, 2]</td>
      <td>[11, 12]</td>
      <td>1</td>
      <td>[1, 2]</td>
      <td>[22, 22]</td>
      <td>[Peter, Blackburn]</td>
    </tr>
  </tbody>
</table>
</div></div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-2:-Create-your-DataBlock">Step 2: Create your <code>DataBlock</code><a class="anchor-link" href="#Step-2:-Create-your-DataBlock"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_tok_tfm</span> <span class="o">=</span> <span class="n">TokenClassBatchTokenizeTransform</span><span class="p">(</span><span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span><span class="p">,</span> <span class="n">target_label_names</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextBlock</span><span class="p">(</span><span class="n">batch_tokenize_tfm</span><span class="o">=</span><span class="n">batch_tok_tfm</span><span class="p">,</span> <span class="n">input_return_type</span><span class="o">=</span><span class="n">TokenClassTextInput</span><span class="p">),</span> <span class="n">TokenCategoryBlock</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">labels</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">get_x</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">proc_tokens</span><span class="p">}</span>


<span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span> <span class="n">get_x</span><span class="o">=</span><span class="n">get_x</span><span class="p">,</span> <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">&quot;proc_ner_tags&quot;</span><span class="p">),</span> <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Step-3:-Build-your-DataLoaders">Step 3: Build your <code>DataLoaders</code><a class="anchor-link" href="#Step-3:-Build-your-DataLoaders"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">proc_df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;id&#39;, &#39;labels&#39;])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(2, torch.Size([4, 130]), torch.Size([4, 130]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">dataloaders</span><span class="o">=</span><span class="n">dls</span><span class="p">,</span> <span class="n">max_n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">trunc_at</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea "><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word / target label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[('The', 'O'), ('Brady', 'B-PER'), ('bill', 'O'), (',', 'O'), ('calling', 'O'), ('for', 'O'), ('a', 'O'), ('waiting', 'O'), ('period', 'O'), ('before', 'O'), ('someone', 'O'), ('could', 'O'), ('buy', 'O'), ('a', 'O'), ('gun', 'O'), ('so', 'O'), ('a', 'O'), ('background', 'O'), ('check', 'O'), ('could', 'O')]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[('The', 'O'), ('mayor', 'O'), ('of', 'O'), ('Acatepec', 'B-LOC'), (',', 'O'), ('a', 'O'), ('small', 'O'), ('town', 'O'), ('some', 'O'), ('310', 'O'), ('miles', 'O'), ('(', 'O'), ('500', 'O'), ('km', 'O'), (')', 'O'), ('south', 'O'), ('of', 'O'), ('Mexico', 'B-LOC'), ('City', 'I-LOC'), (',', 'O')]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[('A', 'O'), ('few', 'O'), ('years', 'O'), ('ago', 'O'), (',', 'O'), ('barter', 'O'), ('deals', 'O'), ('accounted', 'O'), ('for', 'O'), ('up', 'O'), ('to', 'O'), ('25-30', 'O'), ('percent', 'O'), ('of', 'O'), ('Russian', 'B-MISC'), ('exports', 'O'), ('because', 'O'), ('"', 'O'), ('thousands', 'O'), ('(', 'O')]</td>
    </tr>
  </tbody>
</table></div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tests">Tests<a class="anchor-link" href="#Tests"> </a></h2><p>The tests below to ensure the core DataBlock code above works for <strong>all</strong> pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.</p>
<p><strong>Note</strong>: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue <em>(or a PR if you'd like to fix it yourself)</em></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;conll2003&quot;</span><span class="p">)</span>
<span class="n">conll2003_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea "><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>arch</th>
      <th>tokenizer</th>
      <th>model_name</th>
      <th>result</th>
      <th>error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>albert</td>
      <td>AlbertTokenizerFast</td>
      <td>hf-internal-testing/tiny-albert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>bert</td>
      <td>BertTokenizerFast</td>
      <td>hf-internal-testing/tiny-bert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>big_bird</td>
      <td>BigBirdTokenizerFast</td>
      <td>google/bigbird-roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>camembert</td>
      <td>CamembertTokenizerFast</td>
      <td>camembert-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>convbert</td>
      <td>ConvBertTokenizerFast</td>
      <td>YituTech/conv-bert-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>deberta</td>
      <td>DebertaTokenizerFast</td>
      <td>hf-internal-testing/tiny-deberta</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>6</th>
      <td>bert</td>
      <td>BertTokenizerFast</td>
      <td>sshleifer/tiny-distilbert-base-cased</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>electra</td>
      <td>ElectraTokenizerFast</td>
      <td>hf-internal-testing/tiny-electra</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td>funnel</td>
      <td>FunnelTokenizerFast</td>
      <td>huggingface/funnel-small-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td>gpt2</td>
      <td>GPT2TokenizerFast</td>
      <td>sshleifer/tiny-gpt2</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>10</th>
      <td>layoutlm</td>
      <td>LayoutLMTokenizerFast</td>
      <td>hf-internal-testing/tiny-layoutlm</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>11</th>
      <td>longformer</td>
      <td>LongformerTokenizerFast</td>
      <td>allenai/longformer-base-4096</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>12</th>
      <td>mpnet</td>
      <td>MPNetTokenizerFast</td>
      <td>microsoft/mpnet-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>13</th>
      <td>ibert</td>
      <td>RobertaTokenizerFast</td>
      <td>kssteven/ibert-roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>14</th>
      <td>mobilebert</td>
      <td>MobileBertTokenizerFast</td>
      <td>google/mobilebert-uncased</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>15</th>
      <td>rembert</td>
      <td>RemBertTokenizerFast</td>
      <td>google/rembert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>16</th>
      <td>roformer</td>
      <td>RoFormerTokenizerFast</td>
      <td>junnyu/roformer_chinese_sim_char_ft_small</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>17</th>
      <td>roberta</td>
      <td>RobertaTokenizerFast</td>
      <td>roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>18</th>
      <td>squeezebert</td>
      <td>SqueezeBertTokenizerFast</td>
      <td>squeezebert/squeezebert-uncased</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>19</th>
      <td>xlm_roberta</td>
      <td>XLMRobertaTokenizerFast</td>
      <td>xlm-roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>20</th>
      <td>xlnet</td>
      <td>XLNetTokenizerFast</td>
      <td>xlnet-base-cased</td>
      <td>PASSED</td>
      <td></td>
    </tr>
  </tbody>
</table></div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

