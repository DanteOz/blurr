---

title: data.seq2seq.core


keywords: fastai
sidebar: home_sidebar

summary: "This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
description: "This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
nb_path: "nbs/10_data-seq2seq-core.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/10_data-seq2seq-core.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>What we&#39;re running with at the time this documentation was generated:
torch: 1.10.1+cu111
fastai: 2.5.3
transformers: 4.16.2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">Setup<a class="anchor-link" href="#Setup"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;facebook/bart-large-cnn&quot;</span>
<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">BLURR</span><span class="o">.</span><span class="n">get_hf_objects</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">model_cls</span><span class="o">=</span><span class="n">BartForConditionalGeneration</span><span class="p">)</span>
<span class="n">hf_arch</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_config</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;bart&#39;,
 transformers.models.bart.configuration_bart.BartConfig,
 transformers.models.bart.tokenization_bart_fast.BartTokenizerFast,
 transformers.models.bart.modeling_bart.BartForConditionalGeneration)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preprocessing">Preprocessing<a class="anchor-link" href="#Preprocessing"> </a></h2><p>Starting with version 2.0, BLURR provides a preprocessing base class that can be used to build seq2seq preprocessed datasets from pandas DataFrames or Hugging Face Datasets</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqPreprocessor" class="doc_header"><code>class</code> <code>Seq2SeqPreprocessor</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L24" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqPreprocessor</code>(<strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>batch_size</code></strong>:<code>int</code>=<em><code>1000</code></em>, <strong><code>text_attr</code></strong>:<code>str</code>=<em><code>'text'</code></em>, <strong><code>max_input_tok_length</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>target_text_attr</code></strong>:<code>str</code>=<em><code>'summary'</code></em>, <strong><code>max_target_tok_length</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>is_valid_attr</code></strong>:<code>Optional</code>[<code>str</code>]=<em><code>'is_valid'</code></em>, <strong><code>tok_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>) :: <a href="/blurr/data-core.html#Preprocessor"><code>Preprocessor</code></a></p>
</blockquote>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr>
<td><strong><code>batch_size</code></strong></td>
<td><code>int</code></td>
<td><code>1000</code></td>
<td>The number of examples to process at a time</td>
</tr>
<tr>
<td><strong><code>text_attr</code></strong></td>
<td><code>str</code></td>
<td><code>text</code></td>
<td>The attribute holding the text</td>
</tr>
<tr>
<td><strong><code>max_input_tok_length</code></strong></td>
<td><code>Optional[int]</code></td>
<td>``</td>
<td>The maximum length (# of tokens) allowed for inputs. Will default to the max length allowed<br />by the model if not provided</td>
</tr>
<tr>
<td><strong><code>target_text_attr</code></strong></td>
<td><code>str</code></td>
<td><code>summary</code></td>
<td>The attribute holding the summary</td>
</tr>
<tr>
<td><strong><code>max_target_tok_length</code></strong></td>
<td><code>Optional[int]</code></td>
<td>``</td>
<td>The maximum length (# of tokens) allowed for targets</td>
</tr>
<tr>
<td><strong><code>is_valid_attr</code></strong></td>
<td><code>Optional[str]</code></td>
<td><code>is_valid</code></td>
<td>The attribute that should be created if your are processing individual training and validation<br />datasets into a single dataset, and will indicate to which each example is associated</td>
</tr>
<tr>
<td><strong><code>tok_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Tokenization kwargs that will be applied with calling the tokenizer</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mid-level-API">Mid-level API<a class="anchor-link" href="#Mid-level-API"> </a></h2><p>Base tokenization, batch transform, and DataBlock methods</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Seq2SeqTextInput"><a href="/blurr/data-seq2seq-core.html#Seq2SeqTextInput"><code>Seq2SeqTextInput</code></a><a class="anchor-link" href="#Seq2SeqTextInput"> </a></h3><p>A <a href="/blurr/data-seq2seq-core.html#Seq2SeqTextInput"><code>Seq2SeqTextInput</code></a> object is returned from the decodes method of <a href="/blurr/data-seq2seq-core.html#Seq2SeqBatchTokenizeTransform"><code>Seq2SeqBatchTokenizeTransform</code></a> as a means to customize <code>@typedispatch</code>ed functions like <code>DataLoaders.show_batch</code> and <code>Learner.show_results</code>. The value will the your "input_ids".</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqTextInput" class="doc_header"><code>class</code> <code>Seq2SeqTextInput</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L69" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqTextInput</code>(<strong><code>x</code></strong>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#TextInput"><code>TextInput</code></a></p>
</blockquote>
<p>The base represenation of your inputs; used by the various fastai <code>show</code> methods</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Seq2SeqBatchTokenizeTransform"><a href="/blurr/data-seq2seq-core.html#Seq2SeqBatchTokenizeTransform"><code>Seq2SeqBatchTokenizeTransform</code></a><a class="anchor-link" href="#Seq2SeqBatchTokenizeTransform"> </a></h3><p>We create a subclass of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a> for summarization tasks to add <code>decoder_input_ids</code> and <code>labels</code> (if we want Hugging Face to calculate the loss for us) to our inputs during training. See <a href="https://huggingface.co/transformers/glossary.html#labels">here</a> and <a href="https://huggingface.co/transformers/glossary.html#decoder-input-ids">here</a> for more information on these additional inputs used in summarization, translation, and conversational training tasks. How they should look for particular architectures can be found by looking at those model's <code>forward</code> function's docs (See <a href="https://huggingface.co/transformers/model_doc/bart.html#transformers.BartModel.forward">here</a> for BART for example)</p>
<p>Note also that <code>labels</code> is simply target_ids shifted to the right by one since the task to is to predict the next token based on the current (and all previous) <code>decoder_input_ids</code>.</p>
<p>And lastly, we also update our targets to just be the <code>input_ids</code> of our target sequence so that fastai's <code>Learner.show_results</code> works (again, almost all the fastai bits require returning a single tensor to work).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqBatchTokenizeTransform" class="doc_header"><code>class</code> <code>Seq2SeqBatchTokenizeTransform</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L74" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqBatchTokenizeTransform</code>(<strong><code>hf_arch</code></strong>:<code>str</code>, <strong><code>hf_config</code></strong>:<code>PretrainedConfig</code>, <strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>hf_model</code></strong>:<code>PreTrainedModel</code>, <strong><code>include_labels</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>, <strong><code>max_length</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>max_target_length</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>padding</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>truncation</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>is_split_into_words</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>tok_kwargs</code></strong>=<em><code>{}</code></em>, <strong><code>text_gen_kwargs</code></strong>=<em><code>{}</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a></p>
</blockquote>
<p>Handles everything you need to assemble a mini-batch of inputs and targets, as well as
decode the dictionary produced as a byproduct of the tokenization process in the <code>encodes</code> method.</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_arch</code></strong></td>
<td><code>str</code></td>
<td></td>
<td>The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)</td>
</tr>
<tr>
<td><strong><code>hf_config</code></strong></td>
<td><code>PretrainedConfig</code></td>
<td></td>
<td>A specific configuration instance you want to use</td>
</tr>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr>
<td><strong><code>hf_model</code></strong></td>
<td><code>PreTrainedModel</code></td>
<td></td>
<td>A Hugging Face model</td>
</tr>
<tr>
<td><strong><code>include_labels</code></strong></td>
<td><code>bool</code></td>
<td><code>True</code></td>
<td>To control whether the "labels" are included in your inputs. If they are, the loss will be calculated in<br />the model's forward function and you can simply use <a href="/blurr/modeling-core.html#PreCalculatedLoss"><code>PreCalculatedLoss</code></a> as your <code>Learner</code>'s loss function to use it</td>
</tr>
<tr>
<td><strong><code>ignore_token_id</code></strong></td>
<td><code>int</code></td>
<td><code>-100</code></td>
<td>The token ID that should be ignored when calculating the loss</td>
</tr>
<tr>
<td><strong><code>max_length</code></strong></td>
<td><code>int</code></td>
<td>``</td>
<td>To control the length of the padding/truncation of the input sequence. It can be an integer or None,<br />in which case it will default to the maximum length the model can accept. If the model has no<br />specific maximum input length, truncation/padding to max_length is deactivated.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>max_target_length</code></strong></td>
<td><code>int</code></td>
<td>``</td>
<td>To control the length of the padding/truncation of the target sequence. It can be an integer or None,<br />in which case it will default to the maximum length the model can accept. If the model has no<br />specific maximum input length, truncation/padding to max_length is deactivated.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>padding</code></strong></td>
<td><code>Union[bool, str]</code></td>
<td><code>True</code></td>
<td>To control the <code>padding</code> applied to your <code>hf_tokenizer</code> during tokenization. If None, will default to<br /><code>False</code> or `'do_not_pad'.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>truncation</code></strong></td>
<td><code>Union[bool, str]</code></td>
<td><code>True</code></td>
<td>To control <code>truncation</code> applied to your <code>hf_tokenizer</code> during tokenization. If None, will default to<br /><code>False</code> or <code>do_not_truncate</code>.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>is_split_into_words</code></strong></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>The <code>is_split_into_words</code> argument applied to your <code>hf_tokenizer</code> during tokenization. Set this to <code>True</code><br />if your inputs are pre-tokenized (not numericalized)</td>
</tr>
<tr>
<td><strong><code>tok_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Any other keyword arguments you want included when using your <code>hf_tokenizer</code> to tokenize your inputs</td>
</tr>
<tr>
<td><strong><code>text_gen_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Any keyword arguments to pass to the <code>hf_model.generate</code> method</td>
</tr>
<tr>
<td><strong><code>kwargs</code></strong></td>
<td></td>
<td></td>
<td><em>No Content</em></td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Seq2SeqBatchDecodeTransform"><a href="/blurr/data-seq2seq-core.html#Seq2SeqBatchDecodeTransform"><code>Seq2SeqBatchDecodeTransform</code></a><a class="anchor-link" href="#Seq2SeqBatchDecodeTransform"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqBatchDecodeTransform" class="doc_header"><code>class</code> <code>Seq2SeqBatchDecodeTransform</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L185" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqBatchDecodeTransform</code>(<strong><code>input_return_type</code></strong>:<code>Type</code>=<em><code>TextInput</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#BatchDecodeTransform"><code>BatchDecodeTransform</code></a></p>
</blockquote>
<p>A class used to cast your inputs as <code>input_return_type</code> for fastai <code>show</code> methods</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Seq2SeqTextBlock"><a href="/blurr/data-seq2seq-core.html#Seq2SeqTextBlock"><code>Seq2SeqTextBlock</code></a><a class="anchor-link" href="#Seq2SeqTextBlock"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_text_gen_kwargs" class="doc_header"><code>default_text_gen_kwargs</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L191" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_text_gen_kwargs</code>(<strong><code>hf_config</code></strong>, <strong><code>hf_model</code></strong>, <strong><code>task</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">default_text_gen_kwargs</span><span class="p">(</span><span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;max_length&#39;: 142,
 &#39;min_length&#39;: 56,
 &#39;do_sample&#39;: False,
 &#39;early_stopping&#39;: True,
 &#39;num_beams&#39;: 4,
 &#39;temperature&#39;: 1.0,
 &#39;top_k&#39;: 50,
 &#39;top_p&#39;: 1.0,
 &#39;repetition_penalty&#39;: 1.0,
 &#39;bad_words_ids&#39;: None,
 &#39;bos_token_id&#39;: 0,
 &#39;pad_token_id&#39;: 1,
 &#39;eos_token_id&#39;: 2,
 &#39;length_penalty&#39;: 2.0,
 &#39;no_repeat_ngram_size&#39;: 3,
 &#39;encoder_no_repeat_ngram_size&#39;: 0,
 &#39;num_return_sequences&#39;: 1,
 &#39;decoder_start_token_id&#39;: 2,
 &#39;use_cache&#39;: True,
 &#39;num_beam_groups&#39;: 1,
 &#39;diversity_penalty&#39;: 0.0,
 &#39;output_attentions&#39;: False,
 &#39;output_hidden_states&#39;: False,
 &#39;output_scores&#39;: False,
 &#39;return_dict_in_generate&#39;: False,
 &#39;forced_bos_token_id&#39;: 0,
 &#39;forced_eos_token_id&#39;: 2,
 &#39;remove_invalid_values&#39;: False}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqTextBlock" class="doc_header"><code>class</code> <code>Seq2SeqTextBlock</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L211" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqTextBlock</code>(<strong><code>hf_arch</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>hf_config</code></strong>:<code>PretrainedConfig</code>=<em><code>None</code></em>, <strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>=<em><code>None</code></em>, <strong><code>hf_model</code></strong>:<code>PreTrainedModel</code>=<em><code>None</code></em>, <strong><code>batch_tokenize_tfm</code></strong>:<code>Optional</code>[<a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a>]=<em><code>None</code></em>, <strong><code>batch_decode_tfm</code></strong>:<code>Optional</code>[<a href="/blurr/data-core.html#BatchDecodeTransform"><code>BatchDecodeTransform</code></a>]=<em><code>None</code></em>, <strong><code>max_length</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>max_target_length</code></strong>=<em><code>None</code></em>, <strong><code>padding</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>truncation</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>input_return_type</code></strong>=<em><code>Seq2SeqTextInput</code></em>, <strong><code>dl_type</code></strong>=<em><code>SortedDL</code></em>, <strong><code>batch_tokenize_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>batch_decode_kwargs</code></strong>:<code>dict</code>=<em><code>{}</code></em>, <strong><code>tok_kwargs</code></strong>=<em><code>{}</code></em>, <strong><code>text_gen_kwargs</code></strong>=<em><code>{}</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#TextBlock"><code>TextBlock</code></a></p>
</blockquote>
<p>The core <code>TransformBlock</code> to prepare your inputs for training in Blurr with fastai's <code>DataBlock</code> API</p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>hf_arch</code></strong></td>
<td><code>str</code></td>
<td>``</td>
<td>The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an<br />instance of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a> to <code>before_batch_tfm</code>)</td>
</tr>
<tr>
<td><strong><code>hf_config</code></strong></td>
<td><code>PretrainedConfig</code></td>
<td>``</td>
<td>A Hugging Face configuration object (not required if passing in an<br />instance of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a> to <code>before_batch_tfm</code>)</td>
</tr>
<tr>
<td><strong><code>hf_tokenizer</code></strong></td>
<td><code>PreTrainedTokenizerBase</code></td>
<td>``</td>
<td>A Hugging Face tokenizer (not required if passing in an<br />instance of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a> to <code>before_batch_tfm</code>)</td>
</tr>
<tr>
<td><strong><code>hf_model</code></strong></td>
<td><code>PreTrainedModel</code></td>
<td>``</td>
<td>A Hugging Face model (not required if passing in an<br />instance of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a> to <code>before_batch_tfm</code>)</td>
</tr>
<tr>
<td><strong><code>batch_tokenize_tfm</code></strong></td>
<td><code>BatchTokenizeTransform]</code></td>
<td>``</td>
<td>The before_batch_tfm you want to use to tokenize your raw data on the fly<br />(defaults to an instance of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a>)</td>
</tr>
<tr>
<td><strong><code>batch_decode_tfm</code></strong></td>
<td><code>BatchDecodeTransform]</code></td>
<td>``</td>
<td>The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods,<br />(defaults to BatchDecodeTransform)</td>
</tr>
<tr>
<td><strong><code>max_length</code></strong></td>
<td><code>int</code></td>
<td>``</td>
<td>To control the length of the padding/truncation for the input sequence. It can be an integer or None,<br />in which case it will default to the maximum length the model can accept. If the model has no<br />specific maximum input length, truncation/padding to max_length is deactivated.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>max_target_length</code></strong></td>
<td><code>NoneType</code></td>
<td>``</td>
<td>To control the length of the padding/truncation for the target sequence. It can be an integer or None,<br />in which case it will default to the maximum length the model can accept. If the model has no<br />specific maximum input length, truncation/padding to max_length is deactivated.<br />See [Everything you always wanted to know about padding and truncation](<a href="https://huggingface.co/transformers/preprocessing.html#everything-y">https://huggingface.co/transformers/preprocessing.html#everything-y</a></td>
</tr>
<tr>
<td><strong><code>padding</code></strong></td>
<td><code>Union[bool, str]</code></td>
<td><code>True</code></td>
<td>To control the <code>padding</code> applied to your <code>hf_tokenizer</code> during tokenization. If None, will default to<br /><code>False</code> or `'do_not_pad'.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>truncation</code></strong></td>
<td><code>Union[bool, str]</code></td>
<td><code>True</code></td>
<td>To control <code>truncation</code> applied to your <code>hf_tokenizer</code> during tokenization. If None, will default to<br /><code>False</code> or <code>do_not_truncate</code>.<br />See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr>
<td><strong><code>input_return_type</code></strong></td>
<td><code>_TensorMeta</code></td>
<td><code>Seq2SeqTextInput</code></td>
<td>The return type your decoded inputs should be cast too (used by methods such as <code>show_batch</code>)</td>
</tr>
<tr>
<td><strong><code>dl_type</code></strong></td>
<td><code>type</code></td>
<td><code>SortedDL</code></td>
<td>The type of <code>DataLoader</code> you want created (defaults to <code>SortedDL</code>)</td>
</tr>
<tr>
<td><strong><code>batch_tokenize_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Any keyword arguments you want applied to your <code>batch_tokenize_tfm</code></td>
</tr>
<tr>
<td><strong><code>batch_decode_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Any keyword arguments you want applied to your <code>batch_decode_tfm</code> (will be set as a fastai <code>batch_tfms</code>)</td>
</tr>
<tr>
<td><strong><code>tok_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Any keyword arguments you want your Hugging Face tokenizer to use during tokenization</td>
</tr>
<tr>
<td><strong><code>text_gen_kwargs</code></strong></td>
<td><code>dict</code></td>
<td>``</td>
<td>Any keyword arguments you want to have applied with generating text<br />(default: default_text_gen_kwargs)</td>
</tr>
<tr>
<td><strong><code>kwargs</code></strong></td>
<td></td>
<td></td>
<td><em>No Content</em></td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="show_batch"><code>show_batch</code><a class="anchor-link" href="#show_batch"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This module includes the fundamental bits to all Seq2Seq transformers data preparation.</p>

</div>
</div>
</div>
</div>
 

