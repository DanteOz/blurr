---

title: data.seq2seq.core


keywords: fastai
sidebar: home_sidebar

summary: "This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
description: "This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
nb_path: "nbs/10_data-seq2seq-core.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/10_data-seq2seq-core.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>What we&#39;re running with at the time this documentation was generated:
torch: 1.10.2+cu102
fastai: 2.5.3
transformers: 4.16.2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s2">&quot;facebook/bart-large-cnn&quot;</span>
<span class="n">hf_arch</span><span class="p">,</span> <span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">,</span> <span class="n">hf_model</span> <span class="o">=</span> <span class="n">BLURR</span><span class="o">.</span><span class="n">get_hf_objects</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> 
                                                                  <span class="n">model_cls</span><span class="o">=</span><span class="n">BartForConditionalGeneration</span><span class="p">)</span>

<span class="n">hf_arch</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_config</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_tokenizer</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;bart&#39;,
 transformers.models.bart.configuration_bart.BartConfig,
 transformers.models.bart.tokenization_bart_fast.BartTokenizerFast,
 transformers.models.bart.modeling_bart.BartForConditionalGeneration)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Base-tokenization,-batch-transform,-and-DataBlock-methods">Base tokenization, batch transform, and DataBlock methods<a class="anchor-link" href="#Base-tokenization,-batch-transform,-and-DataBlock-methods"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Seq2Seq tasks are essentially conditional generation tasks, this applies to specific derived tasks such as summarization and translation.  Given this, we can use the <em>same</em> HF_Seq2Seq transforms, <a href="/blurr/data-seq2seq-core.html#Seq2SeqTextInput"><code>Seq2SeqTextInput</code></a>, and <a href="/blurr/data-seq2seq-core.html#Seq2SeqTextBlock"><code>Seq2SeqTextBlock</code></a> for these tasks</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqTextInput" class="doc_header"><code>class</code> <code>Seq2SeqTextInput</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L29" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqTextInput</code>(<strong><code>x</code></strong>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#TextInput"><code>TextInput</code></a></p>
</blockquote>
<p>The base represenation of your inputs; used by the various fastai <code>show</code> methods</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We create a subclass of <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a> for summarization tasks to add <code>decoder_input_ids</code> and <code>labels</code> to our inputs during training, which will in turn allow the Hugging Face model to calculate the loss for us.  See <a href="https://huggingface.co/transformers/glossary.html#labels">here</a> and <a href="https://huggingface.co/transformers/glossary.html#decoder-input-ids">here</a> for more information on these additional inputs used in summarization, translation, and conversational training tasks. How they should look for particular architectures can be found by looking at those model's <code>forward</code> function's docs (See <a href="https://huggingface.co/transformers/model_doc/bart.html#transformers.BartModel.forward">here</a> for BART for example)</p>
<p>Note also that <code>labels</code> is simply target_ids shifted to the right by one since the task to is to predict the next token based on the current (and all previous) <code>decoder_input_ids</code>.</p>
<p>And lastly, we also update our targets to just be the <code>input_ids</code> of our target sequence so that fastai's <code>Learner.show_results</code> works (again, almost all the fastai bits require returning a single tensor to work).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_text_gen_kwargs" class="doc_header"><code>default_text_gen_kwargs</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L32" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_text_gen_kwargs</code>(<strong><code>hf_config</code></strong>, <strong><code>hf_model</code></strong>, <strong><code>task</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p><strong>Parameters:</strong></p>
<ul>
<li><p><strong><code>hf_config</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></p>
</li>
<li><p><strong><code>hf_model</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></p>
</li>
<li><p><strong><code>task</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">default_text_gen_kwargs</span><span class="p">(</span><span class="n">hf_config</span><span class="p">,</span> <span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;max_length&#39;: 142,
 &#39;min_length&#39;: 56,
 &#39;do_sample&#39;: False,
 &#39;early_stopping&#39;: True,
 &#39;num_beams&#39;: 4,
 &#39;temperature&#39;: 1.0,
 &#39;top_k&#39;: 50,
 &#39;top_p&#39;: 1.0,
 &#39;repetition_penalty&#39;: 1.0,
 &#39;bad_words_ids&#39;: None,
 &#39;bos_token_id&#39;: 0,
 &#39;pad_token_id&#39;: 1,
 &#39;eos_token_id&#39;: 2,
 &#39;length_penalty&#39;: 2.0,
 &#39;no_repeat_ngram_size&#39;: 3,
 &#39;encoder_no_repeat_ngram_size&#39;: 0,
 &#39;num_return_sequences&#39;: 1,
 &#39;decoder_start_token_id&#39;: 2,
 &#39;use_cache&#39;: True,
 &#39;num_beam_groups&#39;: 1,
 &#39;diversity_penalty&#39;: 0.0,
 &#39;output_attentions&#39;: False,
 &#39;output_hidden_states&#39;: False,
 &#39;output_scores&#39;: False,
 &#39;return_dict_in_generate&#39;: False,
 &#39;forced_bos_token_id&#39;: 0,
 &#39;forced_eos_token_id&#39;: 2,
 &#39;remove_invalid_values&#39;: False}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqBatchTokenizeTransform" class="doc_header"><code>class</code> <code>Seq2SeqBatchTokenizeTransform</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L49" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqBatchTokenizeTransform</code>(<strong><code>hf_arch</code></strong>:<code>str</code>, <strong><code>hf_config</code></strong>:<code>PretrainedConfig</code>, <strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>, <strong><code>hf_model</code></strong>:<code>PreTrainedModel</code>, <strong><code>ignore_token_id</code></strong>:<code>int</code>=<em><code>-100</code></em>, <strong><code>max_length</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>max_target_length</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>padding</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>truncation</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>is_split_into_words</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>tok_kwargs</code></strong>=<em><code>{}</code></em>, <strong><code>text_gen_kwargs</code></strong>=<em><code>{}</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a></p>
</blockquote>
<p>Handles everything you need to assemble a mini-batch of inputs and targets, as well as
decode the dictionary produced as a byproduct of the tokenization process in the <code>encodes</code> method.</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>hf_arch</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>    <p>The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)</p></li>
</ul>
<ul>
<li><strong><code>hf_config</code></strong> : <em><code>&lt;class 'transformers.configuration_utils.PretrainedConfig'&gt;</code></em>    <p>A specific configuration instance you want to use</p></li>
</ul>
<ul>
<li><strong><code>hf_tokenizer</code></strong> : <em><code>&lt;class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'&gt;</code></em>  <p>A Hugging Face tokenizer</p></li>
</ul>
<ul>
<li><strong><code>hf_model</code></strong> : <em><code>&lt;class 'transformers.modeling_utils.PreTrainedModel'&gt;</code></em>   <p>A Hugging Face model</p></li>
</ul>
<ul>
<li><strong><code>ignore_token_id</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em>    <p>The token ID that should be ignored when calculating the loss</p></li>
</ul>
<ul>
<li><strong><code>max_length</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>To control the length of the padding/truncation of the input sequence. It can be an integer or None,
in which case it will default to the maximum length the model can accept. If the model has no
specific maximum input length, truncation/padding to max_length is deactivated.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>max_target_length</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em>  <p>To control the length of the padding/truncation of the target sequence. It can be an integer or None,
in which case it will default to the maximum length the model can accept. If the model has no
specific maximum input length, truncation/padding to max_length is deactivated.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>padding</code></strong> : <em><code>typing.Union[bool, str]</code></em>, <em>optional</em>  <p>To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to
`False` or `'do_not_pad'.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>truncation</code></strong> : <em><code>typing.Union[bool, str]</code></em>, <em>optional</em>   <p>To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to
`False` or `do_not_truncate`.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>is_split_into_words</code></strong> : <em><code>&lt;class 'bool'&gt;</code></em>, <em>optional</em>   <p>The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`
if your inputs are pre-tokenized (not numericalized)</p></li>
</ul>
<ul>
<li><strong><code>tok_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>    <p>Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs</p></li>
</ul>
<ul>
<li><strong><code>text_gen_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>Any keyword arguments to pass to the `hf_model.generate` method</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We include a new AFTER batch <code>Transform</code> and <code>TransformBlock</code> specific to text-2-text tasks.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqBatchDecodeTransform" class="doc_header"><code>class</code> <code>Seq2SeqBatchDecodeTransform</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L128" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqBatchDecodeTransform</code>(<strong><code>input_return_type</code></strong>:<code>Type</code>[<code>CT_co</code>]=<em><code>TextInput</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#BatchDecodeTransform"><code>BatchDecodeTransform</code></a></p>
</blockquote>
<p>A class used to cast your inputs as <code>input_return_type</code> for fastai <code>show</code> methods</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><p><strong><code>input_return_type</code></strong> : <em><code>typing.Type</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Seq2SeqTextBlock" class="doc_header"><code>class</code> <code>Seq2SeqTextBlock</code><a href="https://github.com/ohmeow/blurr/tree/master/blurr/data/seq2seq/core.py#L134" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Seq2SeqTextBlock</code>(<strong><code>hf_arch</code></strong>:<code>str</code>=<em><code>None</code></em>, <strong><code>hf_config</code></strong>:<code>PretrainedConfig</code>=<em><code>None</code></em>, <strong><code>hf_tokenizer</code></strong>:<code>PreTrainedTokenizerBase</code>=<em><code>None</code></em>, <strong><code>hf_model</code></strong>:<code>PreTrainedModel</code>=<em><code>None</code></em>, <strong><code>before_batch_tfm</code></strong>:<a href="/blurr/data-core.html#BatchTokenizeTransform"><code>BatchTokenizeTransform</code></a>=<em><code>None</code></em>, <strong><code>after_batch_tfm</code></strong>:<a href="/blurr/data-core.html#BatchDecodeTransform"><code>BatchDecodeTransform</code></a>=<em><code>None</code></em>, <strong><code>max_length</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>max_target_length</code></strong>=<em><code>None</code></em>, <strong><code>padding</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>truncation</code></strong>:<code>Union</code>[<code>bool</code>, <code>str</code>]=<em><code>True</code></em>, <strong><code>input_return_type</code></strong>=<em><code>Seq2SeqTextInput</code></em>, <strong><code>dl_type</code></strong>=<em><code>SortedDL</code></em>, <strong><code>tok_kwargs</code></strong>=<em><code>{}</code></em>, <strong><code>text_gen_kwargs</code></strong>=<em><code>{}</code></em>, <strong><code>before_batch_kwargs</code></strong>=<em><code>{}</code></em>, <strong><code>after_batch_kwargs</code></strong>=<em><code>{}</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/blurr/data-core.html#TextBlock"><code>TextBlock</code></a></p>
</blockquote>
<p>The core <code>TransformBlock</code> to prepare your inputs for training in Blurr with fastai's <code>DataBlock</code> API</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong><code>hf_arch</code></strong> : <em><code>&lt;class 'str'&gt;</code></em>, <em>optional</em>    <p>The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an
instance of `BatchTokenizeTransform` to `before_batch_tfm`)</p></li>
</ul>
<ul>
<li><strong><code>hf_config</code></strong> : <em><code>&lt;class 'transformers.configuration_utils.PretrainedConfig'&gt;</code></em>, <em>optional</em>    <p>A Hugging Face configuration object (not required if passing in an
instance of `BatchTokenizeTransform` to `before_batch_tfm`)</p></li>
</ul>
<ul>
<li><strong><code>hf_tokenizer</code></strong> : <em><code>&lt;class 'transformers.tokenization_utils_base.PreTrainedTokenizerBase'&gt;</code></em>, <em>optional</em>  <p>A Hugging Face tokenizer (not required if passing in an
instance of `BatchTokenizeTransform` to `before_batch_tfm`)</p></li>
</ul>
<ul>
<li><strong><code>hf_model</code></strong> : <em><code>&lt;class 'transformers.modeling_utils.PreTrainedModel'&gt;</code></em>, <em>optional</em>   <p>A Hugging Face model (not required if passing in an
instance of `BatchTokenizeTransform` to `before_batch_tfm`)</p></li>
</ul>
<ul>
<li><strong><code>before_batch_tfm</code></strong> : <em><code>&lt;class 'blurr.data.core.BatchTokenizeTransform'&gt;</code></em>, <em>optional</em>    <p>The before batch transform you want to use to tokenize your raw data on the fly
(defaults to an instance of `BatchTokenizeTransform` created using the Hugging Face objects defined above)</p></li>
</ul>
<ul>
<li><strong><code>after_batch_tfm</code></strong> : <em><code>&lt;class 'blurr.data.core.BatchDecodeTransform'&gt;</code></em>, <em>optional</em>   <p>The batch_tfms to apply to the creation of your DataLoaders,
(defaults to BatchDecodeTransform created using the Hugging Face objects defined above)</p></li>
</ul>
<ul>
<li><strong><code>max_length</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em> <p>To control the length of the padding/truncation for the input sequence. It can be an integer or None,
in which case it will default to the maximum length the model can accept. If the model has no
specific maximum input length, truncation/padding to max_length is deactivated.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>max_target_length</code></strong> : <em><code>&lt;class 'NoneType'&gt;</code></em>, <em>optional</em> <p>To control the length of the padding/truncation for the target sequence. It can be an integer or None,
in which case it will default to the maximum length the model can accept. If the model has no
specific maximum input length, truncation/padding to max_length is deactivated.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-y</p></li>
</ul>
<ul>
<li><strong><code>padding</code></strong> : <em><code>typing.Union[bool, str]</code></em>, <em>optional</em>  <p>To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to
`False` or `'do_not_pad'.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>truncation</code></strong> : <em><code>typing.Union[bool, str]</code></em>, <em>optional</em>   <p>To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to
`False` or `do_not_truncate`.
See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)</p></li>
</ul>
<ul>
<li><strong><code>input_return_type</code></strong> : <em><code>&lt;class 'torch._C._TensorMeta'&gt;</code></em>, <em>optional</em> <p>The return type your decoded inputs should be cast too (used by methods such as `show_batch`)</p></li>
</ul>
<ul>
<li><strong><code>dl_type</code></strong> : <em><code>&lt;class 'type'&gt;</code></em>, <em>optional</em>   <p>The type of `DataLoader` you want created (defaults to `SortedDL`)</p></li>
</ul>
<ul>
<li><strong><code>tok_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>    <p>Any keyword arguments you want your Hugging Face tokenizer to use during tokenization</p></li>
</ul>
<ul>
<li><strong><code>text_gen_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>Any keyword arguments you want to have applied with generating text
(default: default_text_gen_kwargs)</p></li>
</ul>
<ul>
<li><strong><code>before_batch_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>   <p>Any keyword arguments you want applied to `TextBlock`
Any keyword arguments you want applied to your before batch tfm</p></li>
</ul>
<ul>
<li><strong><code>after_batch_kwargs</code></strong> : <em><code>&lt;class 'dict'&gt;</code></em>, <em>optional</em>    <p>Any keyword arguments you want applied to your after batch tfm (or referred to in fastai as `batch_tfms`)</p></li>
</ul>
<ul>
<li><strong><code>kwargs</code></strong> : <em><code>&lt;class 'inspect._empty'&gt;</code></em></li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>... and a <code>DataLoaders.show_batch</code> for seq2seq tasks</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This module includes the fundamental bits to all Seq2Seq transformers data preparation.</p>

</div>
</div>
</div>
</div>
 

